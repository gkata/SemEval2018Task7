<?xml version="1.0" encoding="UTF-8"?>
<doc>
<text id="H94-1014">
<title>Language Modeling With Sentence-Level Mixtures</title>
<abstract>
This paper introduces a simple <entity id="H94-1014.1">mixture language model</entity> that attempts to capture <entity id="H94-1014.2">long distance constraints</entity> in a <entity id="H94-1014.3">sentence</entity> or <entity id="H94-1014.4">paragraph</entity>. The model is an <entity id="H94-1014.5">m-component mixture</entity> of <entity id="H94-1014.6">digram models</entity>. The models were constructed using a <entity id="H94-1014.7">SK vocabulary</entity> and trained using a 76 million word <entity id="H94-1014.8">Wall Street Journal text corpus</entity>. Using the <entity id="H94-1014.9">BU recognition system</entity>, experiments show a 7% improvement in <entity id="H94-1014.10">recognition accuracy</entity> with the <entity id="H94-1014.11">mixture digram models</entity> as compared to using a <entity id="H94-1014.12">Digram model</entity>.
</abstract>

</text>

<text id="W93-0105">
<title>Identifying Unknown Proper Names In Newswire Text</title>
<abstract>
The identification of unknown <entity id="W93-0105.1">proper names</entity> in <entity id="W93-0105.2">text</entity> is a significant challenge for <entity id="W93-0105.3">NLP systems</entity> operating on <entity id="W93-0105.4">unrestricted text</entity>. A system which indexes <entity id="W93-0105.5">documents</entity> according to <entity id="W93-0105.6">name references</entity> can be useful for <entity id="W93-0105.7">information retrieval</entity> or as a <entity id="W93-0105.8">preprocessor</entity> for more <entity id="W93-0105.9">knowledge intensive tasks</entity> such as <entity id="W93-0105.10">database extraction</entity>. This paper describes a system which uses <entity id="W93-0105.11">text skimming techniques</entity> for deriving <entity id="W93-0105.12">proper names</entity> and their <entity id="W93-0105.13">semantic attributes</entity> automatically from <entity id="W93-0105.14">newswire text</entity>, without relying on any listing of <entity id="W93-0105.15">name elements</entity>. In order to identify new names, the system treats <entity id="W93-0105.16">proper names</entity> as (potentially) <entity id="W93-0105.17">context-dependent linguistic expressions</entity>. In addition to using <entity id="W93-0105.18">information</entity> in the <entity id="W93-0105.19">local context</entity>, the system exploits a <entity id="W93-0105.20">computational model of discourse</entity> which identifies individuals based on the way they are described in the text, instead of relying on their description in a pre-existing <entity id="W93-0105.21">knowledge base</entity>.
</abstract>

</text>

<text id="L08-1274">
<title>Representation of Atypical Entities in Ontologies</title>
<abstract>
This paper is a contribution to <entity id="L08-1274.1">formal ontology study</entity>. Some <entity id="L08-1274.2">entities</entity> belong more or less to a <entity id="L08-1274.3">class</entity>. In particular, some individual <entity id="L08-1274.4">entities</entity> are attached to <entity id="L08-1274.5">classes</entity> whereas they do not check all the <entity id="L08-1274.6">properties</entity> of the <entity id="L08-1274.7">class</entity>. To specify whether an individual <entity id="L08-1274.8">entity</entity> belonging to a <entity id="L08-1274.9">class</entity> is typical or not, we borrow the <entity id="L08-1274.10">topological concepts</entity> of interior, border, closure, and exterior. We define a system of <entity id="L08-1274.11">relations</entity> by adapting these <entity id="L08-1274.12">topological operators</entity>. A scale of <entity id="L08-1274.13">typicality</entity>, based on <entity id="L08-1274.14">topology</entity>, is introduced. It enables to define levels of <entity id="L08-1274.15">typicality</entity> where individual <entity id="L08-1274.16">entities</entity> are more or less typical elements of a <entity id="L08-1274.17">concept</entity>.
</abstract>

</text>

<text id="L08-1598">
<title>Geo-WordNet: Automatic Georeferencing of WordNet</title>
<abstract>
<entity id="L08-1598.1">WordNet</entity> has been used extensively as a resource for the <entity id="L08-1598.2">Word Sense Disambiguation (WSD) task</entity>, both as a <entity id="L08-1598.3">sense inventory</entity> and a <entity id="L08-1598.4">repository</entity> of <entity id="L08-1598.5">semantic relationships</entity>. Recently, we investigated the possibility to use it as a resource for the <entity id="L08-1598.6">Geographical Information Retrieval task</entity>, more specifically for the <entity id="L08-1598.7">toponym disambiguation task</entity>, which could be considered a specialization of <entity id="L08-1598.8">WSD</entity>. We found that it would be very useful to assign to <entity id="L08-1598.9">geographical entities</entity> in <entity id="L08-1598.10">WordNet</entity> their coordinates, especially in order to implement <entity id="L08-1598.11">geometric shape-based disambiguation methods</entity>. This paper presents <entity id="L08-1598.12">Geo-WordNet</entity>, an <entity id="L08-1598.13">automatic annotation</entity> of <entity id="L08-1598.14">WordNet</entity> with <entity id="L08-1598.15">geographical coordinates</entity>. The <entity id="L08-1598.16">annotation</entity> has been carried out by extracting <entity id="L08-1598.17">geographical synsets</entity> from <entity id="L08-1598.18">WordNet</entity>, together with their <entity id="L08-1598.19">holonyms</entity> and <entity id="L08-1598.20">hypernyms</entity>, and comparing them to the entries in the <entity id="L08-1598.21">Wikipedia-World geographical database</entity>. A <entity id="L08-1598.22">weight</entity> was calculated for each of the <entity id="L08-1598.23">candidate annotations</entity>, on the basis of <entity id="L08-1598.24">matches</entity> found between the <entity id="L08-1598.25">database entries</entity> and <entity id="L08-1598.26">synset gloss</entity>, <entity id="L08-1598.27">holonyms</entity> and <entity id="L08-1598.28">hypernyms</entity>. The resulting resource may be used in <entity id="L08-1598.29">Geographical Information Retrieval related tasks</entity>, especially for <entity id="L08-1598.30">toponym disambiguation</entity>.
</abstract>

</text>

<text id="E03-2013">
<title>Robust Generic And Query-Based Summarization</title>
<abstract>
"We present a <entity id="E03-2013.1">robust summarisation system</entity> developed within the <entity id="E03-2013.2">GATE architecture</entity> that makes use of <entity id="E03-2013.3">robust components</entity> for <entity id="E03-2013.4">semantic tagging</entity> and <entity id="E03-2013.5">coreference resolution</entity> provided by <entity id="E03-2013.6">GATE</entity>. Our <entity id="E03-2013.7">system</entity> combines <entity id="E03-2013.8">GATE components</entity> with well established <entity id="E03-2013.9">statistical techniques</entity> developed for the purpose of <entity id="E03-2013.10">text summarisation research</entity>. The <entity id="E03-2013.11">system</entity> supports <entity id="E03-2013.12">"generic" and query-based summarisation</entity> addressing the need for <entity id="E03-2013.13">user adaptation</entity>. "
</abstract>

</text>

<text id="M92-1039">
<title>Description Of The LINK System Used For MUC-4</title>
<abstract>
The University of Michigan's <entity id="M92-1039.1">natural language processing system</entity>, called <entity id="M92-1039.2">LINK</entity>, is a <entity id="M92-1039.3">unification-based system</entity> which we have developed over the last four years. Prior to <entity id="M92-1039.4">MUC-4</entity>, <entity id="M92-1039.5">LINK</entity> had been used to extract <entity id="M92-1039.6">information</entity> from <entity id="M92-1039.7">free-form texts</entity> in two narrow <entity id="M92-1039.8">application domains</entity>. One <entity id="M92-1039.9">application corpus</entity> contained <entity id="M92-1039.10">terse descriptions</entity> of symptoms displayed by malfunctioning automobiles, and the repairs which fixed them. The other <entity id="M92-1039.11">corpus</entity> described sequences of <entity id="M92-1039.12">activities</entity> to be performed on an <entity id="M92-1039.13">assembly line</entity>. In empirical testing in these two <entity id="M92-1039.14">domains</entity>, <entity id="M92-1039.15">LINK</entity> correctly processed 70% of previously unseen <entity id="M92-1039.16">descriptions</entity>. A <entity id="M92-1039.17">template</entity> was counted as correct only if all of the <entity id="M92-1039.18">fillers</entity> in the <entity id="M92-1039.19">template</entity> were filled correctly. In addition, <entity id="M92-1039.20">LINK</entity> generated incomplete (but not incorrect) <entity id="M92-1039.21">templates</entity> for another 15% of the <entity id="M92-1039.22">descriptions</entity>. These previous <entity id="M92-1039.23">domains</entity> were much narrower than the <entity id="M92-1039.24">MUC-4 terrorism domain</entity>. As a comparison, the <entity id="M92-1039.25">lexicons</entity> for the previous <entity id="M92-1039.26">domains</entity> contained only 300-500 <entity id="M92-1039.27">words</entity>, compared with 6700 <entity id="M92-1039.28">words</entity> in our <entity id="M92-1039.29">MUC-4 test configuration</entity>. Previous <entity id="M92-1039.30">grammar size</entity> ranged from 75-100 <entity id="M92-1039.31">rules</entity>, compared with over 500 <entity id="M92-1039.32">rules</entity> in the <entity id="M92-1039.33">MUC-4 knowledge base</entity>. In addition, the previous <entity id="M92-1039.34">application domains</entity> consisted only of <entity id="M92-1039.35">single-sentence inputs</entity>. Thus, the integration of <entity id="M92-1039.36">information</entity> from multiple <entity id="M92-1039.37">sentences</entity> was not an issue in our previous work.
</abstract>

</text>

<text id="H89-2057">
<title>Research And Development In Natural Language Understanding</title>
<abstract>
Brief Summary of Objectives: There are three objectives of the contract: to perform research and development in <entity id="H89-2057.1">parallel parsing</entity>, <entity id="H89-2057.2">semantic representation</entity>, <entity id="H89-2057.3">ill-formed input</entity>, <entity id="H89-2057.4">discourse</entity>, and tools for <entity id="H89-2057.5">linguistic knowledge acquisition</entity>, and to integrate <entity id="H89-2057.6">software components</entity> from BBN and elsewhere to produce <entity id="H89-2057.7">Janus, DARPA's New Generation Natural Language Interface</entity>, and to demonstrate state-of-the-art natural <entity id="H89-2057.8">language technology</entity> in <entity id="H89-2057.9">DARPA applications</entity>. The following <entity id="H89-2057.10">software</entity> has been distributed: <entity id="H89-2057.11">natural language system</entity>; <entity id="H89-2057.12">IRACQ, knowledge acquisition system</entity>; <entity id="H89-2057.13">System components</entity> and <entity id="H89-2057.14">knowledge bases</entity> of <entity id="H89-2057.15">Janus</entity>; <entity id="H89-2057.16">KL-TWO knowledge representation and inference system</entity> integrated with <entity id="H89-2057.17">Janus</entity>; various <entity id="H89-2057.18">components</entity> for DARPA's <entity id="H89-2057.19">Spoken Language Systems Project</entity> at BBN.
</abstract>

</text>

<text id="H01-1009">
<title>Automatic Pattern Acquisition For Japanese Information Extraction</title>
<abstract>
One of the central issues for <entity id="H01-1009.1">information extraction</entity> is the cost of <entity id="H01-1009.2">customization</entity> from one <entity id="H01-1009.3">scenario</entity> to another. Research on the <entity id="H01-1009.4">automated acquisition</entity> of <entity id="H01-1009.5">patterns</entity> is important for <entity id="H01-1009.6">portability</entity> and <entity id="H01-1009.7">scalability</entity>. In this paper, we introduce <entity id="H01-1009.8">Tree-Based Pattern representation</entity> where a <entity id="H01-1009.9">pattern</entity> is denoted as a <entity id="H01-1009.10">path</entity> in the <entity id="H01-1009.11">dependency tree</entity> of a <entity id="H01-1009.12">sentence</entity>. We outline the procedure to acquire <entity id="H01-1009.13">Tree-Based Patterns</entity> in <entity id="H01-1009.14">Japanese</entity> from <entity id="H01-1009.15">un-annotated text</entity>. The system extracts the relevant <entity id="H01-1009.16">sentences</entity> from the <entity id="H01-1009.17">training data</entity> based on <entity id="H01-1009.18">TF/IDF scoring</entity> and the common <entity id="H01-1009.19">paths</entity> in the <entity id="H01-1009.20">parse tree</entity> of relevant <entity id="H01-1009.21">sentences</entity> are taken as extracted <entity id="H01-1009.22">patterns</entity>.
</abstract>

</text>

<text id="W93-0108">
<title>Detecting Dependencies Between Semantic Verb Subclasses And Subcategorization Frames In Text Corpora</title>
<abstract>
We present a methodfor individuating dependencies between the <entity id="W93-0108.1">semantic class</entity> of <entity id="W93-0108.2">predicates</entity> and their associated <entity id="W93-0108.3">subcategorization frames</entity>, and describe an <entity id="W93-0108.4">implementation</entity> which allows the <entity id="W93-0108.5">acquisition</entity> of such dependencies from <entity id="W93-0108.6">bracketed texts</entity>.
</abstract>

</text>

<text id="W97-1507">
<title>Application-Driven Automatic Subgrammar Extraction</title>
<abstract>
The <entity id="W97-1507.1">space and run-time requirements</entity> of <entity id="W97-1507.2">broad coverage grammars</entity> appear for many <entity id="W97-1507.3">applications</entity> unreasonably large in relation to the relative simplicity of the task at hand. On the other hand, handcrafted development of <entity id="W97-1507.4">application-dependent grammars</entity> is in danger of duplicating work which is then difficult to re-use in other contexts of application. To overcome this problem, we present in this paper a procedure for the <entity id="W97-1507.5">automatic extraction</entity> of <entity id="W97-1507.6">application-tuned consistent subgrammars</entity> from proved <entity id="W97-1507.7">large-scale generation grammars</entity>. The procedure has been implemented for <entity id="W97-1507.8">large-scale systemic grammars</entity> and builds on the <entity id="W97-1507.9">formal equivalence</entity> between <entity id="W97-1507.10">systemic grammars</entity> and <entity id="W97-1507.11">typed unification based grammars</entity>. Its evaluation for the <entity id="W97-1507.12">generation</entity> of <entity id="W97-1507.13">encyclopedia entries</entity> is described, and directions of future development, <entity id="W97-1507.14">applicability</entity>, and <entity id="W97-1507.15">extensions</entity> are discussed.
</abstract>

</text>

<text id="W00-0719">
<title>Combining Text And Heuristics For Cost-Sensitive Spam Filtering</title>
<abstract>
<entity id="W00-0719.1">Spam filtering</entity> is a <entity id="W00-0719.2">text categorization task</entity> that shows especial features that make it interesting and difficult. First, the <entity id="W00-0719.3">task</entity> has been performed traditionally using <entity id="W00-0719.4">heuristics</entity> from the <entity id="W00-0719.5">domain</entity>. Second, a <entity id="W00-0719.6">cost model</entity> is required to avoid <entity id="W00-0719.7">misclassification</entity> of <entity id="W00-0719.8">legitimate messages</entity>. We present a <entity id="W00-0719.9">comparative evaluation</entity> of several <entity id="W00-0719.10">machine learning algorithms</entity> applied to <entity id="W00-0719.11">spam filtering</entity>, considering the <entity id="W00-0719.12">text</entity> of the <entity id="W00-0719.13">messages</entity> and a set of <entity id="W00-0719.14">heuristics</entity> for the <entity id="W00-0719.15">task</entity>. <entity id="W00-0719.16">Cost-oriented biasing</entity> and evaluation is performed.
</abstract>

</text>

<text id="W02-0209">
<title>A Flexible Framework For Developing Mixed-Initiative Dialog Systems</title>
<abstract>
We present a new <entity id="W02-0209.1">framework</entity> for rapid development of <entity id="W02-0209.2">mixed-initiative dialog systems</entity>. Using this <entity id="W02-0209.3">framework</entity>, a developer can author sophisticated <entity id="W02-0209.4">dialog systems</entity> for multiple <entity id="W02-0209.5">channels of interaction</entity> by specifying an <entity id="W02-0209.6">interaction modality</entity>, a rich <entity id="W02-0209.7">task hierarchy</entity> and <entity id="W02-0209.8">task parameters</entity>, and <entity id="W02-0209.9">domain-specific modules</entity>. The <entity id="W02-0209.10">framework</entity> includes a <entity id="W02-0209.11">dialog history</entity> that tracks input, output, and results. We present the <entity id="W02-0209.12">framework</entity> and preliminary results in two <entity id="W02-0209.13">application domains</entity>.
</abstract>

</text>

<text id="J90-3001">
<title>Resolving Quasi Logical Forms</title>
<abstract>
"SRI International Cambridge Research Centre 23 Millers Yard Cambridge CB2 IRQ U.K.The paper describes <entity id="J90-3001.1">intermediate and resolved logical form representations</entity> of <entity id="J90-3001.2">sentences</entity> involving <entity id="J90-3001.3">referring expressions</entity> and a <entity id="J90-3001.4">reference resolution process</entity> for mapping between these <entity id="J90-3001.5">representations</entity>. The <entity id="J90-3001.6">intermediate representation, Quasi Logical Form (or QLF)</entity>, may contain <entity id="J90-3001.7">unresolved terms</entity> corresponding to <entity id="J90-3001.8">anaphoric noun phrases</entity> covering <entity id="J90-3001.9">bound variable anaphora</entity>, <entity id="J90-3001.10">reflexives</entity>, and <entity id="J90-3001.11">definite descriptions</entity>. <entity id="J90-3001.12">Implicit relations</entity> arising in constructs such as <entity id="J90-3001.13">compound nominals</entity> appear in <entity id="J90-3001.14">QLF</entity> as <entity id="J90-3001.15">unresolved formulae</entity>. The <entity id="J90-3001.16">QLF representation</entity> is also neutral with respect to <entity id="J90-3001.17">ambiguities</entity> corresponding to <entity id="J90-3001.18">quantifier scope</entity> and the <entity id="J90-3001.19">collective/distributive distinction</entity>, the latter being treated as <entity id="J90-3001.20">quantifier resolution</entity>. <entity id="J90-3001.21">Reference candidates</entity> are proposed according to an ordered set of "<entity id="J90-3001.22">reference resolution rules</entity>" producing possible <entity id="J90-3001.23">resolved logical forms</entity> to which <entity id="J90-3001.24">linguistic and pragmatic constraints</entity> are then applied."
</abstract>

</text>

<text id="P98-2155">
<title>Constituent-based Accent Prediction</title>
<abstract>
Near-perfect <entity id="P98-2155.1">automatic accent assignment</entity> is attainable for <entity id="P98-2155.2">citation-style speech</entity>, but better <entity id="P98-2155.3">computational models</entity> are needed to predict <entity id="P98-2155.4">accent</entity> in <entity id="P98-2155.5">extended, spontaneous discourses</entity>. This paper presents an empirically <entity id="P98-2155.6">motivated theory</entity> of the <entity id="P98-2155.7">discourse focusing nature</entity> of <entity id="P98-2155.8">accent</entity> in <entity id="P98-2155.9">spontaneous speech</entity>. Hypotheses based on this <entity id="P98-2155.10">theory</entity> lead to a new approach to <entity id="P98-2155.11">accent prediction</entity>, in which <entity id="P98-2155.12">patterns of deviation</entity> from <entity id="P98-2155.13">citation form accentuation</entity>, defined at the <entity id="P98-2155.14">constituent or noun phrase level</entity>, are automatically learned from an <entity id="P98-2155.15">annotated corpus</entity>. <entity id="P98-2155.16">Machine learning experiments</entity> on 1031 <entity id="P98-2155.17">noun phrases</entity> from eighteen <entity id="P98-2155.18">spontaneous direction-giving monologues</entity> show that <entity id="P98-2155.19">accent assignment</entity> can be significantly improved by up to 4%-6% relative to a hypothetical <entity id="P98-2155.20">baseline system</entity> that would produce only <entity id="P98-2155.21">citation-form accentuation</entity>, giving <entity id="P98-2155.22">error rate reductions</entity> of 11%-25%.
</abstract>

</text>

<text id="W04-0805">
<title>The Italian Lexical Sample Task At Senseval-3</title>
<abstract>
The <entity id="W04-0805.1">Italian lexical sample task</entity> at <entity id="W04-0805.2">SENSEVAL-3</entity> provided a framework to evaluate <entity id="W04-0805.3">supervised and semi-supervised WSD systems</entity>. This paper reports on the <entity id="W04-0805.4">task preparation</entity> - which offered the opportunity to review and refine the <entity id="W04-0805.5">Italian MultiWordNet</entity> - and on the results of the six participants, focussing on both the manual and automatic <entity id="W04-0805.6">tagging procedures</entity>.
</abstract>

</text>

<text id="W06-0701">
<title>Dimensionality Reduction Aids Term Co-Occurrence Based Multi-Document Summarization</title>
<abstract>
A key task in an <entity id="W06-0701.1">extraction system</entity> for <entity id="W06-0701.2">query-oriented multi-document summarisation</entity>, necessary for computing <entity id="W06-0701.3">relevance</entity> and <entity id="W06-0701.4">redundancy</entity>, is modelling <entity id="W06-0701.5">text semantics</entity>. In the <entity id="W06-0701.6">Embra system</entity>, we use a <entity id="W06-0701.7">representation</entity> derived from the <entity id="W06-0701.8">singular value decomposition</entity> of a <entity id="W06-0701.9">term co-occurrence matrix</entity>. We present methods to show the reliability of <entity id="W06-0701.10">performance improvements</entity>. We find that <entity id="W06-0701.11">Embra</entity> performs better with <entity id="W06-0701.12">dimensionality reduction</entity>.
</abstract>

</text>

<text id="W04-3215">
<title>Object- Extraction And Question-Parsing Using CCG</title>
<abstract>
Accurate <entity id="W04-3215.1">dependency recovery</entity> has recently been reported for a number of <entity id="W04-3215.2">wide-coverage statistical parsers</entity> using <entity id="W04-3215.3">Combinatory Categorial Grammar (ccg)</entity>. However, overall figures give no indication of a <entity id="W04-3215.4">parser's performance</entity> on specific <entity id="W04-3215.5">constructions</entity>, nor how suitable a <entity id="W04-3215.6">parser</entity> is for specific <entity id="W04-3215.7">applications</entity>. In this paper we give a detailed evaluation of a <entity id="W04-3215.8">ccg parser</entity> on <entity id="W04-3215.9">object extraction dependencies</entity> found in <entity id="W04-3215.10">wsj text</entity>. We also show how the <entity id="W04-3215.11">parser</entity> can be used to parse <entity id="W04-3215.12">questions</entity> for <entity id="W04-3215.13">Question Answering</entity>. The <entity id="W04-3215.14">accuracy</entity> of the original <entity id="W04-3215.15">parser</entity> on <entity id="W04-3215.16">questions</entity> is very poor, and we propose a novel technique for porting the <entity id="W04-3215.17">parser</entity> to a new <entity id="W04-3215.18">domain</entity>, by creating new <entity id="W04-3215.19">labelled data</entity> at the <entity id="W04-3215.20">lexical category level</entity> only. Using a <entity id="W04-3215.21">supertagger</entity> to assign <entity id="W04-3215.22">categories</entity> to <entity id="W04-3215.23">words</entity>, trained on the new data, leads to a dramatic increase in <entity id="W04-3215.24">question parsing accuracy</entity>.
</abstract>

</text>

<text id="W07-1414">
<title>Dependency-based paraphrasing for recognizing textual entailment</title>
<abstract>
This paper addresses <entity id="W07-1414.1">syntax-based paraphrasing methods</entity> for <entity id="W07-1414.2">Recognizing Textual Entailment (RTE)</entity>. In particular, we describe a <entity id="W07-1414.3">dependency-based paraphrasing algorithm</entity>, using the <entity id="W07-1414.4">DIRT data set</entity>, and its application in the context of a straightforward <entity id="W07-1414.5">RTE system</entity> based on aligning <entity id="W07-1414.6">dependency trees</entity>. We find a small positive effect of <entity id="W07-1414.7">dependency-based paraphrasing</entity> on both the <entity id="W07-1414.8">RTE3 development and test sets</entity>, but the added value of this type of <entity id="W07-1414.9">paraphrasing</entity> deserves further analysis.
</abstract>

</text>

<text id="W06-2103">
<title>A Quantitative Approach To Preposition-Pronoun Contraction In Polish</title>
<abstract>
This paper presents the current results of an ongoing research project on <entity id="W06-2103.1">corpus distribution</entity> of <entity id="W06-2103.2">prepositions</entity> and <entity id="W06-2103.3">pronouns</entity> within <entity id="W06-2103.4">Polish preposition-pronoun contractions</entity>. The goal of the project is to provide a quantitative description of <entity id="W06-2103.5">Polish preposition-pronoun contractions</entity> taking into consideration <entity id="W06-2103.6">morphosyntactic properties</entity> of their components. It is expected that the results will provide a basis for a revision of the traditionally assumed <entity id="W06-2103.7">inflectional paradigms</entity> of <entity id="W06-2103.8">Polish pronouns</entity> and, thus, for a possible remodeling of these <entity id="W06-2103.9">paradigms</entity>. The results of <entity id="W06-2103.10">corpus-based investigations</entity> of the <entity id="W06-2103.11">distribution</entity> of <entity id="W06-2103.12">prepositions</entity> within <entity id="W06-2103.13">preposition-pronoun contractions</entity> can be used for grammar-theoretical and lexicographic purposes.
</abstract>

</text>

<text id="P08-1016">
<title>Lexicalized Phonotactic Word Segmentation</title>
<abstract>
This paper presents a new <entity id="P08-1016.1">unsupervised algorithm</entity> (<entity id="P08-1016.2">WordEnds</entity>) for inferring <entity id="P08-1016.3">word boundaries</entity> from <entity id="P08-1016.4">transcribed adult conversations</entity>. <entity id="P08-1016.5">Phone ngrams</entity> before and after observed pauses are used to bootstrap a simple <entity id="P08-1016.6">discriminative model</entity> of <entity id="P08-1016.7">boundary marking</entity>. This fast <entity id="P08-1016.8">algorithm</entity> delivers high <entity id="P08-1016.9">performance</entity> even on <entity id="P08-1016.10">morphologically complex words</entity> in <entity id="P08-1016.11">English</entity> and <entity id="P08-1016.12">Arabic</entity>, and promising results on accurate <entity id="P08-1016.13">phonetic transcriptions</entity> with extensive <entity id="P08-1016.14">pronunciation variation</entity>. Expanding <entity id="P08-1016.15">training data</entity> beyond the traditional <entity id="P08-1016.16">miniature datasets</entity> pushes <entity id="P08-1016.17">performance numbers</entity> well above those previously reported. This suggests that <entity id="P08-1016.18">WordEnds</entity> is a viable <entity id="P08-1016.19">model</entity> of <entity id="P08-1016.20">child language acquisition</entity> and might be useful in <entity id="P08-1016.21">speech understanding</entity>.
</abstract>

</text>

<text id="W08-1406">
<title>Mixed-Source Multi-Document Speech-to-Text Summarization</title>
<abstract><entity id="W08-1406.1">Speech-to-text summarization systems</entity> usually take as <entity id="W08-1406.2">input</entity> the <entity id="W08-1406.3">output</entity> of an <entity id="W08-1406.4">automatic speech recognition (ASR) system</entity> that is affected by issues like <entity id="W08-1406.5">speech recognition errors</entity>, <entity id="W08-1406.6">disfluencies</entity>, or difficulties in the accurate identification of <entity id="W08-1406.7">sentence boundaries</entity>. We propose the inclusion of related, solid <entity id="W08-1406.8">background information</entity> to cope with the difficulties of summarizing <entity id="W08-1406.9">spoken language</entity> and the use of <entity id="W08-1406.10">multi-document summarization techniques</entity> in <entity id="W08-1406.11">single document speech-to-text summarization</entity>. In this work, we explore the possibilities offered by <entity id="W08-1406.12">phonetic information</entity> to select the <entity id="W08-1406.13">background information</entity> and conduct a <entity id="W08-1406.14">perceptual evaluation</entity> to better assess the relevance of the inclusion of that <entity id="W08-1406.15">information</entity>. Results show that <entity id="W08-1406.16">summaries</entity> generated using this approach are considerably better than those produced by an up-to-date <entity id="W08-1406.17">latent semantic analysis (LSA) summarization method</entity> and suggest that humans prefer <entity id="W08-1406.18">summaries</entity> restricted to the <entity id="W08-1406.19">information</entity> conveyed in the <entity id="W08-1406.20">input source</entity>.
</abstract>

</text>

<text id="P04-3014">
<title>Improving Bitext Word Alignments Via Syntax-Based Reordering Of English</title>
<abstract>
We present an improved method for <entity id="P04-3014.1">automated word alignment</entity> of <entity id="P04-3014.2">parallel texts</entity> which takes advantage of knowledge of <entity id="P04-3014.3">syntactic divergences</entity>, while avoiding the need for <entity id="P04-3014.4">syntactic analysis</entity> of the <entity id="P04-3014.5">less resource rich language</entity>, and retaining the <entity id="P04-3014.6">robustness</entity> of <entity id="P04-3014.7">syntactically agnostic approaches</entity> such as the <entity id="P04-3014.8">IBM word alignment models</entity>. We achieve this by using simple, easily-elicited knowledge to produce <entity id="P04-3014.9">syntax-based heuristics</entity> which transform the <entity id="P04-3014.10">target language</entity> (e.g. <entity id="P04-3014.11">English</entity>) into a form more closely resembling the <entity id="P04-3014.12">source language</entity>, and then by using <entity id="P04-3014.13">standard alignment methods</entity> to align the <entity id="P04-3014.14">transformed bitext</entity>. We present experimental results under variable resource conditions. The method improves <entity id="P04-3014.15">word alignment performance</entity> for <entity id="P04-3014.16">language pairs</entity> such as <entity id="P04-3014.17">English-Korean</entity> and <entity id="P04-3014.18">English-Hindi</entity>, which exhibit <entity id="P04-3014.19">longer-distance syntactic divergences</entity>.
</abstract>

</text>

<text id="C82-1003">
<title>Knowledge Representation Method Based On Predicate Calculus In An Intelligent CAI System</title>
<abstract>
The <entity id="C82-1003.1">knowledge representation method</entity> is introduced to be applied in the <entity id="C82-1003.2">ICAI system</entity> to teach <entity id="C82-1003.3">programming language</entity>. <entity id="C82-1003.4">Knowledge</entity> about <entity id="C82-1003.5">syntax</entity> and <entity id="C82-1003.6">semantics</entity> of that <entity id="C82-1003.7">language</entity> is represented by a set of <entity id="C82-1003.8">axioms</entity> written in the <entity id="C82-1003.9">predicate calculus language</entity>. The <entity id="C82-1003.10">directed graph</entity> of <entity id="C82-1003.11">concepts</entity> is mentioned as a method to represent an instructional structure of the <entity id="C82-1003.12">domain knowledge</entity>. The <entity id="C82-1003.13">proof procedure</entity> to answer student's questions is described.
</abstract>

</text>

<text id="C90-3071">
<title>Information Extraction And Semantic Constraints</title>
<abstract>
We consider the problem of extracting specified types of <entity id="C90-3071.1">information</entity> from <entity id="C90-3071.2">natural language text</entity>. To properly analyze the <entity id="C90-3071.3">text</entity>, we wish to apply <entity id="C90-3071.4">semantic (selectional) constraints</entity> whenever possible; however, we cannot expect to have <entity id="C90-3071.5">semantic patterns</entity> for all the input we may encounter in real <entity id="C90-3071.6">texts</entity>. We therefore use <entity id="C90-3071.7">preference semantics</entity>: selecting the analysis which maximizes the number of <entity id="C90-3071.8">semantic patterns</entity> matched. We describe a specific <entity id="C90-3071.9">information extraction task</entity>, and report on the benefits of using <entity id="C90-3071.10">preference semantics</entity> for this task.
</abstract>

</text>

<text id="L08-1593">
<title>Whats in a Colour? Studying and Contrasting Colours with COMPARA</title>
<abstract>
In this paper we present contrastive colour studies done using <entity id="L08-1593.1">COMPARA</entity>, the largest edited <entity id="L08-1593.2">parallel corpus</entity> in the world (as far as we know). The studies were the result of <entity id="L08-1593.3">semantic annotation</entity> of the <entity id="L08-1593.4">corpus</entity> in this <entity id="L08-1593.5">domain</entity>. We chose to start with colour because it is a relatively contained <entity id="L08-1593.6">lexical category</entity> and the subject of many arguments in <entity id="L08-1593.7">linguistics</entity>. We begin by explaining the criteria involved in the <entity id="L08-1593.8">annotation process</entity>, not only for the <entity id="L08-1593.9">colour categories</entity> but also for the <entity id="L08-1593.10">colour groups</entity> created in order to do finer-grained analyses, presenting also some <entity id="L08-1593.11">quantitative data</entity> regarding these <entity id="L08-1593.12">categories</entity> and <entity id="L08-1593.13">groups</entity>. We proceed to compare the two <entity id="L08-1593.14">languages</entity> according to the diversity of available <entity id="L08-1593.15">lexical items</entity>, <entity id="L08-1593.16">morphological and syntactic properties</entity>, and then try to understand the <entity id="L08-1593.17">translation</entity> of colour. We end by explaining how any user who wants to do serious studies using the <entity id="L08-1593.18">corpus</entity> can collaborate in enhancing the <entity id="L08-1593.19">corpus</entity> and making their <entity id="L08-1593.20">semantic annotations</entity> widely available as well.
</abstract>

</text>

<text id="C02-1128">
<title>Text Authoring, Knowledge Acquisition And Description Logics</title>
<abstract>
We present a principled approach to the problem of connecting a <entity id="C02-1128.1">controlled document authoring system</entity> with a <entity id="C02-1128.2">knowledge base</entity>. We start by describing <entity id="C02-1128.3">closed-world authoring situations</entity>, in which the <entity id="C02-1128.4">knowledge base</entity> is used for constraining the possible <entity id="C02-1128.5">documents</entity> and orienting the <entity id="C02-1128.6">userâ€™s selections</entity>. Then we move to <entity id="C02-1128.7">open-world authoring situations</entity> in which, additionally, choices made during <entity id="C02-1128.8">authoring</entity> are echoed back to the <entity id="C02-1128.9">knowledge base</entity>. In this way the information implicitly encoded in a <entity id="C02-1128.10">document</entity> becomes explicit in the <entity id="C02-1128.11">knowledge base</entity> and can be re-exploited for simplifying the <entity id="C02-1128.12">authoring</entity> of new <entity id="C02-1128.13">documents</entity>. We show how a <entity id="C02-1128.14">Datalog KB</entity> is sufficient for the <entity id="C02-1128.15">closed-world situation</entity>, while a <entity id="C02-1128.16">Description Logic KB</entity> is better-adapted to the more complex <entity id="C02-1128.17">open-world situation</entity>. All along, we pay special attention to logically sound solutions and to <entity id="C02-1128.18">decidability issues</entity> in the different processes.
</abstract>

</text>

<text id="E87-1042">
<title>Temporal Reasoning In Natural Language Understanding: The Temporal Structure Of The Narrative</title>
<abstract>
This paper proposes a new framework for <entity id="E87-1042.1">discourse analysis</entity>, in the spirit of Grosz and Sid-ner (1986), Webber (l987a,b) but differentiated with respect to the <entity id="E87-1042.2">type</entity> or <entity id="E87-1042.3">genre</entity> of <entity id="E87-1042.4">discourse</entity>. It is argued that different <entity id="E87-1042.5">genres</entity> call for different <entity id="E87-1042.6">representations</entity> and <entity id="E87-1042.7">processing strategies</entity>; particularly important is the distinction between <entity id="E87-1042.8">subjective, performative discourse</entity> and <entity id="E87-1042.9">objective discourse</entity>, of which <entity id="E87-1042.10">narrative</entity> is a primary example. This paper concentrates on narratives and introduces the notions of <entity id="E87-1042.11">temporal focus</entity> (proposed also in Webber (1987b)) and <entity id="E87-1042.12">narrative move</entity>. The <entity id="E87-1042.13">processing tasks</entity> involved in reconstructing the <entity id="E87-1042.14">temporal structure</entity> of a <entity id="E87-1042.15">narrative</entity> ( Webber 's <entity id="E87-1042.16">e/s structure</entity>) are formulated in terms of these two notions. The remainder of the paper analyzes the <entity id="E87-1042.17">durational and aspectual knowledge</entity> needed for those <entity id="E87-1042.18">tasks</entity>. Distinctions are established between <entity id="E87-1042.19">grammatical aspect</entity>, <entity id="E87-1042.20">aspectual class</entity> and the <entity id="E87-1042.21">aspectual perspective</entity> of a <entity id="E87-1042.22">sentence</entity> in <entity id="E87-1042.23">discourse</entity>; it is shown that in <entity id="E87-1042.24">English</entity>, <entity id="E87-1042.25">grammatical aspect</entity> under-determines the <entity id="E87-1042.26">aspectual perspective</entity>.
</abstract>

</text>

<text id="L08-1432">
<title>Standardising Bilingual Lexical Resources According to the Lexicon Markup Framework </title>
<abstract>
The <entity id="L08-1432.1">Dutch HLT agency for language and speech technology</entity> (known as <entity id="L08-1432.2">TST-centrale</entity>) at the <entity id="L08-1432.3">Institute for Dutch Lexicology</entity> is responsible for the maintenance, distribution and accessibility of <entity id="L08-1432.4">(Dutch) digital language resources</entity>. In this paper we present a project which aims to standardise the format of a set of <entity id="L08-1432.5">bilingual lexicons</entity> in order to make them available to potential users, to facilitate the exchange of <entity id="L08-1432.6">data</entity> (among the resources and with other <entity id="L08-1432.7">(monolingual) resources</entity> ) and to enable reuse of these <entity id="L08-1432.8">lexicons</entity> for <entity id="L08-1432.9">NLP applications</entity> like <entity id="L08-1432.10">machine translation</entity> and <entity id="L08-1432.11">multilingual information retrieval</entity>. We pay special attention to the methods and tools we used and to some of the problematic issues we encountered during the <entity id="L08-1432.12">conversion process</entity>. As these problems are mainly caused by the fact that the standard <entity id="L08-1432.13">LMF model</entity> fails in representing the detailed <entity id="L08-1432.14">semantic and pragmatic distinctions</entity> made in our <entity id="L08-1432.15">bilingual data</entity>, we propose some modifications to the standard. In general, we think that a standard for <entity id="L08-1432.16">lexicons</entity> should provide a model for <entity id="L08-1432.17">bilingual lexicons</entity> that is able to represent all detailed and <entity id="L08-1432.18">fine-grained translation information</entity> which is generally found in these types of <entity id="L08-1432.19">lexicons</entity>.
</abstract>

</text>

<text id="D08-1034">
<title>Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy </title>
<abstract>
In recent years, with the development of <entity id="D08-1034.1">Chinese semantically annotated corpus</entity>, such as <entity id="D08-1034.2">Chinese Proposition Bank</entity> and <entity id="D08-1034.3">Normalization Bank</entity>, the <entity id="D08-1034.4">Chinese semantic role labeling (SRL) task</entity> has been boosted. Similar to <entity id="D08-1034.5">English</entity>, the <entity id="D08-1034.6">Chinese SRL</entity> can be divided into two tasks : <entity id="D08-1034.7">semantic role identification (SRI)</entity> and <entity id="D08-1034.8">classification (SRC)</entity>. Many features were introduced into these tasks and promising results were achieved. In this paper, we mainly focus on the second task : <entity id="D08-1034.9">SRC</entity>. After exploiting the <entity id="D08-1034.10">linguistic discrepancy</entity> between numbered <entity id="D08-1034.11">arguments</entity> and <entity id="D08-1034.12">ARGMs</entity>, we built a <entity id="D08-1034.13">semantic role classifier</entity> based on a <entity id="D08-1034.14">hierarchical feature selection strategy</entity>. Different from the previous <entity id="D08-1034.15">SRC systems</entity>, we divided <entity id="D08-1034.16">SRC</entity> into three sub tasks in sequence and <entity id="D08-1034.17">trained models</entity> for each sub task. Under the <entity id="D08-1034.18">hierarchical architecture</entity>, each <entity id="D08-1034.19">argument</entity> should first be determined whether it is a numbered <entity id="D08-1034.20">argument</entity> or an <entity id="D08-1034.21">ARGM</entity>, and then be classified into <entity id="D08-1034.22">fine-gained categories</entity>. Finally, we integrated the idea of exploiting <entity id="D08-1034.23">argument interdependence</entity> into our system and further improved the performance. With the novel method, the <entity id="D08-1034.24">classification precision</entity> of our system is 94.68%, which outperforms the strong baseline significantly. It is also the state-of-the-art on <entity id="D08-1034.25">Chinese SRC</entity>.
</abstract>

</text>

<text id="E06-1015">
<title>Making Tree Kernels Practical For Natural Language Learning </title>
<abstract>
In recent years <entity id="E06-1015.1">tree kernels</entity> have been proposed for the <entity id="E06-1015.2">automatic learning</entity> of <entity id="E06-1015.3">natural language applications</entity>. Unfortunately, they show (a) an inherent <entity id="E06-1015.4">super linear complexity</entity> and (b) a lower <entity id="E06-1015.5">accuracy</entity> than <entity id="E06-1015.6">traditional attribute/value methods</entity>. In this paper, we show that <entity id="E06-1015.7">tree kernels</entity> are very helpful in the <entity id="E06-1015.8">processing of natural language</entity> as (a) we provide a simple algorithm to compute <entity id="E06-1015.9">tree kernels</entity> in <entity id="E06-1015.10">linear average running time</entity> and (b) our study on the <entity id="E06-1015.11">classification properties</entity> of diverse <entity id="E06-1015.12">tree kernels</entity> show that <entity id="E06-1015.13">kernel combinations</entity> always improve the traditional methods. Experiments with <entity id="E06-1015.14">Support Vector Machines</entity> on the predicate <entity id="E06-1015.15">argument classification task</entity> provide empirical support to our thesis.
</abstract>

</text>

<text id="A83-1016">
<title>Utilizing Domain-Specific Information For Processing Compact Text </title>
<abstract>
This paper Identifies the types of <entity id="A83-1016.1">sentence fragments</entity> found in the <entity id="A83-1016.2">text</entity> of two domains : <entity id="A83-1016.3">medical records</entity> and <entity id="A83-1016.4">Navy equipment status messages</entity>. The <entity id="A83-1016.5">fragment types</entity> are related to <entity id="A83-1016.6">full sentence forms</entity> on the basis of the elements which were regularly deleted. A breakdown of the <entity id="A83-1016.7">fragment types</entity> and their distributions In the two domains Is presented. An approach to reconstructing the <entity id="A83-1016.8">semantic class</entity> of deleted elements In the <entity id="A83-1016.9">medical records</entity> is proposed which Is based on the <entity id="A83-1016.10">semantic patterns</entity> recognized In the domain.
</abstract>

</text>

<text id="H90-1103">
<title>Opportunities For Advanced Speech Processing In Military Computer-Based Systems </title>
<abstract>
This paper presents a study of <entity id="H90-1103.1">military applications</entity> of advanced <entity id="H90-1103.2">speech processing technology</entity> which includes three major elements: (1) review and assessment of current efforts in <entity id="H90-1103.3">military applications</entity> of <entity id="H90-1103.4">speech technology</entity> ; (2) identification of opportunities for future <entity id="H90-1103.5">military applications</entity> of advanced <entity id="H90-1103.6">speech technology</entity> ; and (3) identification of problem areas where research in <entity id="H90-1103.7">speech processing</entity> is needed to meet <entity id="H90-1103.8">application requirements</entity>, and of current research thrusts which appear promising. The relationship of this study to previous assessments of <entity id="H90-1103.9">military applications</entity> of <entity id="H90-1103.10">speech technology</entity> is discussed, and substantial recent progress is noted. Current efforts in <entity id="H90-1103.11">military applications</entity> of <entity id="H90-1103.12">speech technology</entity> which are highlighted include : (1) <entity id="H90-1103.13">narrowband (2400 b/s) and very low-rate (50-1200 b/s) secure voice communication</entity> ; (2) <entity id="H90-1103.14">voice/data integration in computer networks</entity> ; (3) <entity id="H90-1103.15">speech recognition</entity> in <entity id="H90-1103.16">fighter aircraft</entity>, military helicopters, battle management, and air traffic control <entity id="H90-1103.17">training systems</entity> ; and (4) <entity id="H90-1103.18">noise and interference removal</entity> for <entity id="H90-1103.19">human listeners</entity>. Opportunities for advanced applications are identified by means of descriptions of several generic systems which would be possible with advances in <entity id="H90-1103.20">speech technology</entity> and in <entity id="H90-1103.21">system integration</entity>. These generic systems include : (1) <entity id="H90-1103.22">integrated multi-rate voice/data communications terminal</entity> ; (2) <entity id="H90-1103.23">interactive speech enhancement system</entity> ; (3) <entity id="H90-1103.24">voice-controlled pilot's associate system</entity> ; (4) advanced <entity id="H90-1103.25">air traffic control training systems</entity> ; (5) <entity id="H90-1103.26">battle management command and control support system</entity> with <entity id="H90-1103.27">spoken natural language interface</entity> ; and (6) <entity id="H90-1103.28">spoken language translation system</entity>. In identifying problem areas and research efforts to meet application requirements, it is observed that some of the most promising research involves the integration of <entity id="H90-1103.29">speech algorithm techniques</entity> including <entity id="H90-1103.30">speech coding</entity>, <entity id="H90-1103.31">speech recognition</entity>, and <entity id="H90-1103.32">speaker recognition</entity>.
</abstract>

</text>

<text id="A00-1009">
<title>A Framework For MT And Multilingual NLG Systems Based On Uniform Lexico-Structural Processing </title>
<abstract>
In this paper we describe an implemented framework for developing <entity id="A00-1009.1">monolingual or multilingual natural language generation (NLG) applications</entity> and <entity id="A00-1009.2">machine translation (MT) applications</entity>. The framework demonstrates a uniform approach to <entity id="A00-1009.3">generation and transfer</entity> based on <entity id="A00-1009.4">declarative lexico-structural transformations</entity> of <entity id="A00-1009.5">dependency structures</entity> of syntactic or <entity id="A00-1009.6">conceptual levels</entity> ("<entity id="A00-1009.7">uniform lexico-structural processing </entity>"). We describe how this framework has been used in practical <entity id="A00-1009.8">NLG</entity> and <entity id="A00-1009.9">MT applications</entity>, and report the lessons learned.
</abstract>

</text>

<text id="W93-0307">
<title> Structural Ambiguity And Conceptual Relations </title>
<abstract>
 <entity id="W93-0307.1">Lexical co-occurrence statistics</entity> are becoming widely used in the <entity id="W93-0307.2">syntactic analysis</entity> of <entity id="W93-0307.3">unconstrained text</entity>. However, analyses based solely on <entity id="W93-0307.4">lexical relationships</entity> suffer from <entity id="W93-0307.5">sparseness</entity> of <entity id="W93-0307.6">data</entity>: it is sometimes necessary to use a <entity id="W93-0307.7">less informed model</entity> in order to reliably estimate <entity id="W93-0307.8">statistical parameters</entity>. For example, the <entity id="W93-0307.9">"lexical association"</entity> strategy for <entity id="W93-0307.10">resolving ambiguous prepositional phrase attachments</entity> [Hindle and Rooth 1991] takes into account only the <entity id="W93-0307.11">attachment site</entity> (a <entity id="W93-0307.12">verb</entity> or its <entity id="W93-0307.13">direct object</entity> ) and the <entity id="W93-0307.14">preposition</entity>, ignoring the <entity id="W93-0307.15">object</entity> of the <entity id="W93-0307.16">preposition</entity>.We investigated an extension of the <entity id="W93-0307.17">lexical association</entity> strategy to make use of <entity id="W93-0307.18">noun class information</entity>, thus permitting a <entity id="W93-0307.19">disambiguation</entity> strategy to take more information into account. Although in preliminary experiments the extended strategy did not yield improved performance over <entity id="W93-0307.20">lexical association</entity> alone, a <entity id="W93-0307.21">qualitative analysis</entity> of <entity id="W93-0307.22">results</entity> suggests that the problem lies not in the <entity id="W93-0307.23">noun class information</entity>, but rather in the <entity id="W93-0307.24">multiplicity of classes</entity> available for each <entity id="W93-0307.25">noun</entity> in the absence of <entity id="W93-0307.26">sense disambiguation</entity>. This suggests several possible revisions of our proposal.
</abstract>

</text>

<text id="W97-1510">
<title>EFLUF - An Implementation Of A FLexible Unification Formalism </title>
<abstract>
In this paper we describe <entity id="W97-1510.1">EFLUF</entity> - an implementation of <entity id="W97-1510.2">FLUF</entity>. The idea with this environment is to achieve a base for experimenting with <entity id="W97-1510.3">unification grammars</entity>. In this environment we want to allow the user to affect as many features as possible of the <entity id="W97-1510.4">formalism</entity>, thus being able to test and compare various <entity id="W97-1510.5">constructions </entity> proposed for <entity id="W97-1510.6">unification-based formalisms</entity>. The paper exemplifies the main features of <entity id="W97-1510.7">EFLUF</entity> and shows how these can be used for defining a <entity id="W97-1510.8">grammar</entity>. The most interesting features of <entity id="W97-1510.9">EFLUF</entity> are the various possibilities to affect the behavior of the system. The user can define new <entity id="W97-1510.10">constructions</entity> and how they unify, define new <entity id="W97-1510.11">syntax</entity> for his <entity id="W97-1510.12">constructions</entity> and use <entity id="W97-1510.13">external unification modules</entity>. The paper also gives a discussion on how a system like <entity id="W97-1510.14">EFLUF</entity> would work for a larger application and suggests some additional features and restrictions that would be needed for this.
</abstract>

</text>

<text id="W00-0737">
<title>Hybrid Text Chunking</title>
<abstract>
This paper proposes an <entity id="W00-0737.1">error-driven HMM-based text chunk tagger</entity> with <entity id="W00-0737.2">context-dependent lexicon</entity>. Compared with standard <entity id="W00-0737.3">HMM-based tagger</entity>, this <entity id="W00-0737.4">tagger</entity> incorporates more <entity id="W00-0737.5">contextual information</entity> into a <entity id="W00-0737.6">lexical entry</entity>. Moreover, an <entity id="W00-0737.7">error-driven learning approach</entity> is adopted to decrease the <entity id="W00-0737.8">memory requirement</entity> by keeping only <entity id="W00-0737.9">positive lexical entries</entity> and makes it possible to further incorporate more <entity id="W00-0737.10">context-dependent lexical entries</entity>. Finally, <entity id="W00-0737.11">memory-based learning</entity> is adopted to further improve the performance of the <entity id="W00-0737.12">chunk tagger</entity>.
</abstract>

</text>

<text id="W02-1020">
<title> User-Friendly Text Prediction For Translators</title>
<abstract> <entity id="W02-1020.1">Text prediction</entity> is a form of <entity id="W02-1020.2">interactive machine translation</entity> that is well suited to <entity id="W02-1020.3">skilled translators</entity>. In principle it can assist in the production of a <entity id="W02-1020.4">target text</entity> with minimal disruption to a translator's normal routine. However, recent evaluations of a prototype <entity id="W02-1020.5">prediction system</entity> showed that it significantly decreased the <entity id="W02-1020.6">productivity</entity> of most <entity id="W02-1020.7">translators</entity> who used it. In this paper, we analyze the reasons for this and propose a solution which consists in seeking <entity id="W02-1020.8">predictions</entity> that maximize the <entity id="W02-1020.9">expected benefit</entity> to the <entity id="W02-1020.10">translator</entity>, rather than just trying to anticipate some amount of <entity id="W02-1020.11">upcoming text</entity>. Using a model of a <entity id="W02-1020.12">â€œtypical translatorâ€</entity> constructed from <entity id="W02-1020.13">data</entity> collected in the evaluations of the <entity id="W02-1020.14">prediction prototype</entity>, we show that this approach has the potential to turn <entity id="W02-1020.15">text prediction</entity> into a help rather than a hindrance to a <entity id="W02-1020.16">translator</entity>.
</abstract>

</text>

<text id="J90-3003">
<title>A Computational Grammar Of Discourse-Neutral Prosodic Phrasing In English </title>
<abstract>
"We describe an experimental <entity id="J90-3003.1">text-to-speech system</entity> that uses information about <entity id="J90-3003.2">syntactic constituency</entity>, <entity id="J90-3003.3">adjacency to a verb</entity>, and <entity id="J90-3003.4">constituent length</entity> to determine <entity id="J90-3003.5">prosodic phrasing</entity> for <entity id="J90-3003.6">synthetic speech</entity>. Central goal of our work has been to characterize <entity id="J90-3003.7">"discourse neutral" phrasing</entity>, i.e. <entity id="J90-3003.8">sentence-level phrasing patterns</entity> that are independent of <entity id="J90-3003.9">discourse semantics</entity>. Our account builds on Bachenko et al. (1986), but differs in its treatment of <entity id="J90-3003.10">clausal structure</entity> and <entity id="J90-3003.11">predicate-argument relations</entity>. Results so far indicate that the current system performs well when measured against a <entity id="J90-3003.12">corpus</entity> of <entity id="J90-3003.13">judgments of prosodic phrasing</entity>."
</abstract>

</text>

<text id="P98-2182">
<title> Noun-Phrase Co-occurrence Statistics for Semi-Automatic Semantic Lexicon Construction </title>
<abstract>
" <entity id="P98-2182.1">Generating semantic lexicons semi-automatically</entity> could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting <entity id="P98-2182.2">potential entries</entity> for a category from an <entity id="P98-2182.3">on-line corpus</entity>, based upon a small set of <entity id="P98-2182.4">exemplars</entity>. Our algorithm finds more correct <entity id="P98-2182.5">terms</entity> and fewer incorrect ones than previous work in this area. Additionally, the <entity id="P98-2182.6">entries</entity> that are generated potentially provide broader coverage of the <entity id="P98-2182.7">category</entity> than would occur to an individual coding them by hand. Our algorithm finds many <entity id="P98-2182.8">terms</entity> not included within <entity id="P98-2182.9">Wordnet</entity> (many more than previous algorithms ), and could be viewed as an "enhancer" of existing <entity id="P98-2182.10">broad-coverage resources</entity>. "
</abstract>

</text>

<text id="W04-0833">
<title> Simple Features For Statistical Word Sense Disambiguation </title>
<abstract>
In this paper, we describe our experiments on <entity id="W04-0833.1">statistical word sense disambiguation (WSD)</entity> using two systems based on different approaches : <entity id="W04-0833.2">Naive Bayes</entity> on <entity id="W04-0833.3">word tokens</entity> and <entity id="W04-0833.4">Maximum Entropy</entity> on <entity id="W04-0833.5">local syntactic and semantic features</entity>. In the first approach, we consider a <entity id="W04-0833.6">context window</entity> and a <entity id="W04-0833.7">sub-window</entity> within it around the <entity id="W04-0833.8">word</entity> to disambiguate. Within the <entity id="W04-0833.9">outside window</entity>, only <entity id="W04-0833.10">content words</entity> are considered, but within the <entity id="W04-0833.11">sub-window</entity>, all <entity id="W04-0833.12">words</entity> are taken into account. Both <entity id="W04-0833.13">window sizes</entity> are tuned by the system for each <entity id="W04-0833.14">word</entity> to disambiguate and <entity id="W04-0833.15">accuracies of 75% and 67%</entity> were respectively obtained for <entity id="W04-0833.16">coarse and fine grained evaluations</entity>. In the second system, <entity id="W04-0833.17">sense resolution</entity> is done using an approximate <entity id="W04-0833.18">syntactic structure</entity> as well as <entity id="W04-0833.19">semantics</entity> of <entity id="W04-0833.20">neighboring nouns</entity> as <entity id="W04-0833.21">features</entity> to a <entity id="W04-0833.22">Maximum Entropy learner</entity>. <entity id="W04-0833.23">Accuracies of 70% and 63%</entity> were obtained for <entity id="W04-0833.24">coarse and fine grained evaluations</entity>.
</abstract>

</text>

<text id="W06-0707">
<title>DUC 2005: Evaluation Of Question-Focused Summarization Systems </title>
<abstract>
The <entity id="W06-0707.1">Document Understanding Conference (DUC) 2005 evaluation</entity> had a single <entity id="W06-0707.2">user-oriented, question-focused summarization task</entity>, which was to synthesize from a set of 25-50 <entity id="W06-0707.3">documents</entity> a well-organized, <entity id="W06-0707.4">fluent answer</entity> to a <entity id="W06-0707.5">complex question</entity>. The evaluation shows that the best <entity id="W06-0707.6">summarization systems</entity> have difficulty <entity id="W06-0707.7">extracting relevant sentences</entity> in response to <entity id="W06-0707.8">complex questions</entity> (as opposed to <entity id="W06-0707.9">representative sentences</entity> that might be appropriate to a <entity id="W06-0707.10">generic summary</entity> ). The relatively generous allowance of 250 <entity id="W06-0707.11">words</entity> for each <entity id="W06-0707.12">answer</entity> also reveals how difficult it is for current <entity id="W06-0707.13">summarization systems</entity> to produce <entity id="W06-0707.14">fluent text</entity> from <entity id="W06-0707.15">multiple documents</entity>.
</abstract>

</text>

<text id="M98-1001">
<title> Overview Of MUC-7</title>
<abstract>
The tasks performed by the systems participating in the <entity id="M98-1001.1">seventh Message Understanding Conference</entity> and the <entity id="M98-1001.2">Second Multilingual Entity Task</entity> are described here in general terms with examples.
</abstract>

</text>

<text id="W07-1420">
<title> Textual Entailment Through Extended Lexical Overlap and Lexico-Semantic Matching</title>
<abstract>
This paper presents two systems for <entity id="W07-1420.1">textual entailment</entity>, both employing <entity id="W07-1420.2">decision trees</entity> as a <entity id="W07-1420.3">supervised learning algorithm</entity>. The first one is based primarily on the concept of <entity id="W07-1420.4">lexical overlap</entity>, considering a <entity id="W07-1420.5">bag of words similarity overlap measure</entity> to form a <entity id="W07-1420.6">mapping</entity> of <entity id="W07-1420.7">terms</entity> in the <entity id="W07-1420.8">hypothesis</entity> to the <entity id="W07-1420.9">source text</entity>. The second system is a <entity id="W07-1420.10">lexico-semantic matching</entity> between the <entity id="W07-1420.11">text</entity> and the <entity id="W07-1420.12">hypothesis</entity> that attempts an alignment between <entity id="W07-1420.13">chunks</entity> in the <entity id="W07-1420.14">hypothesis</entity> and <entity id="W07-1420.15">chunks</entity> in the <entity id="W07-1420.16">text</entity>, and a representation of the <entity id="W07-1420.17">text</entity> and <entity id="W07-1420.18">hypothesis</entity> as two <entity id="W07-1420.19">dependency graphs</entity>. Their performances are compared and their positive and negative aspects are analyzed.
</abstract>

</text>

<text id="N07-2020">
<title>ILR-Based MT Comprehension Test with Multi-Level Questions </title>
<abstract>
We present results from a new <entity id="N07-2020.1">Interagency Language Roundtable (ILR) based comprehension test</entity>. This new <entity id="N07-2020.2">test design</entity> presents <entity id="N07-2020.3">questions</entity> at <entity id="N07-2020.4">multiple ILR difficulty levels</entity> within each <entity id="N07-2020.5">document</entity>. We incorporated <entity id="N07-2020.6">Arabic machine translation (MT) output</entity> from three independent <entity id="N07-2020.7">research sites</entity>, arbitrarily merging these materials into one <entity id="N07-2020.8">MT condition</entity>. We contrast the <entity id="N07-2020.9">MT condition</entity>, for both <entity id="N07-2020.10">text</entity> and <entity id="N07-2020.11">audio data</entity> types, with high quality <entity id="N07-2020.12">human reference Gold Standard (GS) translations</entity>. Overall, subjects achieved <entity id="N07-2020.13">95% comprehension</entity> for <entity id="N07-2020.14">GS</entity> and <entity id="N07-2020.15">74%</entity> for <entity id="N07-2020.16">MT</entity>, across 4 <entity id="N07-2020.17">genres</entity> and 3 <entity id="N07-2020.18">difficulty levels</entity>. Surprisingly, <entity id="N07-2020.19">comprehension rates</entity> do not correlate highly with <entity id="N07-2020.20">translation error rates</entity>, suggesting that we are measuring an additional dimension of <entity id="N07-2020.21">MT quality</entity>. We observed that it takes 15% more time overall to read <entity id="N07-2020.22">MT</entity> than <entity id="N07-2020.23">GS</entity>.
</abstract>

</text>

<text id="P08-2037">
<title> Event Matching Using the Transitive Closure of Dependency Relations</title>
<abstract>
This paper describes a novel <entity id="P08-2037.1">event-matching strategy</entity> using <entity id="P08-2037.2">features</entity> obtained from the <entity id="P08-2037.3">transitive closure</entity> of <entity id="P08-2037.4">dependency relations</entity>. The method yields a <entity id="P08-2037.5">model</entity> capable of <entity id="P08-2037.6">matching events</entity> with an <entity id="P08-2037.7">F-measure</entity> of 66.5%.
</abstract>

</text>

<text id="W08-1504">
<title>Economical Global Access to a VoiceXML Gateway Using Open Source Technologies</title>
<abstract>
<entity id="W08-1504.1">Voice over IP</entity> and the <entity id="W08-1504.2">open source technologies</entity> are becoming popular choices for <entity id="W08-1504.3">organizations</entity>. However, while accessing the <entity id="W08-1504.4">VoiceXML gateways</entity> these systems fail to attract the global <entity id="W08-1504.5">users</entity> economically. The objective of this paper is to demonstrate how an existing <entity id="W08-1504.6">web application</entity> can be modified using <entity id="W08-1504.7">VoiceXML</entity> to enable <entity id="W08-1504.8">non-visual access</entity> from any <entity id="W08-1504.9">phone</entity>. Moreover, we unleash a way for linking an existing <entity id="W08-1504.10">PSTN-based phone line</entity> to a <entity id="W08-1504.11">VoiceXML gateway</entity> even though the <entity id="W08-1504.12">voice service provider (VSP)</entity> does not provide a <entity id="W08-1504.13">local geographical number</entity> to global <entity id="W08-1504.14">customers</entity> to access the application. In addition, we introduce an economical way for <entity id="W08-1504.15">small sized businesses</entity> to overcome the high cost of setting up and using a commercial <entity id="W08-1504.16">VoiceXML gateway</entity>. The method is based on <entity id="W08-1504.17">Asterisk server</entity>. In order to elucidate the entire process, we present a sample <entity id="W08-1504.18">Package Tracking System application</entity>, which is based on an existing <entity id="W08-1504.19">website</entity> and provides the same functionality as the <entity id="W08-1504.20">website</entity> does. We also present an <entity id="W08-1504.21">online demonstration</entity>, which provides global access to commercial <entity id="W08-1504.22">voice platforms</entity> (i.e. <entity id="W08-1504.23">Voxeo</entity>, <entity id="W08-1504.24">Tellme Studio</entity>, <entity id="W08-1504.25">Bevocal</entity> and <entity id="W08-1504.26">DemandVoice</entity>). This paper also discusses various scenarios in which <entity id="W08-1504.27">spoken interaction</entity> can play a significant role.
</abstract>

</text>

<text id="P05-1011">
<title>Probabilistic Disambiguation Models For Wide-Coverage HPSG Parsing </title>
<abstract>
This paper reports the development of <entity id="P05-1011.1">log-linear models</entity> for the <entity id="P05-1011.2">disambiguation in wide-coverage HPSG parsing</entity>. The estimation of <entity id="P05-1011.3">log-linear models</entity> requires high <entity id="P05-1011.4">computational cost</entity>, especially with <entity id="P05-1011.5">wide-coverage grammars</entity>. Using techniques to reduce the <entity id="P05-1011.6">estimation cost</entity>, we trained the models using 20 sections of <entity id="P05-1011.7">Penn Tree-bank</entity>. A series of experiments empirically evaluated the <entity id="P05-1011.8">estimation techniques</entity>, and also examined the performance of the <entity id="P05-1011.9">disambiguation models</entity> on the <entity id="P05-1011.10">parsing</entity> of real-world <entity id="P05-1011.11">sentences</entity>.
</abstract>

</text>

<text id="P02-1021">
<title>Semi-Supervised Maximum Entropy Based Approach To Acronym And Abbreviation Normalization In Medical Texts </title>
<abstract> <entity id="P02-1021.1">Text normalization</entity> is an important aspect of successful <entity id="P02-1021.2">information retrieval</entity> from <entity id="P02-1021.3">medical documents</entity> such as <entity id="P02-1021.4">clinical notes</entity>, <entity id="P02-1021.5">radiology reports</entity> and <entity id="P02-1021.6">discharge summaries</entity>. In the <entity id="P02-1021.7">medical domain</entity>, a significant part of the general problem of <entity id="P02-1021.8">text normalization</entity> is <entity id="P02-1021.9">abbreviation and acronym disambiguation</entity>. Numerous <entity id="P02-1021.10">abbreviations</entity> are used routinely throughout such <entity id="P02-1021.11">texts</entity> and knowing their meaning is critical to <entity id="P02-1021.12">data retrieval</entity> from the <entity id="P02-1021.13">document</entity>. In this paper I will demonstrate a method of <entity id="P02-1021.14">automatically generating training data</entity> for <entity id="P02-1021.15">Maximum Entropy (ME) modeling</entity> of <entity id="P02-1021.16">abbreviations</entity> and <entity id="P02-1021.17">acronyms</entity> and will show that using <entity id="P02-1021.18">ME modeling</entity> is a promising technique for <entity id="P02-1021.19">abbreviation and acronym normalization</entity>. I report on the results of an experiment involving training a number of <entity id="P02-1021.20">ME models</entity> used to normalize <entity id="P02-1021.21">abbreviations</entity> and <entity id="P02-1021.22">acronyms</entity> on a sample of 10,000 <entity id="P02-1021.23">rheumatology notes</entity> with ~89% <entity id="P02-1021.24">accuracy</entity>.
</abstract>

</text>

<text id="C92-2090">
<title>From Cogram To Alcogram: Toward A Controlled English Grammar Checker</title>
<abstract>
In this paper we describe the roots of <entity id="C92-2090.1">Controlled English (CE)</entity>, the analysis of several existing <entity id="C92-2090.2">CE grammars</entity>, the development of a well-founded <entity id="C92-2090.3">150-rule CE grammar (COGRAM)</entity>, the elaboration of an <entity id="C92-2090.4">algorithmic variant (ALCOGRAM)</entity> as a basis for <entity id="C92-2090.5">NLP applications</entity>, the use of <entity id="C92-2090.6">ALCOGRAM</entity> in a <entity id="C92-2090.7">CA1 program</entity> teaching <entity id="C92-2090.8">writers</entity> how to use it effectively, and the preparatory study into a <entity id="C92-2090.9">Controlled English grammar</entity> and <entity id="C92-2090.10">style checker</entity> within a <entity id="C92-2090.11">desktop publishing (DTP) environment</entity>.
</abstract>

</text>

<text id="D08-1091">
<title>Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing </title>
<abstract>
We present a <entity id="D08-1091.1">discriminative, latent variable approach</entity> to <entity id="D08-1091.2">syntactic parsing</entity> in which <entity id="D08-1091.3">rules</entity> exist at multiple scales of <entity id="D08-1091.4">refinement</entity>. The model is formally a <entity id="D08-1091.5">latent variable CRF grammar</entity> over <entity id="D08-1091.6">trees</entity>, learned by iteratively splitting <entity id="D08-1091.7">grammar productions</entity> (not <entity id="D08-1091.8">categories</entity> ). Different regions of the <entity id="D08-1091.9">grammar</entity> are refined to different degrees, yielding <entity id="D08-1091.10">grammars</entity> which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the <entity id="D08-1091.11">split-and-merge grammars</entity> of Petrov et al. (2006). In addition, our <entity id="D08-1091.12">discriminative approach</entity> integrally admits <entity id="D08-1091.13">features</entity> beyond <entity id="D08-1091.14">local tree configurations</entity>. We present a <entity id="D08-1091.15">multi-scale training method</entity> along with an efficient <entity id="D08-1091.16">CKY-style dynamic program</entity>. On a variety of <entity id="D08-1091.17">domains</entity> and <entity id="D08-1091.18">languages</entity>, this <entity id="D08-1091.19">method</entity> produces the best published <entity id="D08-1091.20">parsing accuracies</entity> with the smallest reported <entity id="D08-1091.21">grammars</entity>.
</abstract>

</text>

<text id="C02-2028">
<title> Chinese Syntactic Parsing Based On Extended GLR Parsing Algorithm With PCFG</title>
<abstract>
This paper presents an <entity id="C02-2028.1">extended GLR parsing algorithm</entity> with <entity id="C02-2028.2">grammar PCFG*</entity> that is based on <entity id="C02-2028.3">Tomita's GLR parsing algorithm</entity> and extends it further. We also define a new <entity id="C02-2028.4">grammar PCFG*</entity> that is based on <entity id="C02-2028.5">PCFG</entity> and assigns not only <entity id="C02-2028.6">probability</entity> but also <entity id="C02-2028.7">frequency</entity> associated with each <entity id="C02-2028.8">rule</entity>. So our <entity id="C02-2028.9">syntactic parsing system</entity> is implemented based on <entity id="C02-2028.10">rule-based approach</entity> and <entity id="C02-2028.11">statistics approach</entity>. Furthermore our experiments are executed in two fields : <entity id="C02-2028.12">Chinese base noun phrase identification</entity> and <entity id="C02-2028.13">full syntactic parsing</entity>. And the results of these two fields are compared from three ways. The experiments prove that the <entity id="C02-2028.14">extended GLR parsing algorithm with PCFG*</entity> is an efficient <entity id="C02-2028.15">parsing method</entity> and a straightforward way to combine <entity id="C02-2028.16">statistical property</entity> with <entity id="C02-2028.17">rules</entity>. The experiment results of these two fields are presented in this paper.
</abstract>

</text>

<text id="E89-1004">
<title> Dialog Control In A Natural Language System </title>
<abstract>
In this paper a method for <entity id="E89-1004.1">controlling the dialog</entity> in a <entity id="E89-1004.2">natural language (NL) system</entity> is presented. It provides a deep <entity id="E89-1004.3">modeling</entity> of <entity id="E89-1004.4">information processing</entity> based on <entity id="E89-1004.5">time dependent propositional attitudes</entity> of the <entity id="E89-1004.6">interacting agents</entity>. <entity id="E89-1004.7">Knowledge</entity> about the <entity id="E89-1004.8">state</entity> of the <entity id="E89-1004.9">dialog</entity> is represented in a dedicated <entity id="E89-1004.10">language</entity> and changes of this <entity id="E89-1004.11">state</entity> are described by a compact set of <entity id="E89-1004.12">rules</entity>. An appropriate organization of <entity id="E89-1004.13">rule application</entity> is introduced including the <entity id="E89-1004.14">initiation</entity> of an <entity id="E89-1004.15">adequate system reaction</entity>. Finally the application of the method in an <entity id="E89-1004.16">NL consultation system</entity> is outlined.
</abstract>

</text>

<text id="L08-1539">
<title>Designing and Evaluating a Russian Tagset</title>
<abstract>
This paper reports the principles behind designing a <entity id="L08-1539.1">tagset</entity> to cover <entity id="L08-1539.2">Russian morphosyntactic phenomena</entity>, modifications of the <entity id="L08-1539.3">core tagset</entity>, and its evaluation. The <entity id="L08-1539.4">tagset</entity> and associated <entity id="L08-1539.5">morphosyntactic specifications</entity> are based on the <entity id="L08-1539.6">MULTEXT-East framework</entity>, while the decisions in designing it were aimed at achieving a balance between parameters important for <entity id="L08-1539.7">linguists</entity> and the possibility to detect and disambiguate them automatically. The final <entity id="L08-1539.8">tagset</entity> contains about 600 <entity id="L08-1539.9">tags</entity> and achieves about <entity id="L08-1539.10">95% accuracy</entity> on the disambiguated portion of the <entity id="L08-1539.11">Russian National Corpus</entity>. We have also produced a <entity id="L08-1539.12">test set</entity> of <entity id="L08-1539.13">tagging models and corpora</entity> that can be shared with other researchers.
</abstract>

</text>

<text id="E03-1023">
<title>Effect Of Cross-Language IR In Bilingual Lexicon Acquisition From Comparable Corpora</title>
<abstract>
Within the framework of <entity id="E03-1023.1">translation knowledge acquisition</entity> from <entity id="E03-1023.2">WWW news sites</entity>, this paper studies issues on the effect of <entity id="E03-1023.3">cross-language retrieval</entity> of relevant <entity id="E03-1023.4">texts</entity> in <entity id="E03-1023.5">bilingual lexicon acquisition</entity> from <entity id="E03-1023.6">comparable corpora</entity>. We experimentally show that it is quite effective to reduce the <entity id="E03-1023.7">candidate bilingual term pairs</entity> against which <entity id="E03-1023.8">bilingual term correspondences</entity> are estimated, in terms of both <entity id="E03-1023.9">computational complexity</entity> and the performance of precise <entity id="E03-1023.10">estimation</entity> of <entity id="E03-1023.11">bilingual term correspondences</entity>.
</abstract>

</text>

<text id="N06-1050">
<title>Creating A Test Collection For Citation- Based IR Experiments</title>
<abstract>
We present an approach to building a <entity id="N06-1050.1">test collection</entity> of <entity id="N06-1050.2">research papers</entity>. The approach is based on the <entity id="N06-1050.3">Cranfield 2 tests</entity> but uses as its vehicle a current conference; <entity id="N06-1050.4">research questions</entity> and <entity id="N06-1050.5">relevance judgements</entity> of all <entity id="N06-1050.6">cited papers</entity> are elicited from conference authors. The resultant <entity id="N06-1050.7">test collection</entity> is different from <entity id="N06-1050.8">TREC's</entity> in that it comprises <entity id="N06-1050.9">scientific articles</entity> rather than <entity id="N06-1050.10">newspaper text</entity> and, thus, allows for <entity id="N06-1050.11">IR experiments</entity> that include <entity id="N06-1050.12">citation information</entity>. The <entity id="N06-1050.13">test collection</entity> currently consists of 170 <entity id="N06-1050.14">queries</entity> with <entity id="N06-1050.15">relevance judgements</entity>; the <entity id="N06-1050.16">document collection</entity> is the <entity id="N06-1050.17">ACL Anthology</entity>. We describe properties of our <entity id="N06-1050.18">queries</entity> and <entity id="N06-1050.19">relevance judgements</entity>, and demonstrate the use of the <entity id="N06-1050.20">test collection</entity> in an experimental setup. One potentially problematic property of our <entity id="N06-1050.21">collection</entity> is that <entity id="N06-1050.22">queries</entity> have a low number of <entity id="N06-1050.23">relevant documents</entity>; we discuss ways of alleviating this.
</abstract>

</text>

<text id="E95-1001">
<title>On Reasoning With Ambiguities</title>
<abstract>
The paper adresses the problem of reasoning with <entity id="E95-1001.1">ambiguities</entity>. <entity id="E95-1001.2">Semantic representations</entity> are presented that leave <entity id="E95-1001.3">scope relations</entity> between <entity id="E95-1001.4">quantifiers</entity> and/or other <entity id="E95-1001.5">operators</entity> unspecified. <entity id="E95-1001.6">Truth conditions</entity> are provided for these <entity id="E95-1001.7">representations</entity> and different <entity id="E95-1001.8">consequence relations</entity> are judged on the basis of intuitive <entity id="E95-1001.9">correctness</entity>. Finally <entity id="E95-1001.10">inference patterns</entity> are presented that operate directly on these <entity id="E95-1001.11">underspecified structures</entity>, i.e. do not rely on any <entity id="E95-1001.12">translation</entity> into the set of their <entity id="E95-1001.13">disambiguations</entity>.
</abstract>

</text>

<text id="H94-1035">
<title>Information based Intonation Synthesis</title>
<abstract>
This paper presents a model for generating <entity id="H94-1035.1">prosodically appropriate synthesized responses</entity> to <entity id="H94-1035.2">database queries</entity> using <entity id="H94-1035.3">Combinatory Categorial Grammar</entity> (CCG - cf. [22]), a <entity id="H94-1035.4">formalism</entity> which easily integrates the notions of <entity id="H94-1035.5">syntactic constituency</entity>, <entity id="H94-1035.6">prosodie phrasing</entity> and <entity id="H94-1035.7">information structure</entity>. The <entity id="H94-1035.8">model</entity> determines <entity id="H94-1035.9">accent locations</entity> within <entity id="H94-1035.10">phrases</entity> on the basis of <entity id="H94-1035.11">contrastive sets</entity> derived from the <entity id="H94-1035.12">discourse structure</entity> and a <entity id="H94-1035.13">domain-independent knowledge base</entity>.
</abstract>

</text>

<text id="X98-1010">
<title>Coreference Resolution Strategies From An Application Perspective</title>
<abstract>
As part of our <entity id="X98-1010.1">TIPSTER III research program</entity>, we have continued our research into strategies to resolve <entity id="X98-1010.2">coreferences</entity> within a <entity id="X98-1010.3">free text document</entity>; this research was begun during our <entity id="X98-1010.4">TIPSTER II research program</entity>. In the <entity id="X98-1010.5">TIPSTER II Proceedings paper</entity>, "An Evaluation of <entity id="X98-1010.6">Coreference Resolution Strategies</entity> for <entity id="X98-1010.7">Acquiring Associated Information</entity>", the goal was to evaluate the contributions of various techniques for associating an <entity id="X98-1010.8">entity</entity> with three types of <entity id="X98-1010.9">information</entity>: 1) <entity id="X98-1010.10">name variations</entity>, 2) <entity id="X98-1010.11">descriptive phrases</entity>, and 3) <entity id="X98-1010.12">location information</entity>. This paper discusses the evolution of the <entity id="X98-1010.13">coreference resolution techniques</entity> of the <entity id="X98-1010.14">NLToolset</entity>, as they have been applied to an <entity id="X98-1010.15">information extraction application</entity>, similar to the <entity id="X98-1010.16">MUC Scenario Template task</entity>. Development of this application motivated new <entity id="X98-1010.17">coreference resolution algorithms</entity> which were specific to the type of <entity id="X98-1010.18">entity</entity> being handled. It also has raised the importance of understanding the <entity id="X98-1010.19">structure</entity> of a <entity id="X98-1010.20">document</entity> in order to guide the <entity id="X98-1010.21">coreference resolution process</entity>. In the following paper, Section 2 discusses <entity id="X98-1010.22">entity related coreference resolution techniques</entity> and Section 3, the relevance of <entity id="X98-1010.23">document zoning</entity>. Section 4 concludes with a discussion of future work, which will include <entity id="X98-1010.24">location merging</entity>, <entity id="X98-1010.25">event coreference resolution</entity>, and <entity id="X98-1010.26">event merging</entity>.
</abstract>

</text>

<text id="W97-0616">
<title>How To Obey The 7 Commandments For Spoken Dialogue?</title>
<abstract>
We describe the design and implementation of the <entity id="W97-0616.1">dialogue management module</entity> in a <entity id="W97-0616.2">voice operated car-driver information system</entity>. The <entity id="W97-0616.3">literature</entity> on designing 'good' <entity id="W97-0616.4">user interfaces</entity> involving <entity id="W97-0616.5">natural language dialogue</entity> in general and <entity id="W97-0616.6">speech</entity> in particular is abundant with useful guidelines for actual development. We have tried to summarize these guidelines in 7 <entity id="W97-0616.7">'meta-guidelines', or commandments</entity>. Even though state-of-the-art <entity id="W97-0616.8">Speech Recognition modules</entity> perform well, <entity id="W97-0616.9">speech recognition errors</entity> cannot be precluded. For the current application, the fact that the car is an <entity id="W97-0616.10">acoustically hostile environment</entity> is an extra complication. This means that special attention should be paid to effective <entity id="W97-0616.11">methods</entity> to compensate for <entity id="W97-0616.12">speech recognition errors</entity>. Moreover, this should be done in a way which is not disturbing for the driver. In this paper, we show how these <entity id="W97-0616.13">constraints</entity> influence the design and subsequent implementation of the <entity id="W97-0616.14">Dialogue Manager module</entity>, and how the additional requirements fit in with the 7 <entity id="W97-0616.15">commandments</entity>.
</abstract>

</text>

<text id="H05-1022">
<title>HMM Word And Phrase Alignment For Statistical Machine Translation</title>
<abstract><entity id="H05-1022.1">HMM-based models</entity> are developed for the <entity id="H05-1022.2">alignment</entity> of <entity id="H05-1022.3">words</entity> and <entity id="H05-1022.4">phrases</entity> in <entity id="H05-1022.5">bitext</entity>. The <entity id="H05-1022.6">models</entity> are formulated so that <entity id="H05-1022.7">alignment</entity> and <entity id="H05-1022.8">parameter estimation</entity> can be performed efficiently. We find that <entity id="H05-1022.9">Chinese-English word alignment performance</entity> is comparable to that of <entity id="H05-1022.10">IBM Model-4</entity> even over large <entity id="H05-1022.11">training bitexts</entity>. <entity id="H05-1022.12">Phrase pairs</entity> extracted from <entity id="H05-1022.13">word alignments</entity> generated under the <entity id="H05-1022.14">model</entity> can also be used for <entity id="H05-1022.15">phrase-based translation</entity>, and in <entity id="H05-1022.16">Chinese to English and Arabic to English translation</entity>, <entity id="H05-1022.17">performance</entity> is comparable to <entity id="H05-1022.18">systems</entity> based on <entity id="H05-1022.19">Model-4 alignments</entity>. <entity id="H05-1022.20">Direct phrase pair induction</entity> under the <entity id="H05-1022.21">model</entity> is described and shown to improve <entity id="H05-1022.22">translation performance</entity>.
</abstract>

</text>

<text id="W00-1313">
<title>Query Translation In Chinese-English Cross-Language Information Retrieval</title>
<abstract>
This paper proposed a new <entity id="W00-1313.1">query translation method</entity> based on the <entity id="W00-1313.2">mutual information matrices</entity> of <entity id="W00-1313.3">terms</entity> in the <entity id="W00-1313.4">Chinese and English corpora</entity>. Instead of looking up a <entity id="W00-1313.5">bilingual phrase dictionary</entity>, the <entity id="W00-1313.6">compositional phrase</entity> (the <entity id="W00-1313.7">translation</entity> of <entity id="W00-1313.8">phrase</entity> can be derived from the <entity id="W00-1313.9">translation</entity> of its <entity id="W00-1313.10">components</entity>) in the <entity id="W00-1313.11">query</entity> can be indirectly translated via a <entity id="W00-1313.12">general-purpose Chinese- English dictionary look-up procedure</entity>. A novel <entity id="W00-1313.13">selection method</entity> for <entity id="W00-1313.14">translations</entity> of <entity id="W00-1313.15">query terms</entity> is also presented in detail. Our <entity id="W00-1313.16">query translation method</entity> ultimately constructs an <entity id="W00-1313.17">English query</entity> in which each <entity id="W00-1313.18">query term</entity> has a <entity id="W00-1313.19">weight</entity>. The evaluation results show that the <entity id="W00-1313.20">retrieval performance</entity> achieved by our <entity id="W00-1313.21">query translation method</entity> is about 73% of <entity id="W00-1313.22">monolingual information retrieval</entity> and is about 28% higher than that of simple <entity id="W00-1313.23">word-by-word translation</entity> way.
</abstract>

</text>

<text id="W00-0106">
<title>Analyzing The Reading Comprehension Task</title>
<abstract>
In this paper we describe a method for analyzing the <entity id="W00-0106.1">reading comprehension task</entity>. First, we describe a method of classifying <entity id="W00-0106.2">facts</entity> (information) into categories or <entity id="W00-0106.3">levels</entity>; where each <entity id="W00-0106.4">level</entity> signifies a different <entity id="W00-0106.5">degree of difficulty</entity> of extracting a <entity id="W00-0106.6">fact</entity> from a piece of <entity id="W00-0106.7">text</entity> containing it. We then proceed to show how one can use this <entity id="W00-0106.8">model</entity> the analyze the <entity id="W00-0106.9">complexity</entity> of the <entity id="W00-0106.10">reading comprehension task</entity>. Finally, we analyze five different <entity id="W00-0106.11">reading comprehension tasks</entity> and present results from this <entity id="W00-0106.12">analysis</entity>.
</abstract>

</text>

<text id="J00-4001">
<title>Automatic Text Categorization In Terms Of Genre And Author</title>
<abstract>
The two main factors that characterize a <entity id="J00-4001.1">text</entity> are its <entity id="J00-4001.2">content</entity> and its <entity id="J00-4001.3">style</entity>, and both can be used as a means of <entity id="J00-4001.4">categorization</entity>. In this paper we present an approach to <entity id="J00-4001.5">text categorization</entity> in terms of <entity id="J00-4001.6">genre</entity> and author for <entity id="J00-4001.7">Modern Greek</entity>. In contrast to previous <entity id="J00-4001.8">stylometric approaches</entity>, we attempt to take full advantage of existing <entity id="J00-4001.9">natural language processing (NLP) tools</entity>. To this end, we propose a set of <entity id="J00-4001.10">style markers</entity> including <entity id="J00-4001.11">analysis-level measures</entity> that represent the way in which the <entity id="J00-4001.12">input text</entity> has been analyzed and capture useful <entity id="J00-4001.13">stylistic information</entity> without additional cost. We present a set of small-scale but reasonable experiments in <entity id="J00-4001.14">text genre detection</entity>, <entity id="J00-4001.15">author identification</entity>, and <entity id="J00-4001.16">author verification</entity> tasks and show that the <entity id="J00-4001.17">proposed method</entity> performs better than the most popular <entity id="J00-4001.18">distributional lexical measures</entity>, i.e., functions of <entity id="J00-4001.19">vocabulary richness</entity> and <entity id="J00-4001.20">frequencies of occurrence</entity> of the most frequent <entity id="J00-4001.21">words</entity>. All the presented experiments are based on <entity id="J00-4001.22">unrestricted text</entity> downloaded from the <entity id="J00-4001.23">World Wide Web</entity> without any <entity id="J00-4001.24">manual text preprocessing</entity> or <entity id="J00-4001.25">text sampling</entity>. Various performance issues regarding the <entity id="J00-4001.26">training set</entity> size and the significance of the proposed <entity id="J00-4001.27">style markers</entity> are discussed. Our <entity id="J00-4001.28">system</entity> can be used in any <entity id="J00-4001.29">application</entity> that requires fast and easily adaptable <entity id="J00-4001.30">text categorization</entity> in terms of <entity id="J00-4001.31">stylistically homogeneous categories</entity>. Moreover, the procedure of defining <entity id="J00-4001.32">analysis-level markers</entity> can be followed in order to extract useful <entity id="J00-4001.33">stylistic information</entity> using existing <entity id="J00-4001.34">text processing tools</entity>.
</abstract>

</text>

<text id="I08-1019">
<title>Identifying Cross-Document Relations between Sentences</title>
<abstract>
A pair of <entity id="I08-1019.1">sentences</entity> in different <entity id="I08-1019.2">newspaper articles</entity> on an <entity id="I08-1019.3">event</entity> can have one of several <entity id="I08-1019.4">relations</entity>. Of these, we have focused on two, i.e., <entity id="I08-1019.5">equivalence</entity> and <entity id="I08-1019.6">transition</entity>. <entity id="I08-1019.7">Equivalence</entity> is the <entity id="I08-1019.8">relation</entity> between two <entity id="I08-1019.9">sentences</entity> that have the same <entity id="I08-1019.10">information</entity> on an <entity id="I08-1019.11">event</entity>. <entity id="I08-1019.12">Transition</entity> is the <entity id="I08-1019.13">relation</entity> between two <entity id="I08-1019.14">sentences</entity> that have the same <entity id="I08-1019.15">information</entity> except for values of <entity id="I08-1019.16">numeric attributes</entity>. We propose methods of identifying these <entity id="I08-1019.17">relations</entity>. We first split a <entity id="I08-1019.18">dataset</entity> consisting of pairs of <entity id="I08-1019.19">sentences</entity> into <entity id="I08-1019.20">clusters</entity> according to their similarities, and then construct a <entity id="I08-1019.21">classifier</entity> for each <entity id="I08-1019.22">cluster</entity> to identify <entity id="I08-1019.23">equivalence relations</entity>. We also adopt a <entity id="I08-1019.24">"coarse-to-fine" approach</entity>. We further propose using the identified <entity id="I08-1019.25">equivalence relations</entity> to address the task of identifying <entity id="I08-1019.26">transition relations</entity>.
</abstract>

</text>

<text id="W05-1010">
<title>Automatic Acquisition Of Bilingual Rules For Extraction Of Bilingual Word Pairs From Parallel Corpora</title>
<abstract>
In this paper, we propose a new <entity id="W05-1010.1">learning method</entity> to solve the <entity id="W05-1010.2">sparse data problem</entity> in <entity id="W05-1010.3">automatic extraction</entity> of <entity id="W05-1010.4">bilingual word pairs</entity> from <entity id="W05-1010.5">parallel corpora</entity> with various <entity id="W05-1010.6">languages</entity>. Our <entity id="W05-1010.7">learning method</entity> automatically acquires <entity id="W05-1010.8">rules</entity>, which are effective to solve the <entity id="W05-1010.9">sparse data problem</entity>, only from <entity id="W05-1010.10">parallel corpora</entity> without any <entity id="W05-1010.11">bilingual resource</entity> (e.g., a <entity id="W05-1010.12">bilingual dictionary</entity>, <entity id="W05-1010.13">machine translation systems</entity>) beforehand. We call this method <entity id="W05-1010.14">Inductive Chain Learning (ICL)</entity>. The <entity id="W05-1010.15">ICL</entity> can limit the <entity id="W05-1010.16">search scope</entity> for the <entity id="W05-1010.17">decision</entity> of <entity id="W05-1010.18">equivalents</entity>. Using <entity id="W05-1010.19">ICL</entity>, the <entity id="W05-1010.20">recall</entity> in three <entity id="W05-1010.21">systems</entity> based on <entity id="W05-1010.22">similarity measures</entity> improved respectively 8.0, 6.1 and 6.0 percentage points. In addition, the <entity id="W05-1010.23">recall value</entity> of <entity id="W05-1010.24">GIZA++</entity> improved 6.6 percentage points using <entity id="W05-1010.25">ICL</entity>.
</abstract>

</text>

<text id="W06-1604">
<title>Detecting Parser Errors Using Web-based Semantic Filters</title>
<abstract><entity id="W06-1604.1">NLP systems</entity> for tasks such as <entity id="W06-1604.2">question answering</entity> and <entity id="W06-1604.3">information extraction</entity> typically rely on <entity id="W06-1604.4">statistical parsers</entity>. But the efficacy of such <entity id="W06-1604.5">parsers</entity> can be surprisingly low, particularly for <entity id="W06-1604.6">sentences</entity> drawn from <entity id="W06-1604.7">heterogeneous corpora</entity> such as the <entity id="W06-1604.8">Web</entity>. We have observed that <entity id="W06-1604.9">incorrect parses</entity> often result in wildly implausible <entity id="W06-1604.10">semantic interpretations</entity> of <entity id="W06-1604.11">sentences</entity>, which can be detected automatically using <entity id="W06-1604.12">semantic information</entity> obtained from the <entity id="W06-1604.13">Web</entity>. Based on this observation, we introduce <entity id="W06-1604.14">Web-based semantic filtering</entity> â€” a novel, domain-independent method for automatically detecting and discarding <entity id="W06-1604.15">incorrect parses</entity>. We measure the effectiveness of our <entity id="W06-1604.16">filtering system</entity>, called <entity id="W06-1604.17">WOODWARD</entity>, on two <entity id="W06-1604.18">test collections</entity>. On a set of <entity id="W06-1604.19">TREC questions</entity>, it reduces error by 67%. On a set of more complex <entity id="W06-1604.20">Penn Treebank sentences</entity>, the <entity id="W06-1604.21">reduction in error rate</entity> was 20%.
</abstract>

</text>

<text id="W07-1210">
<title>Semantic Composition with (Robust) Minimal Recursion Semantics</title>
<abstract>
We discuss <entity id="W07-1210.1">semantic composition</entity> in <entity id="W07-1210.2">Minimal Recursion Semantics (MRS)</entity> and <entity id="W07-1210.3">Robust Minimal Recursion Semantics (RMRS)</entity>. We demonstrate that a previously defined <entity id="W07-1210.4">formal algebra</entity> applies to <entity id="W07-1210.5">grammar engineering</entity> across a much greater range of <entity id="W07-1210.6">frameworks</entity> than was originally envisaged. We show how this <entity id="W07-1210.7">algebra</entity> can be adapted to <entity id="W07-1210.8">composition</entity> in <entity id="W07-1210.9">grammar frameworks</entity> where a <entity id="W07-1210.10">lexicon</entity> is not assumed, and how this underlies a practical implementation of <entity id="W07-1210.11">semantic construction</entity> for the <entity id="W07-1210.12">RASP system</entity>.
</abstract>

</text>

<text id="D07-1073">
<title>Exploiting Wikipedia as External Knowledge for Named Entity Recognition</title>
<abstract>
We explore the use of <entity id="D07-1073.1">Wikipedia</entity> as <entity id="D07-1073.2">external knowledge</entity> to improve <entity id="D07-1073.3">named entity recognition (NER)</entity>. Our method retrieves the corresponding <entity id="D07-1073.4">Wikipedia entry</entity> for each <entity id="D07-1073.5">candidate word sequence</entity> and extracts a <entity id="D07-1073.6">category label</entity> from the first <entity id="D07-1073.7">sentence</entity> of the <entity id="D07-1073.8">entry</entity>, which can be thought of as a <entity id="D07-1073.9">definition part</entity>. These <entity id="D07-1073.10">category labels</entity> are used as <entity id="D07-1073.11">features</entity> in a <entity id="D07-1073.12">CRF-based NE tagger</entity>. We demonstrate using the <entity id="D07-1073.13">CoNLL 2003 dataset</entity> that the <entity id="D07-1073.14">Wikipedia category labels</entity> extracted by such a simple <entity id="D07-1073.15">method</entity> actually improve the accuracy of <entity id="D07-1073.16">NER</entity>.
</abstract>

</text>

<text id="P07-2053">
<title>Poster paper : HunPos - an open source trigram tagger</title>
<abstract>
In the world of <entity id="P07-2053.1">non-proprietary NLP software</entity> the standard, and perhaps the best, <entity id="P07-2053.2">HMM-based POS tagger</entity> is <entity id="P07-2053.3">TnT</entity> (Brants, 2000). We argue here that some of the criticism aimed at <entity id="P07-2053.4">HMM performance</entity> on <entity id="P07-2053.5">languages with rich morphology</entity> should more properly be directed at <entity id="P07-2053.6">TnT's peculiar license</entity>, free but not open source, since it is those details of the implementation which are hidden from the user that hold the key for improved <entity id="P07-2053.7">POS tagging</entity> across a wider variety of <entity id="P07-2053.8">languages</entity>. We present <entity id="P07-2053.9">HunPos, a free and open source (LGPL-licensed) alternative</entity>, which can be tuned by the user to fully utilize the potential of <entity id="P07-2053.10">HMM architectures</entity>, offering <entity id="P07-2053.11">performance</entity> comparable to <entity id="P07-2053.12">more complex models</entity>, but preserving the ease and speed of the <entity id="P07-2053.13">training and tagging process</entity>.
</abstract>

</text>

<text id="C08-1027">
<title>Syntactic Reordering Integrated with Phrase-based SMT</title>
<abstract>
We present a novel approach to <entity id="C08-1027.1">word reordering</entity> which successfully integrates <entity id="C08-1027.2">syntactic structural knowledge</entity> with <entity id="C08-1027.3">phrase-based SMT</entity>. This is done by constructing a <entity id="C08-1027.4">lattice of alternatives</entity> based on automatically learned <entity id="C08-1027.5">probabilistic syntactic rules</entity>. In <entity id="C08-1027.6">decoding</entity>, the <entity id="C08-1027.7">alternatives</entity> are scored based on the <entity id="C08-1027.8">output word order</entity>, not the <entity id="C08-1027.9">order of the input</entity>. Unlike previous approaches, this makes it possible to successfully integrate <entity id="C08-1027.10">syntactic reordering</entity> with <entity id="C08-1027.11">phrase-based SMT</entity>. On an <entity id="C08-1027.12">English-Danish task</entity>, we achieve an absolute improvement in <entity id="C08-1027.13">translation quality</entity> of <entity id="C08-1027.14">1.1% BLEU</entity>. Manual evaluation supports the claim that the present <entity id="C08-1027.15">approach</entity> is significantly superior to previous <entity id="C08-1027.16">approaches</entity>.
</abstract>

</text>

<text id="P03-1060">
<title>A Syllable based Word Recognition Model For Korean Noun Extraction</title>
<abstract><entity id="P03-1060.1">Noun extraction</entity> is very important for many <entity id="P03-1060.2">NLP applications</entity> such as <entity id="P03-1060.3">information retrieval</entity>, <entity id="P03-1060.4">automatic text classification</entity>, and <entity id="P03-1060.5">information extraction</entity>. Most of the previous <entity id="P03-1060.6">Korean noun extraction systems</entity> use a <entity id="P03-1060.7">morphological analyzer</entity> or a <entity id="P03-1060.8">Part-of- Speech (POS) tagger</entity>. Therefore, they require much of the <entity id="P03-1060.9">linguistic knowledge</entity> such as <entity id="P03-1060.10">morpheme dictionaries and rules</entity> (e.g. <entity id="P03-1060.11">morphosyntactic rules</entity> and <entity id="P03-1060.12">morphological rules</entity>). This paper proposes a new <entity id="P03-1060.13">noun extraction method</entity> that uses the <entity id="P03-1060.14">syllable based word recognition model</entity>. It finds the most probable <entity id="P03-1060.15">syllable-tag sequence</entity> of the <entity id="P03-1060.16">input sentence</entity> by using <entity id="P03-1060.17">automatically acquired statistical information</entity> from the <entity id="P03-1060.18">POS tagged corpus</entity> and extracts <entity id="P03-1060.19">nouns</entity> by detecting <entity id="P03-1060.20">word boundaries</entity>. Furthermore, it does not require any labor for constructing and maintaining <entity id="P03-1060.21">linguistic knowledge</entity>. We have performed various experiments with a wide range of <entity id="P03-1060.22">variables</entity> influencing the performance. The experimental results show that without <entity id="P03-1060.23">morphological analysis</entity> or <entity id="P03-1060.24">POS tagging</entity>, the <entity id="P03-1060.25">proposed method</entity> achieves comparable <entity id="P03-1060.26">performance</entity> with the previous <entity id="P03-1060.27">methods</entity>.
</abstract>

</text>

<text id="P06-2112">
<title>Word Alignment For Languages With Scarce Resources Using Bilingual Corpora Of Other Language Pairs</title>
<abstract>
This paper proposes an approach to improve <entity id="P06-2112.1">word alignment</entity> for <entity id="P06-2112.2">languages with scarce resources</entity> using <entity id="P06-2112.3">bilingual corpora</entity> of other <entity id="P06-2112.4">language pairs</entity>. To perform <entity id="P06-2112.5">word alignment</entity> between <entity id="P06-2112.6">languages L1 and L2</entity>, we introduce a third <entity id="P06-2112.7">language L3</entity>. Although only small amounts of <entity id="P06-2112.8">bilingual data</entity> are available for the desired <entity id="P06-2112.9">language pair L1-L2</entity>, <entity id="P06-2112.10">large-scale bilingual corpora</entity> in <entity id="P06-2112.11">L1-L3</entity> and <entity id="P06-2112.12">L2-L3</entity> are available. Based on these two additional <entity id="P06-2112.13">corpora</entity> and with <entity id="P06-2112.14">L3</entity> as the <entity id="P06-2112.15">pivot language</entity>, we build a <entity id="P06-2112.16">word alignment model</entity> for <entity id="P06-2112.17">L1 and L2</entity>. This <entity id="P06-2112.18">approach</entity> can build a <entity id="P06-2112.19">word alignment model</entity> for two <entity id="P06-2112.20">languages</entity> even if no <entity id="P06-2112.21">bilingual corpus</entity> is available in this <entity id="P06-2112.22">language pair</entity>. In addition, we build another <entity id="P06-2112.23">word alignment model</entity> for <entity id="P06-2112.24">L1 and L2</entity> using the small <entity id="P06-2112.25">L1-L2 bilingual corpus</entity>. Then we interpolate the above two <entity id="P06-2112.26">models</entity> to further improve <entity id="P06-2112.27">word alignment</entity> between <entity id="P06-2112.28">L1 and L2</entity>. Experimental results indicate a <entity id="P06-2112.29">relative error rate reduction</entity> of 21.30% as compared with the <entity id="P06-2112.30">method</entity> only using the <entity id="P06-2112.31">small bilingual corpus</entity> in <entity id="P06-2112.32">L1 and L2</entity>.
</abstract>

</text>

<text id="C88-1052">
<title>Presuppositions As Beliefs</title>
<abstract>
Most <entity id="C88-1052.1">theories</entity> of <entity id="C88-1052.2">presupposition</entity> implicitly assume that <entity id="C88-1052.3">presuppositions</entity> are <entity id="C88-1052.4">facts</entity>, and that all <entity id="C88-1052.5">agents</entity> involved in a <entity id="C88-1052.6">discourse</entity> share <entity id="C88-1052.7">belief</entity> in the <entity id="C88-1052.8">presuppositions</entity> that it generates. These unrealistic assumptions can be eliminated if each <entity id="C88-1052.9">presupposition</entity> is treated as the <entity id="C88-1052.10">belief</entity> of an <entity id="C88-1052.11">agent</entity>. However, it is not enough to consider only the <entity id="C88-1052.12">beliefs</entity> of the <entity id="C88-1052.13">speaker</entity>; we show that the <entity id="C88-1052.14">beliefs</entity> of other <entity id="C88-1052.15">agents</entity> are often involved. We describe a new model, including an improved definition of <entity id="C88-1052.16">presupposition</entity>, that treats <entity id="C88-1052.17">presuppositions</entity> as <entity id="C88-1052.18">beliefs</entity> and considers the <entity id="C88-1052.19">beliefs</entity> of all <entity id="C88-1052.20">agents</entity> involved in the <entity id="C88-1052.21">discourse</entity>. We show that treating <entity id="C88-1052.22">presuppositions</entity> as <entity id="C88-1052.23">beliefs</entity> makes it possible to explain phenomena that cannot be explained otherwise.
</abstract>

</text>

<text id="C96-1009">
<title>Extracting Nested Collocations</title>
<abstract>
This paper provides an approach to the <entity id="C96-1009.1">semi-automatic extraction</entity> of <entity id="C96-1009.2">collocations</entity> from <entity id="C96-1009.3">corpora</entity> using statistics. The growing availability of large <entity id="C96-1009.4">textual corpora</entity>, and the increasing number of applications of <entity id="C96-1009.5">collocation extraction</entity>, has given rise to various approaches on the topic. In this paper, we address the problem of <entity id="C96-1009.6">nested collocations</entity>; that is, those being part of longer <entity id="C96-1009.7">collocations</entity>. Most approaches till now, treated <entity id="C96-1009.8">substring</entity> of <entity id="C96-1009.9">collocations</entity> as <entity id="C96-1009.10">collocations</entity> only if they appeared frequently enough by themselves in the <entity id="C96-1009.11">corpus</entity>. These techniques left a lot of <entity id="C96-1009.12">collocations</entity> unextracted. In this paper, we propose an algorithm for a <entity id="C96-1009.13">semi-automatic extraction</entity> of <entity id="C96-1009.14">nested uninterrupted and interrupted collocations</entity>, paying particular attention to <entity id="C96-1009.15">nested collocation</entity>.
</abstract>

</text>

<text id="E03-1072">
<title>The Role Of Initiative In Tutorial Dialogue</title>
<abstract>
This work is the first systematic investigation of <entity id="E03-1072.1">initiative</entity> in <entity id="E03-1072.2">human-human tutorial dialogue</entity>. We studied <entity id="E03-1072.3">initiative management</entity> in two <entity id="E03-1072.4">dialogue strategies</entity>: <entity id="E03-1072.5">didactic tutoring</entity> and <entity id="E03-1072.6">Socratic tutoring</entity>. We hypothesized that <entity id="E03-1072.7">didactic tutoring</entity> would be mostly <entity id="E03-1072.8">tutor-initiative</entity> while <entity id="E03-1072.9">Socratic tutoring</entity> would be <entity id="E03-1072.10">mixed-initiative</entity>, and that more <entity id="E03-1072.11">student initiative</entity> would lead to more <entity id="E03-1072.12">learning</entity> (i.e., task success for the tutor). Surprisingly, students had <entity id="E03-1072.13">initiative</entity> more of the time in the <entity id="E03-1072.14">didactic dialogues</entity> (21% of the turns) than in the <entity id="E03-1072.15">Socratic dialogues</entity> (10% of the turns), and there was no direct relationship between <entity id="E03-1072.16">student initiative</entity> and <entity id="E03-1072.17">learning</entity>. However, <entity id="E03-1072.18">Socratic dialogues</entity> were more interactive than <entity id="E03-1072.19">didactic dialogues</entity> as measured by percentage of <entity id="E03-1072.20">tutor utterances</entity> that were questions and percentage of <entity id="E03-1072.21">words</entity> in the <entity id="E03-1072.22">dialogue</entity> uttered by the student, and <entity id="E03-1072.23">interactivity</entity> had a positive correlation with <entity id="E03-1072.24">learning</entity>.
</abstract>

</text>

<text id="C04-1097">
<title>Linguistically Informed Statistical Models Of Constituent Structure For Ordering In Sentence Realization</title>
<abstract>
We present several <entity id="C04-1097.1">statistical models</entity> of <entity id="C04-1097.2">syntactic constituent order</entity> for <entity id="C04-1097.3">sentence realization</entity>. We compare several models, including simple <entity id="C04-1097.4">joint models</entity> inspired by existing <entity id="C04-1097.5">statistical parsing models</entity>, and several novel <entity id="C04-1097.6">conditional models</entity>. The <entity id="C04-1097.7">conditional models</entity> leverage a large set of <entity id="C04-1097.8">linguistic features</entity> without <entity id="C04-1097.9">manual feature selection</entity>. We apply and evaluate the <entity id="C04-1097.10">models</entity> in <entity id="C04-1097.11">sentence realization</entity> for <entity id="C04-1097.12">French and German</entity> and find that a particular <entity id="C04-1097.13">conditional model</entity> outperforms all others. We employ a version of that <entity id="C04-1097.14">model</entity> in an evaluation on <entity id="C04-1097.15">unordered trees</entity> from the <entity id="C04-1097.16">Penn TreeBank</entity>. We offer this result on <entity id="C04-1097.17">standard data</entity> as a reference-point for evaluations of <entity id="C04-1097.18">ordering</entity> in <entity id="C04-1097.19">sentence realization</entity>.
</abstract>

</text>

<text id="L08-1530">
<title>Linguistic Resources for Reconstructing Spontaneous Speech Text</title>
<abstract>
The output of a <entity id="L08-1530.1">speech recognition system</entity> is not always ideal for <entity id="L08-1530.2">subsequent downstream processing</entity>, in part because <entity id="L08-1530.3">speakers</entity> themselves often make mistakes. A system would accomplish <entity id="L08-1530.4">speech reconstruction</entity> of its <entity id="L08-1530.5">spontaneous speech input</entity> if its output were to represent, in flawless, fluent, and <entity id="L08-1530.6">content-preserving English</entity>, the <entity id="L08-1530.7">message</entity> that the <entity id="L08-1530.8">speaker</entity> intended to convey. These cleaner <entity id="L08-1530.9">speech transcripts</entity> would allow for more accurate <entity id="L08-1530.10">language processing</entity> as needed for <entity id="L08-1530.11">NLP tasks</entity> such as <entity id="L08-1530.12">machine translation</entity> and <entity id="L08-1530.13">conversation summarization</entity>, which often rely on <entity id="L08-1530.14">grammatical input</entity>. Recognizing that <entity id="L08-1530.15">supervised statistical methods</entity> to identify and transform ill-formed areas of the <entity id="L08-1530.16">transcript</entity> will require <entity id="L08-1530.17">richly labeled resources</entity>, we have built the <entity id="L08-1530.18">Spontaneous Speech Reconstruction corpus</entity>. This small <entity id="L08-1530.19">corpus</entity> of <entity id="L08-1530.20">reconstructed and aligned conversational telephone speech transcriptions</entity> for the <entity id="L08-1530.21">Fisher conversational telephone speech corpus</entity> ( Strassel and Walker, 2004 ) was annotated on several levels including <entity id="L08-1530.22">string transformations</entity> and <entity id="L08-1530.23">predicate-argument structure</entity>, and will be shared with the linguistic research community.
</abstract>

</text>

<text id="L08-1122">
<title>RUNDKAST: an Annotated Norwegian Broadcast News Speech Corpus</title>
<abstract>
This paper describes the <entity id="L08-1122.1">Norwegian broadcast news speech corpus RUNDKAST</entity>. The <entity id="L08-1122.2">corpus</entity> contains <entity id="L08-1122.3">recordings</entity> of approximately 77 hours of <entity id="L08-1122.4">broadcast news shows</entity> from the Norwegian broadcasting company NRK. The <entity id="L08-1122.5">corpus</entity> covers both <entity id="L08-1122.6">read and spontaneous speech</entity> as well as <entity id="L08-1122.7">spontaneous dialogues</entity> and <entity id="L08-1122.8">multipart discussions</entity>, including frequent occurrences of <entity id="L08-1122.9">non-speech material</entity> (e.g. music, jingles). The recordings have large variations in <entity id="L08-1122.10">speaking styles</entity>, <entity id="L08-1122.11">dialect use</entity> and recording/transmission quality. <entity id="L08-1122.12">RUNDKAST</entity> has been annotated for research in <entity id="L08-1122.13">speech technology</entity>. The entire <entity id="L08-1122.14">corpus</entity> has been manually segmented and transcribed using hierarchical levels. A subset of one hour of <entity id="L08-1122.15">read and spontaneous speech</entity> from 10 different <entity id="L08-1122.16">speakers</entity> has been manually annotated using broad <entity id="L08-1122.17">phonetic labels</entity>. We provide a description of the database content, the <entity id="L08-1122.18">annotation tools</entity> and strategies, and the conventions used for the different <entity id="L08-1122.19">levels</entity> of <entity id="L08-1122.20">annotation</entity>. A <entity id="L08-1122.21">corpus</entity> of this kind has up to this point not been available for <entity id="L08-1122.22">Norwegian</entity>, but is considered a necessary part of the infrastructure for <entity id="L08-1122.23">language technology research</entity> in Norway. The <entity id="L08-1122.24">RUNDKAST corpus</entity> is planned to be included in a future national <entity id="L08-1122.25">Norwegian language resource bank</entity>.
</abstract>

</text>

<text id="N03-1032">
<title>Frequency Estimates For Statistical Word Similarity Measures</title>
<abstract><entity id="N03-1032.1">Statistical measures</entity> of <entity id="N03-1032.2">word similarity</entity> have application in many areas of <entity id="N03-1032.3">natural language processing</entity>, such as <entity id="N03-1032.4">language modeling</entity> and <entity id="N03-1032.5">information retrieval</entity>. We report a comparative study of two methods for estimating <entity id="N03-1032.6">word cooccurrence frequencies</entity> required by <entity id="N03-1032.7">word similarity measures</entity>. Our <entity id="N03-1032.8">frequency estimates</entity> are generated from a terabyte-sized <entity id="N03-1032.9">corpus</entity> of <entity id="N03-1032.10">Web data</entity>, and we study the impact of <entity id="N03-1032.11">corpus size</entity> on the effectiveness of the <entity id="N03-1032.12">measures</entity>. We base the evaluation on one <entity id="N03-1032.13">TOEFL question set</entity> and two <entity id="N03-1032.14">practice questions sets</entity>, each consisting of a number of <entity id="N03-1032.15">multiple choice questions</entity> seeking the best <entity id="N03-1032.16">synonym</entity> for a given <entity id="N03-1032.17">target word</entity>. For two question sets, a <entity id="N03-1032.18">context</entity> for the <entity id="N03-1032.19">target word</entity> is provided, and we examine a number of <entity id="N03-1032.20">word similarity measures</entity> that exploit this <entity id="N03-1032.21">context</entity>. Our best combination of <entity id="N03-1032.22">similarity measure</entity> and <entity id="N03-1032.23">frequency estimation method</entity> answers 6-8% more questions than the best results previously reported for the same question sets.
</abstract>

</text>

<text id="A88-1011">
<title>TRIPHONE Analysis: A Combined Method For The Correction Of Orthographical And Typographical Errors</title>
<abstract>
Most existing systems for the <entity id="A88-1011.1">correction</entity> of <entity id="A88-1011.2">word level errors</entity> are oriented toward either <entity id="A88-1011.3">typographical or orthographical errors</entity>. <entity id="A88-1011.4">Triphone analysis</entity> is a new <entity id="A88-1011.5">correction strategy</entity> which combines <entity id="A88-1011.6">phonemic transcription</entity> with <entity id="A88-1011.7">trigram analysis</entity>. It corrects both kinds of <entity id="A88-1011.8">errors</entity> (also in combination) and is superior for <entity id="A88-1011.9">orthographical errors</entity>.
</abstract>

</text>

<text id="H92-1082">
<title>An Efficient A* Stack Decoder Algorithm For Continuous Speech Recognition With A Stochastic Language Model</title>
<abstract>
The <entity id="H92-1082.1">stack decoder</entity> is an attractive algorithm for controlling the <entity id="H92-1082.2">acoustic and language model</entity> matching in a <entity id="H92-1082.3">continuous speech recognizer</entity>. A previous paper described a near-optimal admissible <entity id="H92-1082.4">Viterbi A* search algorithm</entity> for use with <entity id="H92-1082.5">non-cross-word acoustic models</entity> and <entity id="H92-1082.6">no-grammar language models</entity> [16]. This paper extends this algorithm to include <entity id="H92-1082.7">unigram language models</entity> and describes a modified version of the algorithm which includes the <entity id="H92-1082.8">full (forward) decoder</entity>, <entity id="H92-1082.9">cross-word acoustic models</entity> and <entity id="H92-1082.10">longer-span language models</entity>. The resultant algorithm is not admissible, but has been demonstrated to have a low probability of <entity id="H92-1082.11">search error</entity> and to be very efficient.
</abstract>

</text>

<text id="A00-1017">
<title>A Representation For Complex And Evolving Data Dependencies In Generation</title>
<abstract>
This paper introduces an approach to representing the kinds of <entity id="A00-1017.1">information</entity> that components in a <entity id="A00-1017.2">natural language generation (NLG) system</entity> will need to communicate to one another. This information may be partial, may involve more than one level of analysis and may need to include information about the history of a <entity id="A00-1017.3">derivation</entity>. We present a general <entity id="A00-1017.4">representation scheme</entity> capable of handling these cases. In addition, we make a proposal for organising <entity id="A00-1017.5">intermodule communication</entity> in an <entity id="A00-1017.6">NLG system</entity> by having a <entity id="A00-1017.7">central server</entity> for this <entity id="A00-1017.8">information</entity>. We have validated the approach by a reanalysis of an existing <entity id="A00-1017.9">NLG system</entity> and through a <entity id="A00-1017.10">full implementation</entity> of a <entity id="A00-1017.11">runnable specification</entity>.
</abstract>

</text>

<text id="W93-0312">
<title>Example-Based Sense Tagging Of Running Chinese Text</title>
<abstract>
This paper describes a <entity id="W93-0312.1">sense tagging technique</entity> for the <entity id="W93-0312.2">automatic sense tagging</entity> of <entity id="W93-0312.3">running Chinese text</entity>. The system takes as input <entity id="W93-0312.4">running Chinese text</entity>, and outputs <entity id="W93-0312.5">sense disambiguated text</entity>. Whereas previous work (Yarowsky, 1992; Gale et al., 1992, 1993) relies heavily on the role of statistics, the present system makes use of <entity id="W93-0312.6">Machine Readable/Tractable Dictionaries</entity> (Wilks et al., 1990; Guo, in press) and an <entity id="W93-0312.7">example-based reasoning technique</entity> (Nagao, 1984; Sumita et al., 1990) to treat <entity id="W93-0312.8">novel words</entity>, <entity id="W93-0312.9">compound words</entity>, and <entity id="W93-0312.10">phrases</entity> found in the <entity id="W93-0312.11">input text</entity>.
</abstract>

</text>

<text id="W98-0501">
<title>Towards An Implementable Dependency Grammar</title>
<abstract><entity id="W98-0501.1">Syntactic models</entity> should be descriptively adequate and parsable. A <entity id="W98-0501.2">syntactic description</entity> is autonomous in the sense that it has certain explicit <entity id="W98-0501.3">formal properties</entity>. Such a description relates to the <entity id="W98-0501.4">semantic interpretation</entity> of the <entity id="W98-0501.5">sentences</entity>, and to the <entity id="W98-0501.6">surface text</entity>. As the <entity id="W98-0501.7">formalism</entity> is implemented in a <entity id="W98-0501.8">broad-coverage syntactic parser</entity>, we concentrate on issues that must be resolved by any practical system that uses such models. The correspondence between the <entity id="W98-0501.9">structure</entity> and <entity id="W98-0501.10">linear order</entity> is discussed.
</abstract>

</text>

<text id="W00-1302">
<title>What's Yours And What's Mine: Determining Intellectual Attribution In Scientific Text</title>
<abstract>
We believe that identifying the structure of <entity id="W00-1302.1">scientific argumentation</entity> in articles can help in tasks such as <entity id="W00-1302.2">automatic summarization</entity> or the <entity id="W00-1302.3">automated construction</entity> of <entity id="W00-1302.4">citation indexes</entity>. One particularly important aspect of this structure is the question of who a given <entity id="W00-1302.5">scientific statement</entity> is attributed to: other researchers, the field in general, or the authors themselves. We present the algorithm and a systematic evaluation of a system which can recognize the most salient <entity id="W00-1302.6">textual properties</entity> that contribute to the global <entity id="W00-1302.7">argumentative structure</entity> of a <entity id="W00-1302.8">text</entity>. In this paper we concentrate on two particular features, namely the <entity id="W00-1302.9">occurrences</entity> of <entity id="W00-1302.10">prototypical agents</entity> and their actions in <entity id="W00-1302.11">scientific text</entity>.
</abstract>

</text>

<text id="W02-1802">
<title>Some Considerations On Guidelines For Bilingual Alignment And Terminology Extraction</title>
<abstract>
Despite progress in the development of <entity id="W02-1802.1">computational means</entity>, <entity id="W02-1802.2">human input</entity> is still critical in the production of consistent and useable <entity id="W02-1802.3">aligned corpora</entity> and <entity id="W02-1802.4">term banks</entity>. This is especially true for <entity id="W02-1802.5">specialized corpora</entity> and <entity id="W02-1802.6">term banks</entity> whose end-users are often professionals with very stringent requirements for accuracy, consistency and <entity id="W02-1802.7">coverage</entity>. In the compilation of a high quality <entity id="W02-1802.8">Chinese-English legal glossary</entity> for <entity id="W02-1802.9">ELDoS project</entity>, we have identified a number of issues that make the role human input critical for <entity id="W02-1802.10">term alignment</entity> and <entity id="W02-1802.11">extraction</entity>. They include the identification of <entity id="W02-1802.12">low frequency terms</entity>, <entity id="W02-1802.13">paraphrastic expressions</entity>, <entity id="W02-1802.14">discontinuous units</entity>, and maintaining <entity id="W02-1802.15">consistent term granularity</entity>, etc. Although <entity id="W02-1802.16">manual intervention</entity> can more satisfactorily address these issues, steps must also be taken to address <entity id="W02-1802.17">intra- and inter-annotator inconsistency</entity>. </abstract>

</text>

<text id="J94-3004">
<title>The Reconstruction Engine: A Computer Implementation Of The Comparative Method</title>
<abstract>
We describe the implementation of a computer program, the <entity id="J94-3004.1">Reconstruction Engine (RE)</entity>, which models the <entity id="J94-3004.2">comparative method</entity> for establishing <entity id="J94-3004.3">genetic affiliation</entity> among a <entity id="J94-3004.4">group of languages</entity>. The program is a <entity id="J94-3004.5">research tool</entity> designed to aid the <entity id="J94-3004.6">linguist</entity> in evaluating specific hypotheses, by calculating the <entity id="J94-3004.7">consequences</entity> of a set of <entity id="J94-3004.8">postulated sound changes</entity> (proposed by the <entity id="J94-3004.9">linguist</entity>) on <entity id="J94-3004.10">complete lexicons</entity> of <entity id="J94-3004.11">several languages</entity>. It divides the <entity id="J94-3004.12">lexicons</entity> into a <entity id="J94-3004.13">phonologically regular part</entity> and a part that deviates from the <entity id="J94-3004.14">sound laws</entity>. <entity id="J94-3004.15">RE</entity> is bi-directional: given <entity id="J94-3004.16">words</entity> in <entity id="J94-3004.17">modern languages</entity>, it can propose <entity id="J94-3004.18">cognate sets</entity> (with reconstructions); given <entity id="J94-3004.19">reconstructions</entity>, it can project the <entity id="J94-3004.20">modern forms</entity> that would result from regular changes. <entity id="J94-3004.21">RE</entity> operates either interactively, allowing <entity id="J94-3004.22">word-by-word evaluation</entity> of <entity id="J94-3004.23">hypothesized sound changes</entity> and <entity id="J94-3004.24">semantic shifts</entity>, or in a "batch" mode, processing entire <entity id="J94-3004.25">multilingual lexicons</entity>. We describe the algorithms implemented in <entity id="J94-3004.26">RE</entity>, specifically the <entity id="J94-3004.27">parsing</entity> and <entity id="J94-3004.28">combinatorial techniques</entity> used to make <entity id="J94-3004.29">projections</entity> upstream or downstream in the sense of time, the procedures for creating and consolidating <entity id="J94-3004.30">cognate sets</entity> based on these <entity id="J94-3004.31">projections</entity>, and the ad hoc techniques developed for handling the <entity id="J94-3004.32">semantic component</entity> of the <entity id="J94-3004.33">comparative method</entity>. Other programs and <entity id="J94-3004.34">computational approaches</entity> to <entity id="J94-3004.35">historical linguistics</entity> are briefly reviewed. Some results from a study of the <entity id="J94-3004.36">Tamang languages</entity> of Nepal (a subgroup of the <entity id="J94-3004.37">Tibeto-Burman family</entity>) are presented, and data from these <entity id="J94-3004.38">languages</entity> are used throughout for exemplification of the operation of the program. Finally, we discuss features of <entity id="J94-3004.39">RE</entity> that make it possible to handle the complex and sometimes imprecise <entity id="J94-3004.40">representations</entity> of <entity id="J94-3004.41">lexical items</entity>, and speculate on possible directions for future research.
</abstract>

</text>

<text id="P98-2237">
<title>Using Chunk Based Partial Parsing of Spontaneous Speech in Unrestricted Domains for Reducing Word Error Rate in Speech Recognition</title>
<abstract>
In this paper, we present a <entity id="P98-2237.1">chunk based partial parsing system</entity> for <entity id="P98-2237.2">spontaneous, conversational speech</entity> in <entity id="P98-2237.3">unrestricted domains</entity>. We show that the <entity id="P98-2237.4">chunk parses</entity> produced by this <entity id="P98-2237.5">parsing system</entity> can be usefully applied to the task of reranking <entity id="P98-2237.6">N-best lists</entity> from a <entity id="P98-2237.7">speech recognizer</entity>, using a combination of <entity id="P98-2237.8">chunk-based n-gram model scores</entity> and <entity id="P98-2237.9">chunk coverage scores</entity>. The input for the system is <entity id="P98-2237.10">N-best lists</entity> generated from <entity id="P98-2237.11">speech recognizer lattices</entity>. The hypotheses from the <entity id="P98-2237.12">N-best lists</entity> are tagged for <entity id="P98-2237.13">part of speech</entity>, "cleaned up" by a <entity id="P98-2237.14">preprocessing pipe</entity>, parsed by a <entity id="P98-2237.15">part of speech based chunk parser</entity>, and rescored using a <entity id="P98-2237.16">backpropagation neural net</entity> trained on the <entity id="P98-2237.17">chunk based scores</entity>. Finally, the <entity id="P98-2237.18">reranked N-best lists</entity> are generated. The results of a system evaluation are promising in that a <entity id="P98-2237.19">chunk accuracy</entity> of 87.4% is achieved and the best performance on a <entity id="P98-2237.20">randomly selected test set</entity> is a decrease in <entity id="P98-2237.21">word error rate</entity> of 0.3 percent (absolute), measured on the new <entity id="P98-2237.22">first hypotheses</entity> in the <entity id="P98-2237.23">reranked N-best lists</entity>.
</abstract>

</text>

<text id="W05-0409">
<title>Studying Feature Generation From Various Data Representations For Answer Extraction</title>
<abstract>
In this paper, we study how to generate <entity id="W05-0409.1">features</entity> from various <entity id="W05-0409.2">data representations</entity>, such as <entity id="W05-0409.3">surface texts</entity> and <entity id="W05-0409.4">parse trees</entity>, for <entity id="W05-0409.5">answer extraction</entity>. Besides the <entity id="W05-0409.6">features</entity>generated from the <entity id="W05-0409.7">surface texts</entity>, we mainly discuss the <entity id="W05-0409.8">feature generation</entity> in the <entity id="W05-0409.9">parse trees</entity>. We propose and compare three methods, including <entity id="W05-0409.10">feature vector</entity>, <entity id="W05-0409.11">string kernel</entity> and <entity id="W05-0409.12">tree kernel</entity>, to represent the <entity id="W05-0409.13">syntactic features</entity> in <entity id="W05-0409.14">Support Vector Machines</entity>. The experiment on the <entity id="W05-0409.15">TREC question answering task</entity> shows that the <entity id="W05-0409.16">features</entity> generated from the more structured <entity id="W05-0409.17">data representations</entity> significantly improve the performance based on the <entity id="W05-0409.18">features</entity> generated from the <entity id="W05-0409.19">surface texts</entity>. Furthermore, the contribution of the individual <entity id="W05-0409.20">feature</entity> will be discussed in detail.
</abstract>

</text>

<text id="W06-0805">
<title>Exploring Semantic Constraints For Document Retrieval</title>
<abstract>
In this paper, we explore the use of <entity id="W06-0805.1">structured content</entity> as <entity id="W06-0805.2">semantic constraints</entity> for enhancing the performance of traditional <entity id="W06-0805.3">term-based document retrieval</entity> in special domains. First, we describe a method for <entity id="W06-0805.4">automatic extraction</entity> of <entity id="W06-0805.5">semantic content</entity> in the form of <entity id="W06-0805.6">attribute-value (AV) pairs</entity> from <entity id="W06-0805.7">natural language texts</entity> based on <entity id="W06-0805.8">domain models</entity>constructed from a <entity id="W06-0805.9">semi-structured web resource</entity>. Then, we explore the effect of combining a <entity id="W06-0805.10">state-of-the-art term-based IR system</entity> and a simple <entity id="W06-0805.11">constraint-based search system</entity> that uses the extracted <entity id="W06-0805.12">AV pairs</entity>. Our evaluation results have shown that such combination produces some improvement in <entity id="W06-0805.13">IR performance</entity> over the <entity id="W06-0805.14">term-based IR system</entity> on our test collection.
</abstract>

</text>

<text id="W07-0210">
<title>Correlations in the Organization of Large-Scale Syntactic Dependency Networks</title>
<abstract>
We study the <entity id="W07-0210.1">correlations</entity> in the <entity id="W07-0210.2">connectivity patterns</entity> of large scale <entity id="W07-0210.3">syntactic dependency networks</entity>. These <entity id="W07-0210.4">networks</entity> are induced from <entity id="W07-0210.5">treebanks</entity>: their <entity id="W07-0210.6">vertices</entity> denote <entity id="W07-0210.7">word forms</entity> which occur as <entity id="W07-0210.8">nuclei</entity> of <entity id="W07-0210.9">dependency trees</entity>. Their <entity id="W07-0210.10">edges</entity> connect pairs of <entity id="W07-0210.11">vertices</entity> if at least two <entity id="W07-0210.12">instance nuclei</entity> of these <entity id="W07-0210.13">vertices</entity> are linked in the <entity id="W07-0210.14">dependency structure</entity> of a <entity id="W07-0210.15">sentence</entity>. We examine the <entity id="W07-0210.16">syntactic dependency networks</entity> of seven <entity id="W07-0210.17">languages</entity>. In all these cases, we consistently obtain three findings. Firstly, <entity id="W07-0210.18">clustering</entity>, i.e., the probability that two <entity id="W07-0210.19">vertices</entity> which are linked to a common <entity id="W07-0210.20">vertex</entity> are linked on their part, is much higher than expected by chance. Secondly, the <entity id="W07-0210.21">mean clustering</entity> of <entity id="W07-0210.22">vertices</entity> decreases with their <entity id="W07-0210.23">degree</entity> - this finding suggests the presence of a <entity id="W07-0210.24">hierarchical network organization</entity>. Thirdly, the <entity id="W07-0210.25">mean degree</entity> of the <entity id="W07-0210.26">nearest neighbors</entity> of a <entity id="W07-0210.27">vertex</entity> x tends to decrease as the <entity id="W07-0210.28">degree</entity> of x grows - this finding indicates <entity id="W07-0210.29">disassortative mixing</entity> in the sense that <entity id="W07-0210.30">links</entity> tend to connect <entity id="W07-0210.31">vertices</entity> of dissimilar <entity id="W07-0210.32">degrees</entity>. Our results indicate the existence of common patterns in the <entity id="W07-0210.33">large scale organization</entity> of <entity id="W07-0210.34">syntactic dependency networks</entity>.
</abstract>

</text>

<text id="W07-1902">
<title>Aiduti in Japanese Multi-Party Design Conversations</title>
<abstract><entity id="W07-1902.1">Japanese backchannel utterances, aizuti</entity>, in a <entity id="W07-1902.2">multi-party design conversation</entity> were examined, and <entity id="W07-1902.3">aizuti functions</entity> were analyzed in comparison with its functions in <entity id="W07-1902.4">two-party dialogues</entity>. In addition to the two major functions, <entity id="W07-1902.5">signaling acknowledgment</entity> and <entity id="W07-1902.6">turn-management</entity>, it was argued that <entity id="W07-1902.7">aizuti</entity> in <entity id="W07-1902.8">multi-party conversations</entity> are involved in joint construction of design plans through management of the <entity id="W07-1902.9">floor structure</entity>, and display of participants' readiness to engage in collaborative elaboration of jointly constructed proposals.
</abstract>

</text>

<text id="P07-1021">
<title>Mildly Context-Sensitive Dependency Languages</title>
<abstract><entity id="P07-1021.1">Dependency-based representations</entity> of <entity id="P07-1021.2">natural language syntax</entity> require a fine balance between structural flexibility and <entity id="P07-1021.3">computational complexity</entity>. In previous work, several <entity id="P07-1021.4">constraints</entity> have been proposed to identify classes of <entity id="P07-1021.5">dependency structures</entity> that are well-balanced in this sense; the best-known but also most restrictive of these is <entity id="P07-1021.6">projectivity</entity>. Most <entity id="P07-1021.7">constraints</entity> are formulated on <entity id="P07-1021.8">fully specified structures</entity>, which makes them hard to integrate into models where <entity id="P07-1021.9">structures</entity> are composed from <entity id="P07-1021.10">lexical information</entity>. In this paper, we show how two empirically relevant <entity id="P07-1021.11">relaxations</entity> of <entity id="P07-1021.12">projectivity</entity> can be lexicalized, and how combining the resulting <entity id="P07-1021.13">lexicons</entity> with a regular means of <entity id="P07-1021.14">syntactic composition</entity> gives rise to a hierarchy of <entity id="P07-1021.15">mildly context-sensitive dependency languages</entity>.
</abstract>

</text>

<text id="P08-2039">
<title>Segmentation for English-to-Arabic Statistical Machine Translation</title>
<abstract>
In this paper, we report on a set of initial results for <entity id="P08-2039.1">English-to-Arabic Statistical Machine Translation (SMT)</entity>. We show that <entity id="P08-2039.2">morphological decomposition</entity> of the <entity id="P08-2039.3">Arabic</entity> source is beneficial, especially for <entity id="P08-2039.4">smaller-size corpora</entity>, and investigate different <entity id="P08-2039.5">recombination techniques</entity>. We also report on the use of <entity id="P08-2039.6">Factored Translation Models</entity> for <entity id="P08-2039.7">English-to-Arabic translation</entity>.
</abstract>

</text>

<text id="W08-2131">
<title>Discriminative vs. Generative Approaches in Semantic Role Labeling</title>
<abstract>
This paper describes the two algorithms we developed for the <entity id="W08-2131.1">CoNLL 2008 Shared Task</entity> "Joint learning of <entity id="W08-2131.2">syntactic and semantic dependencies</entity>". Both algorithms start parsing the <entity id="W08-2131.3">sentence</entity> using the same <entity id="W08-2131.4">syntactic parser</entity>. The first algorithm uses <entity id="W08-2131.5">machine learning methods</entity> to identify the <entity id="W08-2131.6">semantic dependencies</entity> in four stages: <entity id="W08-2131.7">identification</entity> and <entity id="W08-2131.8">labeling</entity> of <entity id="W08-2131.9">predicates</entity>, <entity id="W08-2131.10">identification</entity> and <entity id="W08-2131.11">labeling</entity> of <entity id="W08-2131.12">arguments</entity>. The second algorithm uses a <entity id="W08-2131.13">generative probabilistic model</entity>, choosing the <entity id="W08-2131.14">semantic dependencies</entity> that maximize the probability with respect to the model. A <entity id="W08-2131.15">hybrid algorithm</entity> combining the best stages of the two algorithms attains 86.62% <entity id="W08-2131.16">labeled syntactic attachment accuracy</entity>, 73.24% <entity id="W08-2131.17">labeled semantic dependency F1</entity> and 79.93% <entity id="W08-2131.18">labeled macro Fl score</entity> for the combined <entity id="W08-2131.19">WSJ and Brown test sets</entity>.
</abstract>

</text>

<text id="P05-1019">
<title>Modelling The Substitutability Of Discourse Connectives</title>
<abstract>Processing <entity id="P05-1019.1">discourse connectives</entity> is important for tasks such as <entity id="P05-1019.2">discourse parsing</entity> and <entity id="P05-1019.3">generation</entity>. For these tasks, it is useful to know which <entity id="P05-1019.4">connectives</entity> can signal the same <entity id="P05-1019.5">coherence relations</entity>. This paper presents experiments into modelling the <entity id="P05-1019.6">substitutability</entity> of <entity id="P05-1019.7">discourse connectives</entity>. It shows that <entity id="P05-1019.8">substitutability</entity> effects <entity id="P05-1019.9">distributional similarity</entity>. A novel <entity id="P05-1019.10">variance-based function</entity> for comparing <entity id="P05-1019.11">probability distributions</entity> is found to assist in predicting <entity id="P05-1019.12">substitutability</entity>.
</abstract>

</text>

<text id="C86-1086">
<title>An Approach To Non-Singular Terms In Discourse</title>
<abstract>
A new <entity id="C86-1086.1">Theory of Names and Descriptions</entity> that offers a uniform treatment for many types of <entity id="C86-1086.2">non-singular concepts</entity> found in <entity id="C86-1086.3">natural language discourse</entity> is presented. We introduce a <entity id="C86-1086.4">layered model</entity> of the <entity id="C86-1086.5">language denotational base</entity> (the universe) in which every world object is assigned a <entity id="C86-1086.6">layer (level)</entity> reflecting its relative <entity id="C86-1086.7">singularity</entity> with respect to other objects in the universe. We define the notion of <entity id="C86-1086.8">relative singularity</entity> of world objects as an abstraction class of the <entity id="C86-1086.9">layer-membership relation</entity>.</abstract>

</text>

<text id="C92-4172">
<title>Syntactic Constraints On Relativization In Japanese</title>
<abstract>
This paper discusses the formalization of <entity id="C92-4172.1">relative clauses</entity> in <entity id="C92-4172.2">Japanese</entity> based on <entity id="C92-4172.3">JPSG framework</entity>. We characterize them as <entity id="C92-4172.4">adjuncts</entity> to <entity id="C92-4172.5">nouns</entity>, and formalize them in terms of <entity id="C92-4172.6">constraints</entity> among <entity id="C92-4172.7">grammatical features</entity>. Furthermore, we claim that there is a <entity id="C92-4172.8">constraint</entity> on the number of <entity id="C92-4172.9">slash elements</entity> and show the supporting facts.
</abstract>

</text>

<text id="E99-1050">
<title>A Corpus-Based Approach To Deriving Lexical Mappings</title>
<abstract>
This paper proposes a novel, <entity id="E99-1050.1">corpus-based method</entity> for producing <entity id="E99-1050.2">mappings</entity> between <entity id="E99-1050.3">lexical resources</entity>. Results from a preliminary experiment using <entity id="E99-1050.4">part of speech tags</entity> suggests this is a promising area for future research.
</abstract>

</text>

<text id="C04-1007">
<title>Combining Hierarchical Clustering And Machine Learning To Predict High-Level Discourse Structure</title>
<abstract>
We propose a novel method to predict the <entity id="C04-1007.1">inter-paragraph discourse structure</entity> of <entity id="C04-1007.2">text</entity>, i.e. to infer which <entity id="C04-1007.3">paragraphs</entity> are related to each other and form larger segments on a higher level. Our method combines a <entity id="C04-1007.4">clustering algorithm</entity> with a <entity id="C04-1007.5">model</entity> of <entity id="C04-1007.6">segment "relatedness"</entity> acquired in a <entity id="C04-1007.7">machine learning step</entity>. The model integrates information from a variety of sources, such as <entity id="C04-1007.8">word co-occurrence</entity>, <entity id="C04-1007.9">lexical chains</entity>, <entity id="C04-1007.10">cue phrases</entity>, <entity id="C04-1007.11">punctuation</entity>, and <entity id="C04-1007.12">tense</entity>. Our method outperforms an approach that relies on <entity id="C04-1007.13">word co-occurrence</entity> alone.
</abstract>

</text>

<text id="E91-1002">
<title>Comparatives And Ellipsis</title>
<abstract>
This paper analyses the <entity id="E91-1002.1">syntax</entity> and <entity id="E91-1002.2">semantics</entity> of <entity id="E91-1002.3">English comparatives</entity>, and some types of <entity id="E91-1002.4">ellipsis</entity>. It improves on other recent analyses in the <entity id="E91-1002.5">computational linguistics</entity> literature in three respects: (i) it uses no <entity id="E91-1002.6">tree- or logical-form rewriting devices</entity> in building <entity id="E91-1002.7">meaning representations</entity> (ii) this results in a <entity id="E91-1002.8">fully reversible linguistic description</entity>, equally suited for analysis or <entity id="E91-1002.9">generation</entity> (iii) the analysis extends to types of <entity id="E91-1002.10">elliptical comparative</entity> not elsewhere treated.
</abstract>

</text>

<text id="L08-1531">
<title>Spock - a Spoken Corpus Client</title>
<abstract> <entity id="L08-1531.1">Spock</entity> is an <entity id="L08-1531.2">open source tool</entity> for the easy deployment of <entity id="L08-1531.3">time-aligned corpora</entity>. It is fully web-based, and has very limited <entity id="L08-1531.4">server-side requirements</entity>. It allows the <entity id="L08-1531.5">end-user</entity> to search the <entity id="L08-1531.6">corpus</entity> in a <entity id="L08-1531.7">text-driven</entity> manner, obtaining both the <entity id="L08-1531.8">transcription</entity> and the corresponding <entity id="L08-1531.9">sound fragment</entity> in the result page. <entity id="L08-1531.10">Spock</entity> has an administration environment to help manage the <entity id="L08-1531.11">sound files</entity> and their respective <entity id="L08-1531.12">transcription files</entity>, and also provides <entity id="L08-1531.13">statistical data</entity> about the files at hand. <entity id="L08-1531.14">Spock</entity> uses a proprietary file format for storing the <entity id="L08-1531.15">alignment data</entity> but the integrated admin environment allows you to import files from a number of common file formats. <entity id="L08-1531.16">Spock</entity> is not intended as a <entity id="L08-1531.17">transcriber program</entity>: it is not meant as an alternative to programs such as <entity id="L08-1531.18">ELAN</entity>, <entity id="L08-1531.19">Wavesurfer</entity>, or <entity id="L08-1531.20">Transcriber</entity>, but rather to make <entity id="L08-1531.21">corpora</entity> created with these tools easily available on line. For the end user, <entity id="L08-1531.22">Spock</entity> provides a very easy way of accessing <entity id="L08-1531.23">spoken corpora</entity>, without the need of installing any special software, which might make <entity id="L08-1531.24">time-aligned corpora</entity> accessible to a large group of users who might otherwise never look at them.
</abstract>

</text>

<text id="E99-1048">
<title>Comparison And Classification Of Dialects</title>
<abstract>
This project measures and classifies <entity id="E99-1048.1">language variation</entity>. In contrast to earlier <entity id="E99-1048.2">dialectology</entity>, we seek a comprehensive characterization of (potentially gradual) differences between <entity id="E99-1048.3">dialects</entity>, rather than a geographic delineation of <entity id="E99-1048.4">(discrete) features</entity> of <entity id="E99-1048.5">individual words</entity> or <entity id="E99-1048.6">pronunciations</entity>. More general characterizations of <entity id="E99-1048.7">dialect differences</entity> then become available. We measure <entity id="E99-1048.8">phonetic (un)relatedness</entity> between <entity id="E99-1048.9">dialects</entity> using <entity id="E99-1048.10">Levenshtein distance</entity>, and classify by <entity id="E99-1048.11">clustering distances</entity> but also by analysis through multidimensional scaling.
</abstract>

</text>

<text id="N06-1027">
<title>Learning To Detect Conversation Focus Of Threaded Discussions</title>
<abstract>
In this paper we present a novel <entity id="N06-1027.1">feature-enriched approach</entity> that learns to detect the <entity id="N06-1027.2">conversation focus</entity> of <entity id="N06-1027.3">threaded discussions</entity> by combining <entity id="N06-1027.4">NLP analysis</entity> and <entity id="N06-1027.5">IR techniques</entity>. Using the <entity id="N06-1027.6">graph-based algorithm HITS</entity>, we integrate different features such as <entity id="N06-1027.7">lexical similarity</entity>, <entity id="N06-1027.8">poster trustworthiness</entity>, and <entity id="N06-1027.9">speech act analysis</entity> of <entity id="N06-1027.10">human conversations</entity> with <entity id="N06-1027.11">feature-oriented link generation functions</entity>. It is the first quantitative study to analyze <entity id="N06-1027.12">human conversation focus</entity> in the context of <entity id="N06-1027.13">online discussions</entity> that takes into account <entity id="N06-1027.14">heterogeneous sources of evidence</entity>. Experimental results using a <entity id="N06-1027.15">threaded discussion corpus</entity> from an undergraduate class show that it achieves significant <entity id="N06-1027.16">performance improvements</entity> compared with the <entity id="N06-1027.17">baseline system</entity>.
</abstract>

</text>

<text id="A92-1028">
<title>Zero Pronoun Resolution In A Machine Translation System By Using Japanese To English Verbal Semantic Attributes</title>
<abstract>
A method of <entity id="A92-1028.1">anaphoral resolution</entity> of <entity id="A92-1028.2">zero pronouns</entity> in <entity id="A92-1028.3">Japanese language texts</entity> using the <entity id="A92-1028.4">verbal semantic attributes</entity> is suggested. This method focuses attention on the <entity id="A92-1028.5">semantic attributes</entity> of <entity id="A92-1028.6">verbs</entity> and examines the context from the relationship between the <entity id="A92-1028.7">semantic attributes</entity> of <entity id="A92-1028.8">verbs</entity> governing <entity id="A92-1028.9">zero pronouns</entity> and the <entity id="A92-1028.10">semantic attributes</entity> of <entity id="A92-1028.11">verbs</entity> governing their <entity id="A92-1028.12">referents</entity>. The <entity id="A92-1028.13">semantic attributes</entity> of <entity id="A92-1028.14">verbs</entity> are created using 2 different viewpoints: <entity id="A92-1028.15">dynamic characteristics</entity> of <entity id="A92-1028.16">verbs</entity> and the relationship of <entity id="A92-1028.17">verbs</entity> to <entity id="A92-1028.18">cases</entity>. By using this method, it is shown that, in the case of <entity id="A92-1028.19">translating newspaper articles</entity>, the major portion (93%) of <entity id="A92-1028.20">anaphoral resolution</entity> of <entity id="A92-1028.21">zero pronouns</entity> necessary for <entity id="A92-1028.22">machine translation</entity> can be achieved by using only <entity id="A92-1028.23">linguistic knowledge</entity>.Factors to be given special attention when incorporating this method into a <entity id="A92-1028.24">machine translation system</entity> are examined, together with suggested conditions for the detection of <entity id="A92-1028.25">zero pronouns</entity> and methods for their conversion. This study considers four factors that are important when implementing this method in a <entity id="A92-1028.26">Japanese to English machine translation system</entity>: the difference in conception between <entity id="A92-1028.27">Japanese</entity> and <entity id="A92-1028.28">English</entity> expressions, the difference in case <entity id="A92-1028.29">frame patterns</entity> between <entity id="A92-1028.30">Japanese</entity> and <entity id="A92-1028.31">English</entity>, restrictions by voice and restriction by <entity id="A92-1028.32">translation structure</entity>. Implementation of the proposed method with due consideration of these points leads to a viable method for <entity id="A92-1028.33">anaphoral resolution</entity> of zero pronouns in a practical <entity id="A92-1028.34">machine translation system</entity>.
</abstract>

</text>

<text id="H93-1092">
<title>Automatic Extraction Of Grammars From Annotated Text</title>
<abstract>
The primary objective of this project is to develop a robust, <entity id="H93-1092.1">high-performance parser</entity> for <entity id="H93-1092.2">English</entity> by automatically extracting a <entity id="H93-1092.3">grammar</entity> from an <entity id="H93-1092.4">annotated corpus</entity> of <entity id="H93-1092.5">bracketed sentences</entity>, called the <entity id="H93-1092.6">Treebank</entity>. The project is a collaboration between the IBM Continuous Speech Recognition Group and the University of Pennsylvania Department of Computer Sciences.</abstract>

</text>

<text id="X96-1007">
<title>The Text REtrieval Conferences (TRECs) - Summary</title>
<abstract>
There have been four <entity id="X96-1007.1">Text REtrieval</entity> Conferences (TRECs); <entity id="X96-1007.2">TREC-1</entity> in November 1992, <entity id="X96-1007.3">TREC-2</entity> in August 1993, <entity id="X96-1007.4">TREC-3</entity> in November 1994 and <entity id="X96-1007.5">TREC-4</entity> in November 1995. The number of participating systems has grown from 25 in <entity id="X96-1007.6">TREC-1</entity> to 36 in <entity id="X96-1007.7">TREC-4</entity>, including most of the major <entity id="X96-1007.8">text retrieval software</entity> companies and most of the universities doing research in <entity id="X96-1007.9">text retrieval</entity> (see table for some of the participants). The diversity of the participating groups has ensured that <entity id="X96-1007.10">TREC</entity> represents many different approaches to <entity id="X96-1007.11">text retrieval</entity>, while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of <entity id="X96-1007.12">TREC</entity>. The test design and test collection used for <entity id="X96-1007.13">document detection</entity> in <entity id="X96-1007.14">TIPSTER</entity> was also used in <entity id="X96-1007.15">TREC</entity>. The participants ran the various tasks, sent results into <entity id="X96-1007.16">NIST</entity> for evaluation, presented the results at the <entity id="X96-1007.17">TREC</entity> conferences, and submitted papers for a proceedings. The <entity id="X96-1007.18">test collection</entity> consists of over 1 million <entity id="X96-1007.19">documents</entity> from diverse <entity id="X96-1007.20">full-text sources</entity>, 250 <entity id="X96-1007.21">topics</entity>, and the set of relevant <entity id="X96-1007.22">documents</entity> or "right answers" to those <entity id="X96-1007.23">topics</entity>. A <entity id="X96-1007.24">Spanish collection</entity> has been built and used during <entity id="X96-1007.25">TREC-3</entity> and <entity id="X96-1007.26">TREC-4</entity>, with a total of 50 <entity id="X96-1007.27">topics</entity>. <entity id="X96-1007.28">TREC-1</entity> required significant system rebuilding by most groups due to the huge increase in the size of the <entity id="X96-1007.29">document collection</entity> (from a traditional <entity id="X96-1007.30">test collection</entity> of several megabytes in size to the 2 gigabyte <entity id="X96-1007.31">TIPSTER collection</entity>). The results from <entity id="X96-1007.32">TREC-2</entity> showed significant improvements over the <entity id="X96-1007.33">TREC-1 results</entity>, and should be viewed as the appropriate baseline representing state-of-the-art <entity id="X96-1007.34">retrieval techniques</entity> as scaled up to handling a 2 gigabyte collection. <entity id="X96-1007.35">TREC-3</entity> therefore provided the first opportunity for more complex experimentation. The major experiments in <entity id="X96-1007.36">TREC-3</entity> included the development of <entity id="X96-1007.37">automatic query expansion techniques</entity>, the use of <entity id="X96-1007.38">passages or sub-documents</entity> to increase the <entity id="X96-1007.39">precision</entity> of <entity id="X96-1007.40">retrieval results</entity>, and the use of the <entity id="X96-1007.41">training information</entity> to select only the best <entity id="X96-1007.42">terms</entity> for routing <entity id="X96-1007.43">queries</entity>. Some groups explored hybrid approaches (such as the use of the <entity id="X96-1007.44">Rocchio methodology</entity> in systems not using a <entity id="X96-1007.45">vector space model</entity>), and others tried approaches that were radically different from their original approaches. <entity id="X96-1007.46">TREC-4</entity> allowed a continuation of many of these complex experiments. The topics were made much shorter and this change triggered extensive investigations in <entity id="X96-1007.47">automatic query expansion</entity>. There were also five new tasks called tracks. These were added to help focus research on certain known problem areas, and included such issues as investigating <entity id="X96-1007.48">searching</entity> as an interactive task by examining the process as well as the outcome, investigating techniques for <entity id="X96-1007.49">merging results</entity> from the various <entity id="X96-1007.50">TREC subcollections</entity>, examining the effects of <entity id="X96-1007.51">corrupted data</entity>, and evaluating routing systems using a specific <entity id="X96-1007.52">effectiveness measure</entity>. Additionally more groups participated in a track for <entity id="X96-1007.53">Spanish retrieval</entity>. The <entity id="X96-1007.54">TREC</entity> conferences have proven to be very successful, allowing broad participation in the overall <entity id="X96-1007.55">DARPA TIPSTER</entity> effort, and causing widespread use of a very large <entity id="X96-1007.56">test collection</entity>. All conferences have had very open, honest discussions of technical issues, and there have been large amounts of "cross-fertilization" of ideas. This will be a continuing effort, with a <entity id="X96-1007.57">TREC-5</entity> conference scheduled in November of 1996.
</abstract>

</text>

<text id="W96-0311">
<title>Morphological Productivity In The Lexicon</title>
<abstract>
In this paper we outline a lexical organization for <entity id="W96-0311.1">Turkish</entity> that makes use of <entity id="W96-0311.2">lexical rules</entity> for <entity id="W96-0311.3">inflections</entity>, <entity id="W96-0311.4">derivations</entity>, and <entity id="W96-0311.5">lexical category</entity> changes to control the proliferation of <entity id="W96-0311.6">lexical entries</entity>. <entity id="W96-0311.7">Lexical rules</entity> handle changes in <entity id="W96-0311.8">grammatical roles</entity>, enforce <entity id="W96-0311.9">type constraints</entity>, and control the mapping of <entity id="W96-0311.10">sub-categorization frames</entity> in <entity id="W96-0311.11">valency changing operations</entity>. A <entity id="W96-0311.12">lexical inheritance hierarchy</entity> facilitates the enforcement of <entity id="W96-0311.13">type constraints</entity>. <entity id="W96-0311.14">Semantic compositions </entity> in <entity id="W96-0311.15">inflections</entity> and <entity id="W96-0311.16">derivations</entity> are constrained by the properties of the <entity id="W96-0311.17">terms</entity> and <entity id="W96-0311.18">predicates</entity>.The design has been tested as part of a <entity id="W96-0311.19">HPSG grammar</entity> for <entity id="W96-0311.20">Turkish</entity>. In terms of performance, run-time execution of the <entity id="W96-0311.21">rules</entity> seems to be a far better alternative than pre-compilation. The latter causes exponential growth in the <entity id="W96-0311.22">lexicon</entity> due to intensive use of <entity id="W96-0311.23">inflections</entity> and <entity id="W96-0311.24">derivations</entity> in <entity id="W96-0311.25">Turkish</entity>.
</abstract>

</text>

<text id="W98-1241">
<title>Reconciliation Of Unsupervised Clustering, Segmentation And Cohesion</title>
<abstract>
This extended abstract examines the progress of a project on <entity id="W98-1241.1">unsupervised language learning</entity>, and focuses on two different approaches to <entity id="W98-1241.2">segmentation</entity>, as well as how <entity id="W98-1241.3">cohesion</entity> may be generalized from it definitive <entity id="W98-1241.4">morpho-syntactic instantiation</entity>. It is intended as a discussion paper, and outlines the specific hypotheses currenlty being tested.
</abstract>

</text>

<text id="W00-1311">
<title>Detection Of Language (Model) Errors</title>
<abstract>
The <entity id="W00-1311.1">bigram language models</entity> are popular, in much <entity id="W00-1311.2">language processing applications</entity>, in both <entity id="W00-1311.3">Indo-European</entity> and <entity id="W00-1311.4">Asian languages</entity>. However, when the <entity id="W00-1311.5">language model</entity> for <entity id="W00-1311.6">Chinese</entity> is applied in a novel <entity id="W00-1311.7">domain</entity>, the <entity id="W00-1311.8">accuracy</entity> is reduced significantly, from 96% to 78% in our evaluation. We apply <entity id="W00-1311.9">pattern recognition techniques</entity> (i.e. <entity id="W00-1311.10">Bayesian, decision tree</entity> and <entity id="W00-1311.11">neural network classifiers</entity>) to discover <entity id="W00-1311.12">language model errors</entity>. We have examined 2 general types of features: <entity id="W00-1311.13">model-based</entity> and <entity id="W00-1311.14">language-specific features</entity>. In our evaluation, <entity id="W00-1311.15">Bayesian classifiers</entity> produce the best <entity id="W00-1311.16">recall performance</entity> of 80% but the <entity id="W00-1311.17">precision</entity> is low (60%). <entity id="W00-1311.18">Neural network</entity> produced good <entity id="W00-1311.19">recall</entity> (75%) and <entity id="W00-1311.20">precision</entity> (80%) but both <entity id="W00-1311.21">Bayesian and Neural network</entity> have low skip ratio (65%). The <entity id="W00-1311.22">decision tree classifier</entity> produced the best <entity id="W00-1311.23">precision</entity> (81%) and skip ratio (76%) but its <entity id="W00-1311.24">recall</entity> is the lowest (73%).
</abstract>

</text>

<text id="W03-0608">
<title>Why Can't Jose Read ? The Problem Of Learning Semantic Associations In A Robot Environment</title>
<abstract>
We study the problem of learning to recognise objects in the context of autonomous agents. We cast object recognition as the process of attaching meaningful <entity id="W03-0608.1">concepts</entity> to specific <entity id="W03-0608.2">regions of an image</entity>. In other words, given a set of images and their captions, the goal is to segment the image, in either an intelligent or naive fashion, then to find the proper <entity id="W03-0608.3">mapping</entity> between <entity id="W03-0608.4">words</entity> and <entity id="W03-0608.5">regions</entity>. In this paper, we demonstrate that a <entity id="W03-0608.6">model</entity> that learns <entity id="W03-0608.7">spatial relationships</entity> between <entity id="W03-0608.8">individual words</entity> not only provides accurate <entity id="W03-0608.9">annotations</entity>, but also allows one to perform <entity id="W03-0608.10">recognition</entity> that respects the real-time constraints of an autonomous, mobile robot.
</abstract>

</text>

<text id="J97-4004">
<title>Critical Tokenization And Its Properties</title>
<abstract>
Tokenization is the process of <entity id="J97-4004.1">mapping sentences</entity> from <entity id="J97-4004.2">character strings</entity> into <entity id="J97-4004.3">strings of words</entity>. This paper sets out to study critical <entity id="J97-4004.4">tokenization</entity>, a distinctive type of <entity id="J97-4004.5">tokenization</entity> following the principle of maximum <entity id="J97-4004.6">tokenization</entity>. The objective in this paper is to develop its mathematical description and understanding. The main results are as follows: (1) Critical points are all and only <entity id="J97-4004.7">unambiguous token boundaries</entity> for any <entity id="J97-4004.8">character string</entity> on a complete <entity id="J97-4004.9">dictionary</entity>; (2) Any critically <entity id="J97-4004.10">tokenized word string</entity> is a minimal element in the partially ordered set of all <entity id="J97-4004.11">tokenized word strings</entity> with respect to the <entity id="J97-4004.12">word string</entity> cover <entity id="J97-4004.13">relation</entity>; (3) Any <entity id="J97-4004.14">tokenized string</entity> can be reproduced from a critically <entity id="J97-4004.15">tokenized word string</entity> but not vice versa; (4) Critical <entity id="J97-4004.16">tokenization forms</entity> the sound mathematical foundation for categorizing <entity id="J97-4004.17">tokenization ambiguity</entity> into critical and hidden types, a precise mathematical understanding of conventional concepts like combinational and overlapping <entity id="J97-4004.18">ambiguities</entity>; (5) Many important maximum <entity id="J97-4004.19">tokenization variations</entity>, such as forward and backward maximum matching and shortest <entity id="J97-4004.20">tokenization</entity>, are all true subclasses of critical <entity id="J97-4004.21">tokenization</entity>.It is believed that critical <entity id="J97-4004.22">tokenization</entity> provides a precise mathematical description of the principle of maximum <entity id="J97-4004.23">tokenization</entity>. Important implications and practical applications of critical <entity id="J97-4004.24">tokenization</entity> in effective <entity id="J97-4004.25">ambiguity resolution</entity> and in efficient <entity id="J97-4004.26">tokenization</entity> implementation are also carefully examined.
</abstract>

</text>

<text id="I08-1002">
<title>An Empirical Comparison of Goodness Measures for Unsupervised Chinese Word Segmentation with a Unified Framework</title>
<abstract>
This paper reports our empirical evaluation and comparison of several popular <entity id="I08-1002.1">goodness measures</entity> for <entity id="I08-1002.2">unsupervised segmentation</entity> of <entity id="I08-1002.3">Chinese texts</entity> using <entity id="I08-1002.4">Bakeoff-3 data sets</entity> with a unified framework. Assuming no prior <entity id="I08-1002.5">knowledge</entity> about <entity id="I08-1002.6">Chinese</entity>, this framework relies on a <entity id="I08-1002.7">goodness measure</entity> to identify <entity id="I08-1002.8">word candidates</entity> from <entity id="I08-1002.9">unlabeled texts</entity> and then applies a generalized <entity id="I08-1002.10">decoding algorithm</entity> to find the optimal <entity id="I08-1002.11">segmentation</entity> of a <entity id="I08-1002.12">sentence</entity> into such <entity id="I08-1002.13">candidates</entity> with the greatest sum of <entity id="I08-1002.14">goodness scores</entity>. Experiments show that description length gain outperforms other measures because of its strength for identifying <entity id="I08-1002.15">short words</entity>. Further performance improvement is also reported, achieved by <entity id="I08-1002.16">proper candidate pruning</entity> and by assemble <entity id="I08-1002.17">segmentation</entity> to integrate the strengths of individual measures.
</abstract>

</text>

<text id="W05-0628">
<title>Semantic Role Labeling As Sequential Tagging</title>
<abstract>
In this paper we present a <entity id="W05-0628.1">semantic role labeling system</entity> submitted to the <entity id="W05-0628.2">CoNLL-2005</entity> shared task. The system makes use of partial and full <entity id="W05-0628.3">syntactic information</entity> and converts the task into a <entity id="W05-0628.4">sequential BlO-tagging</entity>. As a result, the <entity id="W05-0628.5">labeling architecture</entity> is very simple. Building on a state-of-the-art <entity id="W05-0628.6">set of features</entity>, a <entity id="W05-0628.7">binary classifier</entity> for each <entity id="W05-0628.8">label</entity> is trained using <entity id="W05-0628.9">AdaBoost</entity> with fixed depth <entity id="W05-0628.10">decision trees</entity>. The final system, which combines the outputs of two base systems performed <entity id="W05-0628.11">Fi=76.59</entity> on the official <entity id="W05-0628.12">test set</entity>. Additionally, we provide results comparing the system when using <entity id="W05-0628.13">partial vs. full parsing</entity> input information.
</abstract>

</text>

<text id="W06-1114">
<title>Total Rank Distance And Scaled Total Rank Distance: Two Alternative Metrics In Computational Linguistics</title>
<abstract>
In this paper we propose two <entity id="W06-1114.1">metrics</entity> to be used in various fields of <entity id="W06-1114.2">computational linguistics</entity> area. Our construction is based on the supposition that in most of the <entity id="W06-1114.3">natural languages</entity> the most important information is carried by the first part of the unit. We introduce <entity id="W06-1114.4">total rank distance</entity> and <entity id="W06-1114.5">scaled total rank distance</entity>, we prove that they are <entity id="W06-1114.6">metrics</entity> and investigate their max and expected values. Finally, a short application is presented: we investigate the <entity id="W06-1114.7">similarity</entity> of <entity id="W06-1114.8">Romance languages</entity> by computing the <entity id="W06-1114.9">scaled total rank distance</entity> between the digram rankings of each <entity id="W06-1114.10">language</entity>.
</abstract>

</text>

<text id="W07-1109">
<title>Learning Dependency Relations of Japanese Compound Functional Expressions</title>
<abstract>
This paper proposes an approach of processing <entity id="W07-1109.1">Japanese compound functional expressions</entity> by identifying them and analyzing their <entity id="W07-1109.2">dependency relations</entity> through a <entity id="W07-1109.3">machine learning technique</entity>. First, we formalize the task of identifying <entity id="W07-1109.4">Japanese compound functional expressions</entity> in a <entity id="W07-1109.5">text</entity> as a <entity id="W07-1109.6">machine learning based chunking problem</entity>. Next, against the results of identifying compound <entity id="W07-1109.7">functional expressions</entity>, we apply the method of <entity id="W07-1109.8">dependency analysis</entity> based on the <entity id="W07-1109.9">cascaded chunking model</entity>. The results of experimental evaluation show that, the <entity id="W07-1109.10">dependency analysis model</entity> achieves improvements when applied after identifying compound <entity id="W07-1109.11">functional expressions</entity>, compared with the case where it is applied without identifying <entity id="W07-1109.12">compound functional expressions</entity>.
</abstract>

</text>

<text id="D07-1024">
<title>Semi-Supervised Classification for Extracting Protein Interaction Sentences using Dependency Parsing</title>
<abstract>
We introduce a <entity id="D07-1024.1">relation extraction method</entity> to identify the <entity id="D07-1024.2">sentences</entity> in <entity id="D07-1024.3">biomedical text</entity> that indicate an <entity id="D07-1024.4">interaction</entity> among the <entity id="D07-1024.5">protein names</entity> mentioned. Our approach is based on the analysis of the paths between two <entity id="D07-1024.6">protein names</entity> in the <entity id="D07-1024.7">dependency parse trees</entity> of the <entity id="D07-1024.8">sentences</entity>. Given two <entity id="D07-1024.9">dependency trees</entity>, we define two separate <entity id="D07-1024.10">similarity functions (kernels)</entity> based on <entity id="D07-1024.11">cosine similarity</entity> and <entity id="D07-1024.12">edit distance</entity> among the paths between the <entity id="D07-1024.13">protein names</entity>. Using these <entity id="D07-1024.14">similarity functions</entity>, we investigate the performances of two classes of <entity id="D07-1024.15">learning algorithms</entity>, <entity id="D07-1024.16">Support Vector Machines</entity> and <entity id="D07-1024.17">k-nearest-neighbor</entity>, and the semi-supervised counterparts of these algorithms, <entity id="D07-1024.18">transductive SVMs and harmonic functions</entity>, respectively. Significant improvement over the previous results in the literature is reported as well as a new benchmark dataset is introduced. <entity id="D07-1024.19">Semi-supervised algorithms</entity> perform better than their supervised version by a wide margin especially when the amount of <entity id="D07-1024.20">labeled data</entity> is limited.
</abstract>

</text>

<text id="P07-1085">
<title>Unsupervised Language Model Adaptation Incorporating Named Entity Information</title>
<abstract><entity id="P07-1085.1">Language model (LM) adaptation</entity> is important for both <entity id="P07-1085.2">speech and language processing</entity>. It is often achieved by combining a generic <entity id="P07-1085.3">LM</entity> with a <entity id="P07-1085.4">topic-specific model</entity> that is more relevant to the <entity id="P07-1085.5">target document</entity>. Unlike previous work on <entity id="P07-1085.6">unsupervised LM adaptation</entity>, this paper investigates how effectively using <entity id="P07-1085.7">named entity (NE) information</entity>, instead of considering all the <entity id="P07-1085.8">words</entity>, helps <entity id="P07-1085.9">LM adaptation</entity>. We evaluate two <entity id="P07-1085.10">latent topic analysis approaches</entity> in this paper, namely, <entity id="P07-1085.11">clustering</entity> and <entity id="P07-1085.12">Latent Dirichlet Allocation (LDA)</entity>. In addition, a new dynamically adapted <entity id="P07-1085.13">weighting scheme</entity> for <entity id="P07-1085.14">topic mixture models</entity> is proposed based on <entity id="P07-1085.15">LDA topic analysis</entity>. Our experimental results show that the <entity id="P07-1085.16">NE-driven LM adaptation</entity> framework outperforms the baseline generic <entity id="P07-1085.17">LM</entity>. The best result is obtained using the <entity id="P07-1085.18">LDA-based approach</entity> by expanding the <entity id="P07-1085.19">named entities</entity> with <entity id="P07-1085.20">syntactically filtered words</entity>, together with using a large number of <entity id="P07-1085.21">topics</entity>, which yields a <entity id="P07-1085.22">perplexity reduction</entity> of 14.23% compared to the baseline generic <entity id="P07-1085.23">LM</entity>.
</abstract>

</text>

<text id="C08-1005">
<title>Improving Alignments for Better Confusion Networks for Combining Machine Translation Systems</title>
<abstract>
The state-of-the-art system <entity id="C08-1005.1">combination method</entity> for <entity id="C08-1005.2">machine translation (MT)</entity> is the <entity id="C08-1005.3">word-based combination</entity> using <entity id="C08-1005.4">confusion networks</entity>. One of the crucial steps in <entity id="C08-1005.5">confusion network decoding</entity> is the <entity id="C08-1005.6">alignment</entity> of different hypotheses to each other when building a network. In this paper, we present new methods to improve <entity id="C08-1005.7">alignment</entity> of hypotheses using <entity id="C08-1005.8">word synonyms</entity> and a two-pass <entity id="C08-1005.9">alignment</entity> strategy. We demonstrate that combination with the new <entity id="C08-1005.10">alignment</entity> technique yields up to 2.9 <entity id="C08-1005.11">BLEU point improvement</entity> over the best input system and up to 1.3 <entity id="C08-1005.12">BLEU point improvement</entity> over a state-of-the-art <entity id="C08-1005.13">combination method</entity> on two different <entity id="C08-1005.14">language pairs</entity>.
</abstract>

</text>

<text id="N07-1052">
<title>Approximate Factoring for A* Search</title>
<abstract>
We present a novel method for creating <entity id="N07-1052.1">A* estimates</entity> for <entity id="N07-1052.2">structured search problems</entity>. In our approach, we project a <entity id="N07-1052.3">complex model</entity> onto multiple simpler models for which <entity id="N07-1052.4">exact inference</entity> is efficient. We use an <entity id="N07-1052.5">optimization framework</entity> to estimate <entity id="N07-1052.6">parameters</entity> for these <entity id="N07-1052.7">projections</entity> in a way which bounds the true costs. Similar to Klein and Manning (2003), we then combine completion estimates from the simpler models to guide <entity id="N07-1052.8">search</entity> in the original <entity id="N07-1052.9">complex model</entity>. We apply our approach to <entity id="N07-1052.10">bitext parsing</entity> and <entity id="N07-1052.11">lexicalized parsing</entity>, demonstrating its effectiveness in these <entity id="N07-1052.12">domains</entity>.
</abstract>

</text>

<text id="P06-2039">
<title>Parsing Aligned Parallel Corpus By Projecting Syntactic Relations From Annotated Source Corpus</title>
<abstract><entity id="P06-2039.1">Example-based parsing</entity> has already been proposed in literature. In particular, attempts are being made to develop techniques for <entity id="P06-2039.2">language pairs</entity> where the <entity id="P06-2039.3">source</entity> and <entity id="P06-2039.4">target languages</entity> are different, e.g. <entity id="P06-2039.5">Direct Projection Algorithm</entity> (Hwa et al., 2005). This enables one to develop <entity id="P06-2039.6">parsed corpus</entity> for <entity id="P06-2039.7">target languages</entity> having fewer <entity id="P06-2039.8">linguistic tools</entity> with the help of a <entity id="P06-2039.9">resource-rich source language</entity>. The <entity id="P06-2039.10">DPA algorithm</entity> works on the <entity id="P06-2039.11">assumption of Direct Correspondence</entity> which simply means
that the relation between two <entity id="P06-2039.12">words</entity> of the <entity id="P06-2039.13">source language sentence</entity> can be projected directly between the corresponding <entity id="P06-2039.14">words</entity> of the <entity id="P06-2039.15">parallel target language sentence</entity>. However, we find that this assumption does not hold good all the time. This leads to wrong <entity id="P06-2039.16">parsed structure </entity>of the <entity id="P06-2039.17">target language sentence</entity>. As a solution we propose an algorithm called <entity id="P06-2039.18">pseudo DPA (pDPA)</entity> that can work even if <entity id="P06-2039.19">Direct Correspondence assumption</entity> is not guaranteed. The proposed algorithm works in a recursive manner by considering the embedded <entity id="P06-2039.20">phrase structures</entity> from outermost level to the innermost. The present work discusses the <entity id="P06-2039.21">pDPA algorithm</entity>, and illustrates it with respect to <entity id="P06-2039.22">English-Hindi language pair</entity>. <entity id="P06-2039.23">Link Grammar based parsing</entity> has been considered as the underlying <entity id="P06-2039.24">parsing scheme</entity> for this work.
</abstract>

</text>

<text id="C88-1049">
<title>Chart Parsing According To The Slot And Filler Principle</title>
<abstract>
A <entity id="C88-1049.1">parser</entity> is an algorithm that assigns a <entity id="C88-1049.2">structural description</entity> to a <entity id="C88-1049.3">string</entity> according to a <entity id="C88-1049.4">grammar</entity>. It follows from this definition that there are three general issues in <entity id="C88-1049.5">parser design</entity>: the structure to be assigned, the <entity id="C88-1049.6">type of grammar</entity>, the <entity id="C88-1049.7">recognition algorithm</entity>. Common <entity id="C88-1049.8">parsers</entity> employ <entity id="C88-1049.9">phrase structure descriptions</entity>, <entity id="C88-1049.10">rule-based grammars</entity>, and <entity id="C88-1049.11">derivation</entity> or <entity id="C88-1049.12">transition oriented recognition</entity>. The following choices result in a new <entity id="C88-1049.13">parser</entity>: The structure to be assigned to the input is a <entity id="C88-1049.14">dependency tree</entity> with <entity id="C88-1049.15">lexical, morpho-syntactic</entity> and <entity id="C88-1049.16">functional-syntactic information</entity> associated with each node and coded by <entity id="C88-1049.17">complex categories</entity> which are subject to <entity id="C88-1049.18">unification</entity>. The <entity id="C88-1049.19">grammar</entity> is lexicalized, i.e. the <entity id="C88-1049.20">syntactical relationships</entity> are stated as part of the <entity id="C88-1049.21">lexical descriptions</entity> of the elements of the <entity id="C88-1049.22">language</entity>. The algorithm relies on the <entity id="C88-1049.23">slot and filler principle</entity> in order to draw up complex structures. It utilizes a well-formed <entity id="C88-1049.24">substring table (chart)</entity> which allows for discontinuous <entity id="C88-1049.25">segments</entity>.
</abstract>

</text>

<text id="C96-1008">
<title>Communication In Large Distributed AI Systems For Natural Language Processing</title>
<abstract>
We are going to describe the design and implementation of a <entity id="C96-1008.1">communication system</entity> for large <entity id="C96-1008.2">AI</entity> projects, capable of supporting various software components in a heterogeneous hardware and programming-language environment. The system is based on a modification of the <entity id="C96-1008.3">channel approach</entity> introduced by Hoare (1978). It is a <entity id="C96-1008.4">three-layered approach</entity> with a de facto standard network layer (PVM.), core routines, and interfaces to five different programming languages together with support for the transparent exchange of <entity id="C96-1008.5">complex data types</entity>. A special component takes over name service functions. It also records the actual configuration of the >modules present in the application and the created channels. We describe the integration of this communication facility in two versions of a <entity id="C96-1008.6">speech-to-speech translation system</entity>, which dilfer with regard to quality and quantity of data distributed within the applications and with regard to the degree of interactivity involved in processing.
</abstract>

</text>

<text id="E03-1004">
<title>Czech-English Dependency Tree-Based Machine Translation</title>
<abstract>
We present some preliminary results of a <entity id="E03-1004.1">Czech-English translation system</entity> based on <entity id="E03-1004.2">dependency trees</entity>. The fully automated process includes: <entity id="E03-1004.3">morphological tagging</entity>, <entity id="E03-1004.4">analytical and tectogrammatical parsing</entity> of <entity id="E03-1004.5">Czech</entity>, <entity id="E03-1004.6">tectogrammatical transfer</entity> based on <entity id="E03-1004.7">lexical substitution</entity> using <entity id="E03-1004.8">word-to-word translation dictionaries</entity> enhanced by the information from the <entity id="E03-1004.9">English-Czech parallel corpus</entity> of WSJ, and a <entity id="E03-1004.10">simple</entity> <entity id="E03-1004.11">rule-based system</entity> for <entity id="E03-1004.12">generation</entity> from <entity id="E03-1004.13">English tectogrammatical representation</entity>. In the evaluation part, we compare results of the <entity id="E03-1004.14">fully automated and the manually annotated processes</entity> of building the <entity id="E03-1004.15">tectogrammatical representation</entity>.
</abstract>

</text>

<text id="C04-1037">
<title>Optimizing Disambiguation In Swahili</title>
<abstract>
It is argued in this paper that an optimal solution to <entity id="C04-1037.1">disambiguation</entity> is a combination of linguistically motivated <entity id="C04-1037.2">rules</entity> and resolution based on <entity id="C04-1037.3">probability or heuristic rules</entity>. By <entity id="C04-1037.4">disambiguation</entity> is here meant <entity id="C04-1037.5">ambiguity resolution</entity> on all levels of <entity id="C04-1037.6">language analysis</entity>, including <entity id="C04-1037.7">morphology</entity> and <entity id="C04-1037.8">semantics</entity>. The discussion is based on <entity id="C04-1037.9">Swahili</entity>, for which a comprehensive <entity id="C04-1037.10">analysis system</entity> has been developed by using <entity id="C04-1037.11">two-level description</entity> in <entity id="C04-1037.12">morphology</entity> and <entity id="C04-1037.13">constraint grammar formalism</entity> in <entity id="C04-1037.14">disambiguation</entity>. Particular attention is paid to optimising the use of different solutions for achieving maximal <entity id="C04-1037.15">precision</entity> with minimal <entity id="C04-1037.16">rule writing</entity>.
</abstract>

</text>

<text id="C96-2129">
<title>Automatic Detection Of Omissions In Translations</title>
<abstract>
<entity id="C96-2129.1">ADOMIT</entity> is an algorithm for <entity id="C96-2129.2">Automatic Detection</entity> of <entity id="C96-2129.3">OMIssions</entity> in <entity id="C96-2129.4">Translations</entity>. The algorithm relies solely on <entity id="C96-2129.5">geometric analysis</entity> of <entity id="C96-2129.6">bitext maps</entity> and uses no <entity id="C96-2129.7">linguistic information</entity>. This property allows it to deal equally well with omissions that do not correspond to <entity id="C96-2129.8">linguistic units</entity>, such as might result from <entity id="C96-2129.9">word-processing mishaps</entity>. <entity id="C96-2129.10">ADOMIT</entity> has proven itself by discovering many errors in a <entity id="C96-2129.11">hand-constructed gold standard</entity> for evaluating <entity id="C96-2129.12">bitext mapping algorithms</entity>. Quantitative evaluation on simulated omissions showed that, even with today's poor <entity id="C96-2129.13">bitext mapping technology</entity>, <entity id="C96-2129.14">ADOMIT</entity> is a valuable quality control tool for <entity id="C96-2129.15">translators</entity> and <entity id="C96-2129.16">translation bureaus</entity>.
</abstract>

</text>

<text id="L08-1561">
<title>Spatiotemporal Annotation Using MiniSTEx: how to deal with Alternative, Foreign, Vague and/or Obsolete Names?</title>
<abstract>
We are currently developing <entity id="L08-1561.1">MiniSTEx</entity>, a <entity id="L08-1561.2">spatiotemporal annotation system</entity> to handle <entity id="L08-1561.3">temporal and/or geospatial information</entity> directly and indirectly expressed in <entity id="L08-1561.4">texts</entity>. In the end, the aim is to locate all eventualities in a <entity id="L08-1561.5">text</entity> on a time axis and/or a map to ensure an <entity id="L08-1561.6">optimal base</entity> for <entity id="L08-1561.7">automatic temporal and geospatial reasoning</entity>. A first version of <entity id="L08-1561.8">MiniSTEx</entity> was originally developed for <entity id="L08-1561.9">Dutch</entity>, keeping in mind that it should also be useful for other <entity id="L08-1561.10">European languages</entity>, and for <entity id="L08-1561.11">multilingual applications</entity>. In order to meet these desiderata we need the <entity id="L08-1561.12">MiniSTEx system</entity> to be able to draw the conclusions <entity id="L08-1561.13">human readers</entity> belonging to the <entity id="L08-1561.14">intended audience</entity> would also draw, e.g. based on their <entity id="L08-1561.15">(spatiotemporal) world knowledge</entity>, i.e. the <entity id="L08-1561.16">common knowledge</entity> such <entity id="L08-1561.17">readers</entity> share. The <entity id="L08-1561.18">world knowledge</entity> <entity id="L08-1561.19">MiniSTEx</entity> uses is contained in <entity id="L08-1561.20">interconnected tables</entity> in a <entity id="L08-1561.21">database</entity>. At the moment it is used for <entity id="L08-1561.22">Dutch</entity> and <entity id="L08-1561.23">English</entity>. Special attention will be paid to the problems we face when looking at <entity id="L08-1561.24">older texts</entity> or <entity id="L08-1561.25">recent historical or encyclopedic texts</entity>, i.e. <entity id="L08-1561.26">texts</entity> with lots of <entity id="L08-1561.27">references</entity> to times and locations that are not compatible with our current maps and calendars.
</abstract>

</text>

<text id="E03-1030">
<title>Semantic Construction In F-TAG</title>
<abstract>
We propose a <entity id="E03-1030.1">semantic construction method</entity> for <entity id="E03-1030.2">Feature-Based Tree Adjoining Grammar</entity> which is based on the <entity id="E03-1030.3">derived tree</entity>, compare it with related proposals and briefly discuss some <entity id="E03-1030.4">implementation possibilities</entity>.
</abstract>

</text>

<text id="M91-1021">
<title>BBN: Description Of The PLUM System As Used For MUC-3</title>
<abstract>Traditional approaches to the problem of extracting <entity id="M91-1021.1">data</entity> from <entity id="M91-1021.2">texts</entity> have emphasized <entity id="M91-1021.3">handcrafted linguistic knowledge</entity>. In contrast, <entity id="M91-1021.4">BBN's PLUM system (Probabilistic Language Understanding Model)</entity> was developed as part of a <entity id="M91-1021.5">DARPA</entity>-funded research effort on integrating <entity id="M91-1021.6">probabilistic language models</entity> with more <entity id="M91-1021.7">traditional linguistic techniques</entity>. Our research and development goals are more rapid development of <entity id="M91-1021.8">new applications</entity>, the ability to train (and re-train) systems based on <entity id="M91-1021.9">user markings</entity> of <entity id="M91-1021.10">correct and incorrect output</entity>, more accurate selection among <entity id="M91-1021.11">interpretations</entity> when more than one is found, an more <entity id="M91-1021.12">robust partial interpretation</entity> when no <entity id="M91-1021.13">complete interpretation</entity> can be found. We have previously performed experiments on components of the system with <entity id="M91-1021.14">texts</entity> from the <entity id="M91-1021.15">Wall Street Journal</entity>, however, the <entity id="M91-1021.16">MUC-3 task</entity> is the first <entity id="M91-1021.17">end-to-end application</entity> of <entity id="M91-1021.18">plum</entity>. All components except <entity id="M91-1021.19">parsing</entity> were developed in the last 5 months, and cannot therefore be considered fully mature. The <entity id="M91-1021.20">parsing component, the MIT Fast Parser</entity> [4], originated outside <entity id="M91-1021.21">BBN</entity> and has a more extensive history prior to <entity id="M91-1021.22">MUC-3</entity>. A central assumption of our approach is that in processing <entity id="M91-1021.23">unrestricted text</entity> for <entity id="M91-1021.24">data extraction</entity>, a non-trivial amount of the <entity id="M91-1021.25">text</entity> will not be understood. As a result, all components of <entity id="M91-1021.26">plum</entity> are designed to operate on <entity id="M91-1021.27">partially understood input</entity>, taking advantage of <entity id="M91-1021.28">information</entity> when available, and not failing when <entity id="M91-1021.29">information</entity> is unavailable. The following section describes the major <entity id="M91-1021.30">plum components</entity>.
</abstract>

</text>

<text id="A00-1043">
<title>Sentence Reduction For Automatic Text Summarization</title>
<abstract>
We present a novel <entity id="A00-1043.1">sentence reduction system</entity> for automatically removing <entity id="A00-1043.2">extraneous phrases</entity> from <entity id="A00-1043.3">sentences</entity> that are extracted from a <entity id="A00-1043.4">document</entity> for <entity id="A00-1043.5">summarization purpose</entity>. The system uses multiple <entity id="A00-1043.6">sources of knowledge</entity> to decide which <entity id="A00-1043.7">phrases</entity> in an extracted <entity id="A00-1043.8">sentence</entity> can be removed, including <entity id="A00-1043.9">syntactic knowledge</entity>, <entity id="A00-1043.10">context information</entity>, and <entity id="A00-1043.11">statistics</entity> computed from a <entity id="A00-1043.12">corpus</entity> which consists of examples written by human professionals. <entity id="A00-1043.13">Reduction</entity> can significantly improve the <entity id="A00-1043.14">conciseness</entity> of <entity id="A00-1043.15">automatic summaries</entity>.
</abstract>

</text>

<text id="H94-1040">
<title>Combining Knowledge Sources To Reorder N-Best Speech Hypothesis Lists</title>
<abstract>
A simple and general method is described that can combine different <entity id="H94-1040.1">knowledge sources</entity> to reorder <entity id="H94-1040.2">N-best lists</entity> of <entity id="H94-1040.3">hypotheses</entity> produced by a <entity id="H94-1040.4">speech recognizer</entity>. The method is automatically trainable, acquiring <entity id="H94-1040.5">information</entity> from both <entity id="H94-1040.6">positive and negative examples</entity>. In experiments, the method was tested on a 1000-utterance sample of unseen <entity id="H94-1040.7">ATIS data</entity>.
</abstract>

</text>

<text id="W93-0103">
<title>Lexical Concept Acquisition From Collocation Map</title>
<abstract>
This paper introduces an algorithm for automatically acquiring the <entity id="W93-0103.1">conceptual structure</entity> of each <entity id="W93-0103.2">word</entity> from <entity id="W93-0103.3">corpus</entity>. The <entity id="W93-0103.4">concept</entity> of a <entity id="W93-0103.5">word</entity> is defined within the <entity id="W93-0103.6">probabilistic framework</entity>. A variation of <entity id="W93-0103.7">Belief Net</entity> named as <entity id="W93-0103.8">Collocation Map</entity> is used to compute the <entity id="W93-0103.9">probabilities</entity>. The <entity id="W93-0103.10">Belief Net</entity> captures the <entity id="W93-0103.11">conditional independences</entity> of <entity id="W93-0103.12">words</entity>, which is obtained from the <entity id="W93-0103.13">cooccurrence relations</entity>. The <entity id="W93-0103.14">computation</entity> in general <entity id="W93-0103.15">Belief Nets</entity> is known to be NP-hard, so we adopted <entity id="W93-0103.16">Gibbs sampling</entity> for the <entity id="W93-0103.17">approximation</entity> of the <entity id="W93-0103.18">probabilities</entity>.The use of <entity id="W93-0103.19">Belief Net</entity> to model the <entity id="W93-0103.20">lexical meaning</entity> is unique in that the <entity id="W93-0103.21">network</entity> is larger than expected in most other <entity id="W93-0103.22">applications</entity>, and this changes the attitude toward the use of <entity id="W93-0103.23">Belief Net</entity>. The <entity id="W93-0103.24">lexical concept</entity> obtained from the <entity id="W93-0103.25">Collocation Map</entity> best reflects the <entity id="W93-0103.26">subdomain</entity> of <entity id="W93-0103.27">language usage</entity>. The potential application of <entity id="W93-0103.28">conditional probabilities</entity> the <entity id="W93-0103.29">Collocation Map</entity> provides may extend to cover very diverse areas of <entity id="W93-0103.30">language processing</entity> such as <entity id="W93-0103.31">sense disambiguation</entity>, <entity id="W93-0103.32">thesaurus construction</entity>, <entity id="W93-0103.33">automatic indexing</entity>, and <entity id="W93-0103.34">document classification</entity>.
</abstract>

</text>

<text id="W97-1104">
<title>Prediction Of Vowel And Consonant Place Of Articulation</title>
<abstract>
A <entity id="W97-1104.1">deductive approach</entity> is used to predict <entity id="W97-1104.2">vowel and consonant places of articulation</entity>. Based on two main criteria, viz. simple and efficient use of an <entity id="W97-1104.3">acoustic tube</entity>, along with <entity id="W97-1104.4">maximum acoustic dispersion</entity>, the <entity id="W97-1104.5">Distinctive Regions Model (DRM)</entity> of <entity id="W97-1104.6">speech production</entity> derives <entity id="W97-1104.7">regions</entity> that closely correspond to established <entity id="W97-1104.8"> vowel and consonant places of articulation</entity>.
</abstract>

</text>

<text id="H05-1050">
<title>Bootstrapping Without The Boot</title>
<abstract>
<entity id="H05-1050.1">"Bootstrapping" methods</entity> for <entity id="H05-1050.2">learning</entity> require a small amount of <entity id="H05-1050.3">supervision</entity> to seed the <entity id="H05-1050.4">learning process</entity>. We show that it is sometimes possible to eliminate this last bit of <entity id="H05-1050.5">supervision</entity>, by trying many <entity id="H05-1050.6">candidate seeds</entity> and selecting the one with the most <entity id="H05-1050.7">plausible outcome</entity>. We discuss such <entity id="H05-1050.8">"strapping" methods</entity> in general, and exhibit a particular method for strapping <entity id="H05-1050.9">word-sense classifiers</entity> for <entity id="H05-1050.10">ambiguous words</entity>. Our experiments on the <entity id="H05-1050.11">Canadian Hansards</entity> show that our <entity id="H05-1050.12">unsupervised technique</entity> is significantly more effective than picking <entity id="H05-1050.13">seeds</entity> by hand (Yarowsky, 1995), which in turn is known to rival <entity id="H05-1050.14">supervised methods</entity>. "
</abstract>

</text>

<text id="W01-1414">
<title>Adding Domain Specificity To An MT System</title>
<abstract>
In the development of a <entity id="W01-1414.1">machine translation system</entity>, one important issue is being able to adapt to a <entity id="W01-1414.2">specific domain</entity> without requiring time-consuming lexical work. We have experimented with using a <entity id="W01-1414.3">statistical word-alignment algorithm</entity> to derive <entity id="W01-1414.4">word association pairs (French-English)</entity> that complement an existing <entity id="W01-1414.5">multipurpose bilingual dictionary</entity>. This <entity id="W01-1414.6">word association information</entity> is added to the <entity id="W01-1414.7">system</entity> at the time of the automatic creation of our <entity id="W01-1414.8">translation pattern database</entity>, thereby making this <entity id="W01-1414.9">database</entity> more domain specific. This <entity id="W01-1414.10">technique</entity> significantly improves the overall quality of <entity id="W01-1414.11">translation</entity>, as measured in an <entity id="W01-1414.12">independent blind evaluation</entity>.
</abstract>

</text>

<text id="W08-0320">
<title>Improving English-Spanish Statistical Machine Translation: Experiments in Domain Adaptation, Sentence Paraphrasing, Tokenization, and Recasing</title>
<abstract>
We describe the experiments of the UC Berkeley team on improving <entity id="W08-0320.1">English-Spanish machine translation</entity> of <entity id="W08-0320.2">news text</entity>, as part of the <entity id="W08-0320.3">WMT'08 Shared Translation Task</entity>. We experiment with <entity id="W08-0320.4">domain adaptation</entity>, combining a <entity id="W08-0320.5">small in-domain news bi-text</entity> and a large out-of-domain one from the <entity id="W08-0320.6">Europarl corpus</entity>, building two <entity id="W08-0320.7">separate phrase translation models</entity> and two <entity id="W08-0320.8">separate language models</entity>. We further add a <entity id="W08-0320.9">third phrase translation model</entity> trained on a version of the <entity id="W08-0320.10">news bi-text</entity> augmented with <entity id="W08-0320.11">monolingual sentence-level syntactic paraphrases</entity> on the source-language side, and we combine all <entity id="W08-0320.12">models</entity> in a <entity id="W08-0320.13">log-linear model</entity> using <entity id="W08-0320.14">minimum error rate training</entity>. Finally, we experiment with different <entity id="W08-0320.15">tokenization and recasing rules</entity>, achieving 35.09% <entity id="W08-0320.16">Bleu score</entity> on the <entity id="W08-0320.17">WMT'07 news test data</entity> when translating from <entity id="W08-0320.18">English</entity> to <entity id="W08-0320.19">Spanish</entity>, which is a sizable improvement over the <entity id="W08-0320.20">highest Bleu score</entity> achieved on that dataset at <entity id="W08-0320.21">WMT'07</entity>: 33.10% (in fact, by our system). On the <entity id="W08-0320.22">WMT'08 English to Spanish news translation</entity>, we achieve 21.92%, which makes our team the second best on <entity id="W08-0320.23">Bleu score</entity>.
</abstract>

</text>

<text id="W08-0103">
<title>Learning N-Best Correction Models from Implicit User Feedback in a Multi-Modal Local Search Application</title>
<abstract>
We describe a novel <entity id="W08-0103.1">n-best correction model</entity> that can leverage <entity id="W08-0103.2">implicit user feedback</entity> (in the form of clicks) to improve performance in a <entity id="W08-0103.3">multi-modal speech-search application</entity>. The proposed model works in two stages. First, the <entity id="W08-0103.4">n-best list</entity> generated by the <entity id="W08-0103.5">speech recognizer</entity> is expanded with <entity id="W08-0103.6">additional candidates</entity>, based on <entity id="W08-0103.7">confusability information</entity> captured via <entity id="W08-0103.8">user click statistics</entity>. In the second stage, this <entity id="W08-0103.9">expanded list</entity> is rescored and pruned to produce a more accurate and compact <entity id="W08-0103.10">n-best list</entity>. Results indicate that the proposed <entity id="W08-0103.11">n-best correction model</entity> leads to significant improvements over the <entity id="W08-0103.12">existing baseline</entity>, as well as other traditional <entity id="W08-0103.13">n-best rescoring approaches</entity>.
</abstract>

</text>

<text id="W04-0707">
<title>Discourse-New Detectors For Definite Description Resolution: A Survey And A Preliminary Proposal</title>
<abstract>
Vieira and Poesio (2000) proposed an algorithm for <entity id="W04-0707.1">definite description (dd) resolution</entity> that incorporates a number of <entity id="W04-0707.2">heuristics</entity> for detecting <entity id="W04-0707.3">discourse-new descriptions</entity>. The inclusion of such <entity id="W04-0707.4">detectors</entity> was motivated by the observation that more than 50% of <entity id="W04-0707.5">definite descriptions (dds)</entity> in an average <entity id="W04-0707.6">corpus</entity> are <entity id="W04-0707.7">discourse new</entity> (Poesio and Vieira, 1998), but whereas the inclusion of <entity id="W04-0707.8">detectors</entity> for <entity id="W04-0707.9">non-anaphoric pronouns</entity> in algorithms such as <entity id="W04-0707.10">Lap-pin and Leass' (1994)</entity> leads to clear improvements in precision, the improvements in <entity id="W04-0707.11">anaphoric dd resolution</entity> (as opposed to <entity id="W04-0707.12">classification</entity>) brought about by the <entity id="W04-0707.13">detectors</entity> were rather small. In fact, Ng and Cardie (2002a) challenged the motivation for the inclusion of such <entity id="W04-0707.14">detectors</entity>, reporting no improvements, or even worse performance. We re-examine the literature on the topic in detail, and propose a <entity id="W04-0707.15">revised algorithm</entity>, taking advantage of the improved <entity id="W04-0707.16">discourse-new detection techniques</entity> developed by Uryupina (2003).
</abstract>

</text>

<text id="W06-0202">
<title>Comparing Information Extraction Pattern Models</title>
<abstract>
Several recently reported techniques for the <entity id="W06-0202.1">automatic acquisition</entity> of <entity id="W06-0202.2">Information Extraction (IE) systems</entity> have used <entity id="W06-0202.3">dependency trees</entity> as the basis of their <entity id="W06-0202.4">extraction pattern representation</entity>. These approaches have used a variety of <entity id="W06-0202.5">pattern models</entity> (<entity id="W06-0202.6">schemes</entity> for representing <entity id="W06-0202.7">IE patterns</entity> based on particular parts of the <entity id="W06-0202.8">dependency analysis</entity>). An appropriate model should be expressive enough to represent the <entity id="W06-0202.9">information</entity> which is to be extracted from <entity id="W06-0202.10">text</entity> without being overly complicated. Four previously reported <entity id="W06-0202.11">pattern models</entity> are evaluated using existing <entity id="W06-0202.12">IE evaluation corpora</entity> and three <entity id="W06-0202.13">dependency parsers</entity>. It was found that one model, <entity id="W06-0202.14">linked chains</entity>, could represent around 95% of the <entity id="W06-0202.15">information of interest</entity> without generating an unwieldy number of <entity id="W06-0202.16">possible patterns</entity>.
</abstract>

</text>

<text id="I08-1058">
<title>Multilingual Text Entry using Automatic Language Detection</title>
<abstract><entity id="I08-1058.1">Computer users</entity> increasingly need to produce <entity id="I08-1058.2">text</entity> written in <entity id="I08-1058.3">multiple languages</entity>. However, typical <entity id="I08-1058.4">computer interfaces</entity> require the user to change the <entity id="I08-1058.5">text entry software</entity> each time a different <entity id="I08-1058.6">language</entity> is used. This is cumbersome, especially when <entity id="I08-1058.7">language changes</entity> are frequent. To solve this problem, we propose <entity id="I08-1058.8">TypeAny</entity>, a novel <entity id="I08-1058.9">front-end interface</entity> that detects the <entity id="I08-1058.10">language</entity> of the <entity id="I08-1058.11">user's key entry</entity> and automatically dispatches the <entity id="I08-1058.12">input</entity> to the appropriate <entity id="I08-1058.13">text entry system</entity>. Unlike previously reported methods, <entity id="I08-1058.14">TypeAny</entity> can handle more than two <entity id="I08-1058.15">languages</entity>, and can easily support any new <entity id="I08-1058.16">language</entity> even if the available <entity id="I08-1058.17">corpus</entity> is small. When evaluating this method, we obtained <entity id="I08-1058.18">language detection accuracy</entity> of 96.7% when an appropriate <entity id="I08-1058.19">language</entity> had to be chosen from among three <entity id="I08-1058.20">languages</entity>. The number of <entity id="I08-1058.21">control actions</entity> needed to switch <entity id="I08-1058.22">languages</entity> was decreased over 93% when using <entity id="I08-1058.23">TypeAny</entity> rather than a <entity id="I08-1058.24">conventional method</entity>.
</abstract>

</text>

<text id="W07-1402">
<title>A Semantic Approach To Textual Entailment: System Evaluation and Task Analysis</title>
<abstract>
This paper discusses our contribution to the third <entity id="W07-1402.1">RTE Challenge</entity> - the <entity id="W07-1402.2">SALSA RTE system</entity>. It builds on an earlier system based on a relatively <entity id="W07-1402.3">deep linguistic analysis</entity>, which we complement with a <entity id="W07-1402.4">shallow component</entity> based on <entity id="W07-1402.5">word overlap</entity>. We evaluate their (combined) performance on various <entity id="W07-1402.6">data sets</entity>. However, earlier observations that the combination of <entity id="W07-1402.7">features</entity> improves the <entity id="W07-1402.8">overall accuracy</entity> could be replicated only partly.
</abstract>

</text>

<text id="D07-1088">
<title>Extracting Data Records from Unstructured Biomedical Full Text</title>
<abstract>
In this paper, we address the problem of extracting <entity id="D07-1088.1">data records</entity> and their <entity id="D07-1088.2">attributes</entity> from <entity id="D07-1088.3">unstructured biomedical full text</entity>. There has been little effort reported on this in the research community. We argue that <entity id="D07-1088.4">semantics</entity> is important for <entity id="D07-1088.5">record extraction</entity> or finer-grained <entity id="D07-1088.6">language processing tasks</entity>. We derive a <entity id="D07-1088.7">data record template</entity> including <entity id="D07-1088.8">semantic language models</entity> from <entity id="D07-1088.9">unstructured text</entity> and represent them with a <entity id="D07-1088.10">discourse level Conditional Random Fields (CRF) model</entity>. We evaluate the approach from the perspective of <entity id="D07-1088.11">Information Extraction</entity> and achieve significant improvements on <entity id="D07-1088.12">system performance</entity> compared with other <entity id="D07-1088.13">baseline systems</entity>.
</abstract>

</text>

<text id="P08-1001">
<title>Mining Wiki Resources for Multilingual Named Entity Recognition</title>
<abstract>
In this paper, we describe a system by which the <entity id="P08-1001.1">multilingual characteristics</entity> of <entity id="P08-1001.2">Wikipedia</entity> can be utilized to annotate a <entity id="P08-1001.3">large corpus</entity> of <entity id="P08-1001.4">text</entity> with <entity id="P08-1001.5">Named Entity Recognition (NER) tags</entity> requiring minimal human intervention and no <entity id="P08-1001.6">linguistic expertise</entity>. This process, though of value in <entity id="P08-1001.7">languages</entity> for which <entity id="P08-1001.8">resources</entity> exist, is particularly useful for less commonly taught <entity id="P08-1001.9">languages</entity>. We show how the <entity id="P08-1001.10">Wikipedia</entity> format can be used to identify possible <entity id="P08-1001.11">named entities</entity> and discuss in detail the <entity id="P08-1001.12">process</entity> by which we use the <entity id="P08-1001.13">Category structure</entity> inherent to <entity id="P08-1001.14"> Wikipedia</entity> to determine the <entity id="P08-1001.15">named entity type</entity> of a <entity id="P08-1001.16">proposed entity</entity>. We further describe the methods by which <entity id="P08-1001.17">English language data</entity> can be used to bootstrap the <entity id="P08-1001.18">NER process</entity> in other <entity id="P08-1001.19">languages</entity>. We demonstrate the system by using the <entity id="P08-1001.20">generated corpus</entity> as <entity id="P08-1001.21">training sets</entity> for a variant of <entity id="P08-1001.22">BBN's Identifinder</entity> in <entity id="P08-1001.23">French</entity>, <entity id="P08-1001.24">Ukrainian</entity>, <entity id="P08-1001.25">Spanish</entity>, <entity id="P08-1001.26">Polish</entity>, <entity id="P08-1001.27">Russian</entity>, and <entity id="P08-1001.28">Portuguese</entity>, achieving overall <entity id="P08-1001.29">F-scores</entity> as high as 84.7% on independent, <entity id="P08-1001.30">human-annotated corpora</entity>, comparable to a system trained on up to 40,000 <entity id="P08-1001.31">words</entity> of <entity id="P08-1001.32">human-annotated newswire</entity>.
</abstract>

</text>

<text id="J08-1001">
<title>Modeling Local Coherence: An Entity-Based Approach</title>
<abstract>
This article proposes a novel framework for representing and measuring <entity id="J08-1001.1">local coherence</entity>. Central to this approach is the <entity id="J08-1001.2">entity-grid representation</entity> of <entity id="J08-1001.3">discourse</entity>, which captures <entity id="J08-1001.4">patterns</entity> of <entity id="J08-1001.5">entity distribution</entity> in a <entity id="J08-1001.6">text</entity>. The algorithm introduced in the article automatically abstracts a <entity id="J08-1001.7">text</entity> into a set of <entity id="J08-1001.8">entity transition sequences</entity> and records <entity id="J08-1001.9">distributional, syntactic, and referential information</entity> about <entity id="J08-1001.10">discourse entities</entity>. We re-conceptualize <entity id="J08-1001.11">coherence assessment</entity> as a <entity id="J08-1001.12">learning task</entity> and show that our <entity id="J08-1001.13">entity-based representation</entity> is well-suited for <entity id="J08-1001.14">ranking-based generation</entity> and <entity id="J08-1001.15">text classification tasks</entity>. Using the <entity id="J08-1001.16">proposed representation</entity>, we achieve good performance on <entity id="J08-1001.17">text ordering</entity>, <entity id="J08-1001.18">summary coherence evaluation</entity>, and <entity id="J08-1001.19">readability assessment</entity>.
</abstract>

</text>

<text id="P04-1007">
<title>Discriminative Language Modeling With Conditional Random Fields And The Perceptron Algorithm</title>
<abstract>
This paper describes <entity id="P04-1007.1">discriminative language modeling</entity> for a large <entity id="P04-1007.2">vocabulary speech recognition task</entity>. We contrast two <entity id="P04-1007.3">parameter estimation methods</entity>: the <entity id="P04-1007.4">perceptron algorithm</entity>, and a method based on <entity id="P04-1007.5">conditional random fields (CRFs)</entity>. The <entity id="P04-1007.6">models</entity> are encoded as <entity id="P04-1007.7">deterministic weighted finite state automata</entity>, and are applied by intersecting the <entity id="P04-1007.8">automata</entity> with <entity id="P04-1007.9">word-lattices</entity> that are the output from a <entity id="P04-1007.10">baseline recognizer</entity>. The <entity id="P04-1007.11">perceptron algorithm</entity> has the benefit of automatically selecting a relatively small <entity id="P04-1007.12">feature set</entity> in just a couple of passes over the <entity id="P04-1007.13">training data</entity>. However, using the <entity id="P04-1007.14">feature set output</entity> from the <entity id="P04-1007.15">perceptron algorithm</entity> (initialized with their <entity id="P04-1007.16">weights</entity>), <entity id="P04-1007.17">CRF training</entity> provides an additional 0.5% reduction in <entity id="P04-1007.18">word error rate</entity>, for a total 1.8% absolute reduction from the <entity id="P04-1007.19">baseline</entity> of 39.2%.
</abstract>

</text>

<text id="P06-4013">
<title>Archivus: A Multimodal System For Multimedia Meeting Browsing And Retrieval</title>
<abstract>
This paper presents <entity id="P06-4013.1">Archivus, a multimodal language-enabled meeting browsing and retrieval system</entity>. The <entity id="P06-4013.2">prototype</entity> is in an early stage of development, and we are currently exploring the role of <entity id="P06-4013.3">natural language</entity> for interacting in this relatively unfamiliar and complex domain. We briefly describe the design and <entity id="P06-4013.4">implementation status</entity> of the <entity id="P06-4013.5">system</entity>, and then focus on how this <entity id="P06-4013.6">system</entity> is used to elicit <entity id="P06-4013.7">useful data</entity> for supporting <entity id="P06-4013.8">hypotheses</entity> about <entity id="P06-4013.9">multimodal interaction</entity> in the domain of <entity id="P06-4013.10">meeting retrieval</entity> and for developing <entity id="P06-4013.11">NLP modules</entity> for this specific domain.
</abstract>

</text>

<text id="C90-2018">
<title>Feature Logic With Disjunctive Unification</title>
<abstract>
We introduce <entity id="C90-2018.1">feature terms</entity> containing sorts, <entity id="C90-2018.2">variables</entity>, <entity id="C90-2018.3">negation</entity> and <entity id="C90-2018.4">named disjunction</entity> for the specification of <entity id="C90-2018.5">feature structures</entity>. We show that the possibility to label distinctions with <entity id="C90-2018.6">names</entity> has major advantages both for the use of <entity id="C90-2018.7">feature logic</entity> in <entity id="C90-2018.8">computational linguistics</entity> and its implementation. We give an <entity id="C90-2018.9">open world semantics</entity> for <entity id="C90-2018.10">feature terms</entity>, where the <entity id="C90-2018.11">denotation</entity> of a <entity id="C90-2018.12">term</entity> is determined in dependence on the <entity id="C90-2018.13">disjunctive context</entity>, i.e. the choices taken for the <entity id="C90-2018.14">disjunctions</entity>. We define <entity id="C90-2018.15">context-unique feature descriptions</entity>, a <entity id="C90-2018.16">relational, constraint-based representation language</entity> and give a <entity id="C90-2018.17">normalization procedure</entity> that allows to test <entity id="C90-2018.18">consistency</entity> of <entity id="C90-2018.19">feature terms</entity>. This procedure does not only avoid expansion to <entity id="C90-2018.20">disjunctive normal form</entity> but maintains also <entity id="C90-2018.21">structure sharing</entity> between <entity id="C90-2018.22">information</entity> contained in different <entity id="C90-2018.23">disjuncts</entity> as much as possible. <entity id="C90-2018.24">Context-unique feature descriptions</entity> can be easily implemented in environments that support <entity id="C90-2018.25">ordinary unification</entity> (such as <entity id="C90-2018.26">PROLOG</entity>).
</abstract>

</text>

<text id="L08-1588">
<title>MTriage: Web-enabled Software for the Creation, Machine Translation, and Annotation of Smart Documents</title>
<abstract>Progress in the <entity id="L08-1588.1">Machine Translation (MT) research community</entity>, particularly for <entity id="L08-1588.2">statistical approaches</entity>, is intensely <entity id="L08-1588.3">data-driven</entity>. Acquiring <entity id="L08-1588.4">source language documents</entity> for testing, creating <entity id="L08-1588.5">training datasets</entity> for <entity id="L08-1588.6">customized MT lexicons</entity>, and building <entity id="L08-1588.7">parallel corpora</entity> for <entity id="L08-1588.8">MT evaluation</entity> require <entity id="L08-1588.9">translators</entity> and <entity id="L08-1588.10">non-native speaking analysts</entity> to handle large <entity id="L08-1588.11">document collections</entity>. These collections are further complicated by differences in format, <entity id="L08-1588.12">encoding</entity>, <entity id="L08-1588.13">source media</entity>, and access to <entity id="L08-1588.14">metadata</entity> describing the <entity id="L08-1588.15">documents</entity>. Automated tools that allow <entity id="L08-1588.16">language professionals</entity> to quickly annotate, translate, and evaluate <entity id="L08-1588.17">foreign language documents</entity> are essential to improving <entity id="L08-1588.18">MT</entity> quality and efficacy. The purpose of this paper is present our research approach to improving <entity id="L08-1588.19">MT</entity> through pre-processing <entity id="L08-1588.20">source language documents</entity>. In particular, we will discuss the development and use of <entity id="L08-1588.21">MTriage</entity>, an application environment that enables the <entity id="L08-1588.22">translator</entity> to markup <entity id="L08-1588.23">documents</entity> with <entity id="L08-1588.24">metadata</entity> for <entity id="L08-1588.25">MT parameterization and routing</entity>. The use of <entity id="L08-1588.26">MTriage</entity> as a <entity id="L08-1588.27">web-enabled front end</entity> to multiple <entity id="L08-1588.28">MT engines</entity> has leveraged the capabilities of our <entity id="L08-1588.29">human translators</entity> for creating <entity id="L08-1588.30">lexicons</entity> from <entity id="L08-1588.31">NFW (Not-Found-Word) lists</entity>, writing <entity id="L08-1588.32">reference translations</entity>, and creating <entity id="L08-1588.33">parallel corpora</entity> for <entity id="L08-1588.34">MT</entity> development and evaluation.
</abstract>

</text>

<text id="C96-2146">
<title>Analyzing Japanese Double-Subject Construction Having An Adjective Predicate</title>
<abstract>
This paper describes a method for analyzing <entity id="C96-2146.1">Japanese double-subject construction</entity> having an <entity id="C96-2146.2">adjective predicate</entity> based on the <entity id="C96-2146.3">valency structure</entity>. A <entity id="C96-2146.4">simple sentence</entity> usually has only one <entity id="C96-2146.5">subjective case</entity> in most <entity id="C96-2146.6">languages</entity>. However, many <entity id="C96-2146.7">Japanese adjectives</entity> (and some <entity id="C96-2146.8">verbs</entity>) can dominate two <entity id="C96-2146.9">surface subjective cases</entity> within a <entity id="C96-2146.10">simple sentence</entity>. Such <entity id="C96-2146.11">sentence structure</entity> is called the <entity id="C96-2146.12">double-subject construction</entity>. This paper classifies the <entity id="C96-2146.13">Japanese double-subject construction</entity> into four types and describes problems arising when analyzing these types using ordinary <entity id="C96-2146.14">Japanese construction approaches</entity>. This paper proposes a method for analyzing a <entity id="C96-2146.15">Japanese double-subject construction</entity> having an <entity id="C96-2146.16">adjective predicate</entity> in order to overcome thee problems described. By applying this method to <entity id="C96-2146.17">Japanese sentence analysis</entity> in <entity id="C96-2146.18">Japanese-to-English machine translation systems</entity>, <entity id="C96-2146.19">translation accuracy</entity> can be improved because this method can analyze correctly the <entity id="C96-2146.20">double-subject construction</entity>.
</abstract>

</text>

<text id="C04-1163">
<title>A Semantic-Based Approach To Interoperabiltity Of Classification Hierarchies: Evaluation Of Linguistic Techniques</title>
<abstract><entity id="C04-1163.1">Classification Hierarchies (CHs)</entity> are widely used to organize <entity id="C04-1163.2">documents</entity> in a way that makes their <entity id="C04-1163.3">retrieval</entity> easier. Common examples of <entity id="C04-1163.4">CHs</entity> are <entity id="C04-1163.5">Web directories</entity>, marketplace catalogs, and <entity id="C04-1163.6">file systems</entity>. In this paper we discuss and evaluate <entity id="C04-1163.7">CtxMatch</entity>, an approach to interoperability that discovers <entity id="C04-1163.8">mappings</entity> among <entity id="C04-1163.9">CHs</entity> considering the <entity id="C04-1163.10">semantic interpretation</entity> of their <entity id="C04-1163.11">nodes</entity>. <entity id="C04-1163.12">CtxMatch</entity> performs a <entity id="C04-1163.13">linguistic processing</entity> of the <entity id="C04-1163.14">labels</entity> attached to the <entity id="C04-1163.15">nodes</entity>, including <entity id="C04-1163.16">tokenization</entity>, <entity id="C04-1163.17">Part of Speech tagging</entity>, <entity id="C04-1163.18">multiword recognition</entity> and <entity id="C04-1163.19">word sense disambiguation</entity>. We present an evaluation of the overall performance of the approach over <entity id="C04-1163.20">Web directories</entity> as well as a systematic analysis of the <entity id="C04-1163.21">linguistic modules</entity> involved.
</abstract>

</text>

<text id="C94-1014">
<title>A Matching Technique in Example-Based Machine Translation</title>
<abstract>This paper addresses an important problem in <entity id="C94-1014.1">Example-Based Machine Translation (EBMT)</entity>, namely how to measure <entity id="C94-1014.2">similarity</entity> between a <entity id="C94-1014.3">sentence fragment</entity> and a set of <entity id="C94-1014.4">stored examples</entity>. A new method is proposed that measures <entity id="C94-1014.5">similarity</entity> according to both <entity id="C94-1014.6">surface structure</entity> and content. A second contribution is the use of <entity id="C94-1014.7">clustering</entity> to make <entity id="C94-1014.8">retrieval</entity> of the <entity id="C94-1014.9">best matching example</entity> from the <entity id="C94-1014.10">database</entity> more efficient. Results on a large number of test cases from the <entity id="C94-1014.11">CELEX database</entity> are presented.
</abstract>

</text>

<text id="L08-1302">
<title>Connecting Text Mining and Pathways using the PathText Resource</title>
<abstract>Many systems have been developed in the past few years to assist researchers in the discovery of <entity id="L08-1302.1">knowledge</entity> published as <entity id="L08-1302.2">English text</entity>, for example in the <entity id="L08-1302.3">PubMed database</entity>. At the same time, <entity id="L08-1302.4">higher level collective knowledge</entity> is often published using a <entity id="L08-1302.5">graphical notation</entity> representing all the <entity id="L08-1302.6">entities</entity> in a <entity id="L08-1302.7">pathway</entity> and their <entity id="L08-1302.8">interactions</entity>. We believe that these <entity id="L08-1302.9">pathway visualizations</entity> could serve as an effective <entity id="L08-1302.10">user interface</entity> for <entity id="L08-1302.11">knowledge discovery</entity> if they can be linked to the text in publications. Since the <entity id="L08-1302.12">graphical elements</entity> in a <entity id="L08-1302.13">Pathway</entity> are of a very different nature than their corresponding <entity id="L08-1302.14">descriptions</entity> in <entity id="L08-1302.15">English text</entity>, we developed a <entity id="L08-1302.16">prototype system called PathText</entity>. The goal of <entity id="L08-1302.17">PathText</entity> is to serve as a bridge between these two different <entity id="L08-1302.18">representations</entity>. In this paper, we first describe the overall architecture and the <entity id="L08-1302.19">interfaces</entity> of the <entity id="L08-1302.20">PathText system</entity>, and then provide some details about the core <entity id="L08-1302.21">Text Mining components</entity>.
</abstract>

</text>


</doc>