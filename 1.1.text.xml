<?xml version="1.0" encoding="UTF-8" ?>
<doc>


<text id="H01-1001"> <title>Activity detection for information access to oral communication</title> <abstract> <entity id="H01-1001.1">Oral communication</entity> is ubiquitous and carries important information yet it is also time consuming to document. Given the development of <entity id="H01-1001.2">storage media and networks</entity> one could just record and store a <entity id="H01-1001.3">conversation</entity> for documentation. The question is, however, how an interesting information piece would be found in a <entity id="H01-1001.4">large database</entity> . Traditional <entity id="H01-1001.5">information retrieval techniques</entity> use a <entity id="H01-1001.6">histogram</entity> of <entity id="H01-1001.7">keywords</entity> as the <entity id="H01-1001.8">document representation</entity> but <entity id="H01-1001.9">oral communication</entity> may offer additional <entity id="H01-1001.10">indices</entity> such as the time and place of the rejoinder and the attendance. An alternative <entity id="H01-1001.11">index</entity> could be the activity such as discussing, planning, informing, story-telling, etc. This paper addresses the problem of the <entity id="H01-1001.12">automatic detection</entity> of those activities in meeting situation and everyday rejoinders. Several extensions of this basic idea are being discussed and/or evaluated: Similar to activities one can define subsets of larger <entity id="H01-1001.13">database</entity> and detect those automatically which is shown on a large <entity id="H01-1001.14">database</entity> of <entity id="H01-1001.15">TV shows</entity> . <entity id="H01-1001.16">Emotions</entity> and other <entity id="H01-1001.17">indices</entity> such as the <entity id="H01-1001.18">dominance distribution of speakers</entity> might be available on the <entity id="H01-1001.19">surface</entity> and could be used directly. Despite the small size of the <entity id="H01-1001.20">databases</entity> used some results about the effectiveness of these <entity id="H01-1001.21">indices</entity> can be obtained. </abstract>


</text>

<text id="H01-1017"> <title>Dialogue Interaction with the DARPA Communicator Infrastructure: The Development of Useful Software</title> <abstract> To support engaging human users in robust, <entity id="H01-1017.1">mixed-initiative speech dialogue interactions</entity> which reach beyond current capabilities in <entity id="H01-1017.2">dialogue systems</entity> , the <entity id="H01-1017.3">DARPA Communicator program</entity> [1] is funding the development of a <entity id="H01-1017.4">distributed message-passing infrastructure</entity> for <entity id="H01-1017.5">dialogue systems</entity> which all <entity id="H01-1017.6">Communicator</entity> participants are using. In this presentation, we describe the features of and <entity id="H01-1017.7">requirements</entity> for a genuinely useful <entity id="H01-1017.8">software infrastructure</entity> for this purpose. </abstract>


</text>

<text id="H01-1041"> <title>Interlingua-Based Broad-Coverage Korean-to-English Translation in CCLING</title> <abstract> At MIT Lincoln Laboratory, we have been developing a <entity id="H01-1041.1">Korean-to-English machine translation system</entity> <entity id="H01-1041.2">CCLINC (Common Coalition Language System at Lincoln Laboratory)</entity> . The <entity id="H01-1041.3">CCLINC Korean-to-English translation system</entity> consists of two <entity id="H01-1041.4">core modules</entity> , <entity id="H01-1041.5">language understanding and generation modules</entity> mediated by a <entity id="H01-1041.6">language neutral meaning representation</entity> called a <entity id="H01-1041.7">semantic frame</entity> . The key features of the system include: (i) Robust efficient <entity id="H01-1041.8">parsing</entity> of <entity id="H01-1041.9">Korean</entity> (a <entity id="H01-1041.10">verb final language</entity> with <entity id="H01-1041.11">overt case markers</entity> , relatively <entity id="H01-1041.12">free word order</entity> , and frequent omissions of <entity id="H01-1041.13">arguments</entity> ). (ii) High quality <entity id="H01-1041.14">translation</entity> via <entity id="H01-1041.15">word sense disambiguation</entity> and accurate <entity id="H01-1041.16">word order generation</entity> of the <entity id="H01-1041.17">target language</entity> . (iii) <entity id="H01-1041.18">Rapid system development</entity> and porting to new <entity id="H01-1041.19">domains</entity> via <entity id="H01-1041.20">knowledge-based automated acquisition of grammars</entity> . Having been trained on <entity id="H01-1041.21">Korean newspaper articles</entity> on missiles and chemical biological warfare, the system produces the <entity id="H01-1041.22">translation output</entity> sufficient for content understanding of the <entity id="H01-1041.23">original document</entity> . </abstract>


</text>

<text id="H01-1042"> <title>Is That Your Final Answer?</title> <abstract> The purpose of this research is to test the efficacy of applying <entity id="H01-1042.1">automated evaluation techniques</entity> , originally devised for the evaluation of <entity id="H01-1042.2">human language learners</entity> , to the <entity id="H01-1042.3">output</entity> of <entity id="H01-1042.4">machine translation (MT) systems</entity> . We believe that these <entity id="H01-1042.5">evaluation techniques</entity> will provide information about both the <entity id="H01-1042.6">human language learning process</entity> , the <entity id="H01-1042.7">translation process</entity> and the <entity id="H01-1042.8">development</entity> of <entity id="H01-1042.9">machine translation systems</entity> . This, the first experiment in a series of experiments, looks at the <entity id="H01-1042.10">intelligibility</entity> of <entity id="H01-1042.11">MT output</entity> . A <entity id="H01-1042.12">language learning experiment</entity> showed that <entity id="H01-1042.13">assessors</entity> can differentiate <entity id="H01-1042.14">native from non-native language essays</entity> in less than 100 <entity id="H01-1042.15">words</entity> . Even more illuminating was the factors on which the <entity id="H01-1042.16">assessors</entity> made their decisions. We tested this to see if similar criteria could be elicited from duplicating the experiment using <entity id="H01-1042.17">machine translation output</entity> . Subjects were given a set of up to six extracts of <entity id="H01-1042.18">translated newswire text</entity> . Some of the extracts were <entity id="H01-1042.19">expert human translations</entity> , others were <entity id="H01-1042.20">machine translation outputs</entity> . The subjects were given three minutes per extract to determine whether they believed the sample output to be an <entity id="H01-1042.21">expert human translation</entity> or a <entity id="H01-1042.22">machine translation</entity> . Additionally, they were asked to mark the <entity id="H01-1042.23">word</entity> at which they made this decision. The results of this experiment, along with a preliminary analysis of the factors involved in the decision making process will be presented here. </abstract>


</text>

<text id="H01-1049"> <title>Listen-Communicate-Show (LCS): Spoken Language Command of Agent-based Remote Information Access</title> <abstract> <entity id="H01-1049.1">Listen-Communicate-Show (LCS)</entity> is a new paradigm for <entity id="H01-1049.2">human interaction with data sources</entity> . We integrate a <entity id="H01-1049.3">spoken language understanding system</entity> with <entity id="H01-1049.4">intelligent mobile agents</entity> that mediate between <entity id="H01-1049.5">users</entity> and <entity id="H01-1049.6">information sources</entity> . We have built and will demonstrate an application of this approach called <entity id="H01-1049.7">LCS-Marine</entity> . Using <entity id="H01-1049.8">LCS-Marine</entity> , tactical personnel can converse with their logistics system to place a supply or information request. The request is passed to a <entity id="H01-1049.9">mobile, intelligent agent</entity> for execution at the appropriate <entity id="H01-1049.10">database</entity> . <entity id="H01-1049.11">Requestors</entity> can also instruct the system to notify them when the status of a <entity id="H01-1049.12">request</entity> changes or when a <entity id="H01-1049.13">request</entity> is complete. We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in <entity id="H01-1049.14">new domains</entity> . </abstract>


</text>

<text id="H01-1058"> <title>On Combining Language Models : Oracle Approach</title> <abstract> In this paper, we address the problem of combining several <entity id="H01-1058.1">language models (LMs)</entity> . We find that simple <entity id="H01-1058.2">interpolation methods</entity> , like <entity id="H01-1058.3">log-linear and linear interpolation</entity> , improve the <entity id="H01-1058.4">performance</entity> but fall short of the <entity id="H01-1058.5">performance</entity> of an <entity id="H01-1058.6">oracle</entity> . The <entity id="H01-1058.7">oracle</entity> knows the <entity id="H01-1058.8">reference word string</entity> and selects the <entity id="H01-1058.9">word string</entity> with the best <entity id="H01-1058.10">performance</entity> (typically, <entity id="H01-1058.11">word or semantic error rate</entity> ) from a list of <entity id="H01-1058.12">word strings</entity> , where each <entity id="H01-1058.13">word string</entity> has been obtained by using a different <entity id="H01-1058.14">LM</entity> . Actually, the <entity id="H01-1058.15">oracle</entity> acts like a <entity id="H01-1058.16">dynamic combiner</entity> with <entity id="H01-1058.17">hard decisions</entity> using the <entity id="H01-1058.18">reference</entity> . We provide experimental results that clearly show the need for a <entity id="H01-1058.19">dynamic language model combination</entity> to improve the <entity id="H01-1058.20">performance</entity> further . We suggest a method that mimics the behavior of the <entity id="H01-1058.21">oracle</entity> using a <entity id="H01-1058.22">neural network</entity> or a <entity id="H01-1058.23">decision tree</entity> . The method amounts to tagging <entity id="H01-1058.24">LMs</entity> with <entity id="H01-1058.25">confidence measures</entity> and picking the best <entity id="H01-1058.26">hypothesis</entity> corresponding to the <entity id="H01-1058.27">LM</entity> with the best <entity id="H01-1058.28">confidence</entity> . </abstract>


</text>

<text id="H01-1070"> <title>Towards an Intelligent Multilingual Keyboard System</title> <abstract> This paper proposes a practical approach employing <entity id="H01-1070.1">n-gram models</entity> and <entity id="H01-1070.2">error-correction rules</entity> for <entity id="H01-1070.3">Thai key prediction</entity> and <entity id="H01-1070.4">Thai-English language identification</entity> . The paper also proposes <entity id="H01-1070.5">rule-reduction algorithm</entity> applying <entity id="H01-1070.6">mutual information</entity> to reduce the <entity id="H01-1070.7">error-correction rules</entity> . Our algorithm reported more than 99% <entity id="H01-1070.8">accuracy</entity> in both <entity id="H01-1070.9">language identification</entity> and <entity id="H01-1070.10">key prediction</entity> . </abstract>


</text>

<text id="N01-1003"> <title>SPoT: A Trainable Sentence Planner</title> <abstract> <entity id="N01-1003.1">Sentence planning</entity> is a set of inter-related but distinct tasks, one of which is <entity id="N01-1003.2">sentence scoping</entity> , i.e. the choice of <entity id="N01-1003.3">syntactic structure</entity> for elementary <entity id="N01-1003.4">speech acts</entity> and the decision of how to combine them into one or more <entity id="N01-1003.5">sentences</entity> . In this paper, we present <entity id="N01-1003.6">SPoT</entity> , a <entity id="N01-1003.7">sentence planner</entity> , and a new methodology for automatically training <entity id="N01-1003.8">SPoT</entity> on the basis of <entity id="N01-1003.9">feedback</entity> provided by <entity id="N01-1003.10">human judges</entity> . We reconceptualize the task into two distinct phases. First, a very simple, <entity id="N01-1003.11">randomized sentence-plan-generator (SPG)</entity> generates a potentially large list of possible <entity id="N01-1003.12">sentence plans</entity> for a given <entity id="N01-1003.13">text-plan input</entity> . Second, the <entity id="N01-1003.14">sentence-plan-ranker (SPR)</entity> ranks the list of output <entity id="N01-1003.15">sentence plans</entity> , and then selects the top-ranked <entity id="N01-1003.16">plan</entity> . The <entity id="N01-1003.17">SPR</entity> uses <entity id="N01-1003.18">ranking rules</entity> automatically learned from <entity id="N01-1003.19">training data</entity> . We show that the trained <entity id="N01-1003.20">SPR</entity> learns to select a <entity id="N01-1003.21">sentence plan</entity> whose rating on average is only 5% worse than the <entity id="N01-1003.22">top human-ranked sentence plan</entity> . </abstract>


</text>

<text id="P01-1004"> <title>Low-cost, High-performance Translation Retrieval: Dumber is Better</title> <abstract> In this paper, we compare the relative effects of <entity id="P01-1004.1">segment order</entity> , <entity id="P01-1004.2">segmentation</entity> and <entity id="P01-1004.3">segment contiguity</entity> on the <entity id="P01-1004.4">retrieval performance</entity> of a <entity id="P01-1004.5">translation memory system</entity> . We take a selection of both <entity id="P01-1004.6">bag-of-words and segment order-sensitive string comparison methods</entity> , and run each over both <entity id="P01-1004.7">character- and word-segmented data</entity> , in combination with a range of <entity id="P01-1004.8">local segment contiguity models</entity> (in the form of <entity id="P01-1004.9">N-grams</entity> ). Over two distinct <entity id="P01-1004.10">datasets</entity> , we find that <entity id="P01-1004.11">indexing</entity> according to simple <entity id="P01-1004.12">character bigrams</entity> produces a <entity id="P01-1004.13">retrieval accuracy</entity> superior to any of the tested <entity id="P01-1004.14">word N-gram models</entity> . Further,in their optimum <entity id="P01-1004.15">configuration</entity> , <entity id="P01-1004.16">bag-of-words methods</entity> are shown to be equivalent to <entity id="P01-1004.17">segment order-sensitive methods</entity> in terms of <entity id="P01-1004.18">retrieval accuracy</entity> , but much faster. We also provide evidence that our findings are scalable. </abstract>


</text>

<text id="P01-1007"> <title>Guided Parsing of Range Concatenation Languages</title> <abstract> The theoretical study of the <entity id="P01-1007.1">range concatenation grammar [RCG] formalism</entity> has revealed many attractive properties which may be used in <entity id="P01-1007.2">NLP</entity> . In particular, <entity id="P01-1007.3">range concatenation languages [RCL]</entity> can be parsed in <entity id="P01-1007.4">polynomial time</entity> and many classical <entity id="P01-1007.5">grammatical formalisms</entity> can be translated into equivalent <entity id="P01-1007.6">RCGs</entity> without increasing their <entity id="P01-1007.7">worst-case parsing time complexity</entity> . For example, after <entity id="P01-1007.8">translation</entity> into an equivalent <entity id="P01-1007.9">RCG</entity> , any <entity id="P01-1007.10">tree adjoining grammar</entity> can be parsed in <entity id="P01-1007.11">O(n6) time</entity> . In this paper, we study a <entity id="P01-1007.12">parsing technique</entity> whose purpose is to improve the practical efficiency of <entity id="P01-1007.13">RCL parsers</entity> . The <entity id="P01-1007.14">non-deterministic parsing choices</entity> of the <entity id="P01-1007.15">main parser</entity> for a <entity id="P01-1007.16">language L</entity> are directed by a <entity id="P01-1007.17">guide</entity> which uses the <entity id="P01-1007.18">shared derivation forest</entity> output by a prior <entity id="P01-1007.19">RCL parser</entity> for a suitable <entity id="P01-1007.20">superset of L</entity> . The results of a practical evaluation of this method on a <entity id="P01-1007.21">wide coverage English grammar</entity> are given. </abstract>


</text>

<text id="P01-1008"> <title>Extracting Paraphrases from a Parallel Corpus</title> <abstract> While <entity id="P01-1008.1">paraphrasing</entity> is critical both for <entity id="P01-1008.2">interpretation and generation of natural language</entity> , current systems use manual or semi-automatic methods to collect <entity id="P01-1008.3">paraphrases</entity> . We present an <entity id="P01-1008.4">unsupervised learning algorithm</entity> for <entity id="P01-1008.5">identification of paraphrases</entity> from a <entity id="P01-1008.6">corpus of multiple English translations</entity> of the same <entity id="P01-1008.7">source text</entity> . Our approach yields <entity id="P01-1008.8">phrasal and single word lexical paraphrases</entity> as well as <entity id="P01-1008.9">syntactic paraphrases</entity> . </abstract>


</text>

<text id="P01-1009"> <title>Alternative Phrases and Natural Language Information Retrieval</title> <abstract> This paper presents a <entity id="P01-1009.1">formal analysis</entity> for a large class of <entity id="P01-1009.2">words</entity> called <entity id="P01-1009.3">alternative markers</entity> , which includes <entity id="P01-1009.4">other (than)</entity> , <entity id="P01-1009.5">such (as)</entity> , and <entity id="P01-1009.6">besides</entity> . These <entity id="P01-1009.7">words</entity> appear frequently enough in <entity id="P01-1009.8">dialog</entity> to warrant serious <entity id="P01-1009.9">attention</entity> , yet present <entity id="P01-1009.10">natural language search engines</entity> perform poorly on <entity id="P01-1009.11">queries</entity> containing them. I show that the <entity id="P01-1009.12">performance</entity> of a <entity id="P01-1009.13">search engine</entity> can be improved dramatically by incorporating an approximation of the <entity id="P01-1009.14">formal analysis</entity> that is compatible with the <entity id="P01-1009.15">search engine</entity> &apos;s <entity id="P01-1009.16">operational semantics</entity> . The value of this approach is that as the <entity id="P01-1009.17">operational semantics</entity> of <entity id="P01-1009.18">natural language applications</entity> improve, even larger improvements are possible. </abstract>


</text>

<text id="P01-1047"> <title>Extending Lambek grammars: a logical account of minimalist grammars</title> <abstract> We provide a <entity id="P01-1047.1">logical definition</entity> of <entity id="P01-1047.2">Minimalist grammars</entity> , that are <entity id="P01-1047.3">Stabler&apos;s formalization</entity> of <entity id="P01-1047.4">Chomsky&apos;s minimalist program</entity> . Our <entity id="P01-1047.5">logical definition</entity> leads to a neat relation to <entity id="P01-1047.6">categorial grammar</entity> , (yielding a treatment of <entity id="P01-1047.7">Montague semantics</entity> ), a <entity id="P01-1047.8">parsing-as-deduction</entity> in a <entity id="P01-1047.9">resource sensitive logic</entity> , and a <entity id="P01-1047.10">learning algorithm</entity> from <entity id="P01-1047.11">structured data</entity> (based on a <entity id="P01-1047.12">typing-algorithm</entity> and <entity id="P01-1047.13">type-unification</entity> ). Here we emphasize the connection to <entity id="P01-1047.14">Montague semantics</entity> which can be viewed as a <entity id="P01-1047.15">formal computation</entity> of the <entity id="P01-1047.16">logical form</entity> . </abstract>


</text>

<text id="P01-1056"> <title>Evaluating a Trainable Sentence Planner for a Spoken Dialogue System</title> <abstract> <entity id="P01-1056.1">Techniques for automatically training</entity> modules of a <entity id="P01-1056.2">natural language generator</entity> have recently been proposed, but a fundamental concern is whether the <entity id="P01-1056.3">quality</entity> of <entity id="P01-1056.4">utterances</entity> produced with <entity id="P01-1056.5">trainable components</entity> can compete with <entity id="P01-1056.6">hand-crafted template-based or rule-based approaches</entity> . In this paper We experimentally evaluate a <entity id="P01-1056.7">trainable sentence planner</entity> for a <entity id="P01-1056.8">spoken dialogue system</entity> by eliciting <entity id="P01-1056.9">subjective human judgments</entity> . In order to perform an exhaustive comparison, we also evaluate a <entity id="P01-1056.10">hand-crafted template-based generation component</entity> , two <entity id="P01-1056.11">rule-based sentence planners</entity> , and two <entity id="P01-1056.12">baseline sentence planners</entity> . We show that the <entity id="P01-1056.13">trainable sentence planner</entity> performs better than the <entity id="P01-1056.14">rule-based systems</entity> and the <entity id="P01-1056.15">baselines</entity> , and as well as the <entity id="P01-1056.16">hand-crafted system</entity> . </abstract>


</text>

<text id="P01-1070"> <title>Using Machine Learning Techniques to Interpret WH-questions</title> <abstract> We describe a set of <entity id="P01-1070.1">supervised machine learning</entity> experiments centering on the construction of <entity id="P01-1070.2">statistical models</entity> of <entity id="P01-1070.3">WH-questions</entity> . These <entity id="P01-1070.4">models</entity> , which are built from <entity id="P01-1070.5">shallow linguistic features</entity> of <entity id="P01-1070.6">questions</entity> , are employed to predict target variables which represent a <entity id="P01-1070.7">user&apos;s informational goals</entity> . We report on different aspects of the <entity id="P01-1070.8">predictive performance</entity> of our <entity id="P01-1070.9">models</entity> , including the influence of various <entity id="P01-1070.10">training and testing factors</entity> on <entity id="P01-1070.11">predictive performance</entity> , and examine the relationships among the target variables. </abstract>


</text>

<text id="N03-1001"> <title>Effective Utterance Classification with Unsupervised Phonotactic Models</title> <abstract> This paper describes a method for <entity id="N03-1001.1">utterance classification</entity> that does not require <entity id="N03-1001.2">manual transcription</entity> of <entity id="N03-1001.3">training data</entity> . The method combines <entity id="N03-1001.4">domain independent acoustic models</entity> with off-the-shelf <entity id="N03-1001.5">classifiers</entity> to give <entity id="N03-1001.6">utterance classification performance</entity> that is surprisingly close to what can be achieved using conventional <entity id="N03-1001.7">word-trigram recognition</entity> requiring <entity id="N03-1001.8">manual transcription</entity> . In our method, <entity id="N03-1001.9">unsupervised training</entity> is first used to train a <entity id="N03-1001.10">phone n-gram model</entity> for a particular <entity id="N03-1001.11">domain</entity> ; the <entity id="N03-1001.12">output</entity> of <entity id="N03-1001.13">recognition</entity> with this <entity id="N03-1001.14">model</entity> is then passed to a <entity id="N03-1001.15">phone-string classifier</entity> . The <entity id="N03-1001.16">classification accuracy</entity> of the method is evaluated on three different <entity id="N03-1001.17">spoken language system domains</entity> . </abstract>


</text>

<text id="N03-1004"> <title>In Question Answering, Two Heads Are Better Than One</title> <abstract> Motivated by the success of <entity id="N03-1004.1">ensemble methods</entity> in <entity id="N03-1004.2">machine learning</entity> and other areas of <entity id="N03-1004.3">natural language processing</entity> , we developed a <entity id="N03-1004.4">multi-strategy and multi-source approach to question answering</entity> which is based on combining the results from different <entity id="N03-1004.5">answering agents</entity> searching for <entity id="N03-1004.6">answers</entity> in multiple <entity id="N03-1004.7">corpora</entity> . The <entity id="N03-1004.8">answering agents</entity> adopt fundamentally different strategies, one utilizing primarily <entity id="N03-1004.9">knowledge-based mechanisms</entity> and the other adopting <entity id="N03-1004.10">statistical techniques</entity> . We present our <entity id="N03-1004.11">multi-level answer resolution algorithm</entity> that combines results from the <entity id="N03-1004.12">answering agents</entity> at the <entity id="N03-1004.13">question, passage, and/or answer levels</entity> . Experiments evaluating the effectiveness of our <entity id="N03-1004.14">answer resolution algorithm</entity> show a 35.0% relative improvement over our <entity id="N03-1004.15">baseline system</entity> in the number of <entity id="N03-1004.16">questions correctly answered</entity> , and a 32.8% improvement according to the <entity id="N03-1004.17">average precision metric</entity> . </abstract>


</text>

<text id="N03-1012"> <title>Semantic Coherence Scoring Using an Ontology</title> <abstract> In this paper we present <entity id="N03-1012.1">ONTOSCORE</entity> , a system for scoring sets of <entity id="N03-1012.2">concepts</entity> on the basis of an <entity id="N03-1012.3">ontology</entity> . We apply our system to the task of <entity id="N03-1012.4">scoring</entity> alternative <entity id="N03-1012.5">speech recognition hypotheses (SRH)</entity> in terms of their <entity id="N03-1012.6">semantic coherence</entity> . We conducted an <entity id="N03-1012.7">annotation experiment</entity> and showed that <entity id="N03-1012.8">human annotators</entity> can reliably differentiate between semantically coherent and incoherent <entity id="N03-1012.9">speech recognition hypotheses</entity> . An evaluation of our system against the <entity id="N03-1012.10">annotated data</entity> shows that, it successfully classifies 73.2% in a <entity id="N03-1012.11">German corpus</entity> of 2.284 <entity id="N03-1012.12">SRHs</entity> as either coherent or incoherent (given a <entity id="N03-1012.13">baseline</entity> of 54.55%). </abstract>


</text>

<text id="N03-1017"> <title>Statistical Phrase-Based Translation</title> <abstract> We propose a new <entity id="N03-1017.1">phrase-based translation model</entity> and <entity id="N03-1017.2">decoding algorithm</entity> that enables us to evaluate and compare several, previously proposed <entity id="N03-1017.3">phrase-based translation models</entity> . Within our framework, we carry out a large number of experiments to understand better and explain why <entity id="N03-1017.4">phrase-based models</entity> outperform <entity id="N03-1017.5">word-based models</entity> . Our empirical results, which hold for all examined <entity id="N03-1017.6">language pairs</entity> , suggest that the highest levels of performance can be obtained through relatively simple means: <entity id="N03-1017.7">heuristic learning</entity> of <entity id="N03-1017.8">phrase translations</entity> from <entity id="N03-1017.9">word-based alignments</entity> and <entity id="N03-1017.10">lexical weighting</entity> of <entity id="N03-1017.11">phrase translations</entity> . Surprisingly, learning <entity id="N03-1017.12">phrases</entity> longer than three <entity id="N03-1017.13">words</entity> and learning <entity id="N03-1017.14">phrases</entity> from <entity id="N03-1017.15">high-accuracy word-level alignment models</entity> does not have a strong impact on performance. Learning only <entity id="N03-1017.16">syntactically motivated phrases</entity> degrades the performance of our systems. </abstract>


</text>

<text id="N03-1018"> <title>A Generative Probabilistic OCR Model for NLP Applications</title> <abstract> In this paper, we introduce a <entity id="N03-1018.1">generative probabilistic optical character recognition (OCR) model</entity> that describes an end-to-end process in the <entity id="N03-1018.2">noisy channel framework</entity> , progressing from generation of <entity id="N03-1018.3">true text</entity> through its transformation into the <entity id="N03-1018.4">noisy output</entity> of an <entity id="N03-1018.5">OCR system</entity> . The <entity id="N03-1018.6">model</entity> is designed for use in <entity id="N03-1018.7">error correction</entity> , with a focus on <entity id="N03-1018.8">post-processing</entity> the <entity id="N03-1018.9">output</entity> of black-box <entity id="N03-1018.10">OCR systems</entity> in order to make it more useful for <entity id="N03-1018.11">NLP tasks</entity> . We present an implementation of the <entity id="N03-1018.12">model</entity> based on <entity id="N03-1018.13">finite-state models</entity> , demonstrate the <entity id="N03-1018.14">model</entity> &apos;s ability to significantly reduce <entity id="N03-1018.15">character and word error rate</entity> , and provide evaluation results involving <entity id="N03-1018.16">automatic extraction</entity> of <entity id="N03-1018.17">translation lexicons</entity> from <entity id="N03-1018.18">printed text</entity> . </abstract>


</text>

<text id="N03-1026"> <title>Statistical Sentence Condensation using Ambiguity Packing and Stochastic Disambiguation Methods for Lexical-Functional Grammar</title> <abstract> We present an application of <entity id="N03-1026.1">ambiguity packing and stochastic disambiguation techniques</entity> for <entity id="N03-1026.2">Lexical-Functional Grammars (LFG)</entity> to the domain of <entity id="N03-1026.3">sentence condensation</entity> . Our system incorporates a <entity id="N03-1026.4">linguistic parser/generator</entity> for <entity id="N03-1026.5">LFG</entity> , a <entity id="N03-1026.6">transfer component</entity> for <entity id="N03-1026.7">parse reduction</entity> operating on <entity id="N03-1026.8">packed parse forests</entity> , and a <entity id="N03-1026.9">maximum-entropy model</entity> for <entity id="N03-1026.10">stochastic output selection</entity> . Furthermore, we propose the use of standard <entity id="N03-1026.11">parser evaluation methods</entity> for automatically evaluating the <entity id="N03-1026.12">summarization</entity> quality of <entity id="N03-1026.13">sentence condensation systems</entity> . An <entity id="N03-1026.14">experimental evaluation</entity> of <entity id="N03-1026.15">summarization</entity> quality shows a close correlation between the <entity id="N03-1026.16">automatic parse-based evaluation</entity> and a <entity id="N03-1026.17">manual evaluation</entity> of generated <entity id="N03-1026.18">strings</entity> . Overall <entity id="N03-1026.19">summarization</entity> quality of the proposed system is state-of-the-art, with guaranteed <entity id="N03-1026.20">grammaticality</entity> of the <entity id="N03-1026.21">system output</entity> due to the use of a <entity id="N03-1026.22">constraint-based parser/generator</entity> . </abstract>


</text>

<text id="N03-1033"> <title>Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network</title> <abstract> We present a new <entity id="N03-1033.1">part-of-speech tagger</entity> that demonstrates the following ideas: (i) explicit use of both preceding and following <entity id="N03-1033.2">tag contexts</entity> via a <entity id="N03-1033.3">dependency network representation</entity> , (ii) broad use of <entity id="N03-1033.4">lexical features</entity> , including <entity id="N03-1033.5">jointly conditioning on multiple consecutive words</entity> , (iii) effective use of <entity id="N03-1033.6">priors</entity> in <entity id="N03-1033.7">conditional loglinear models</entity> , and (iv) fine-grained modeling of <entity id="N03-1033.8">unknown word features</entity> . Using these ideas together, the resulting <entity id="N03-1033.9">tagger</entity> gives a 97.24% <entity id="N03-1033.10">accuracy</entity> on the <entity id="N03-1033.11">Penn Treebank WSJ</entity> , an <entity id="N03-1033.12">error reduction</entity> of 4.4% on the best previous single automatically learned <entity id="N03-1033.13">tagging</entity> result. </abstract>


</text>

<text id="N03-2003"> <title>Getting More Mileage from Web Text Sources for Conversational Speech Language Modeling using Class-Dependent Mixtures</title> <abstract> Sources of <entity id="N03-2003.1">training data</entity> suitable for <entity id="N03-2003.2">language modeling</entity> of <entity id="N03-2003.3">conversational speech</entity> are limited. In this paper, we show how <entity id="N03-2003.4">training data</entity> can be supplemented with <entity id="N03-2003.5">text</entity> from the <entity id="N03-2003.6">web</entity> filtered to match the <entity id="N03-2003.7">style</entity> and/or <entity id="N03-2003.8">topic</entity> of the target <entity id="N03-2003.9">recognition task</entity> , but also that it is possible to get bigger performance gains from the <entity id="N03-2003.10">data</entity> by using <entity id="N03-2003.11">class-dependent interpolation</entity> of <entity id="N03-2003.12">N-grams</entity> . </abstract>


</text>

<text id="N03-2006"> <title>Adaptation Using Out-of-Domain Corpus within EBMT</title> <abstract> In order to boost the <entity id="N03-2006.1">translation quality</entity> of <entity id="N03-2006.2">EBMT</entity> based on a small-sized <entity id="N03-2006.3">bilingual corpus</entity> , we use an out-of-domain <entity id="N03-2006.4">bilingual corpus</entity> and, in addition, the <entity id="N03-2006.5">language model</entity> of an in-domain <entity id="N03-2006.6">monolingual corpus</entity> . We conducted experiments with an <entity id="N03-2006.7">EBMT system</entity> . The two <entity id="N03-2006.8">evaluation measures</entity> of the <entity id="N03-2006.9">BLEU score</entity> and the <entity id="N03-2006.10">NIST score</entity> demonstrated the effect of using an out-of-domain <entity id="N03-2006.11">bilingual corpus</entity> and the possibility of using the <entity id="N03-2006.12">language model</entity> . </abstract>


</text>

<text id="N03-2015"> <title>Unsupervised Learning of Morphology for English and Inuktitut</title> <abstract> We describe a simple <entity id="N03-2015.1">unsupervised technique</entity> for learning <entity id="N03-2015.2">morphology</entity> by identifying <entity id="N03-2015.3">hubs</entity> in an <entity id="N03-2015.4">automaton</entity> . For our purposes, a <entity id="N03-2015.5">hub</entity> is a <entity id="N03-2015.6">node</entity> in a <entity id="N03-2015.7">graph</entity> with <entity id="N03-2015.8">in-degree</entity> greater than one and <entity id="N03-2015.9">out-degree</entity> greater than one. We create a <entity id="N03-2015.10">word-trie</entity> , transform it into a <entity id="N03-2015.11">minimal DFA</entity> , then identify <entity id="N03-2015.12">hubs</entity> . Those <entity id="N03-2015.13">hubs</entity> mark the boundary between <entity id="N03-2015.14">root</entity> and <entity id="N03-2015.15">suffix</entity> , achieving similar <entity id="N03-2015.16">performance</entity> to more complex mixtures of techniques. </abstract>


</text>

<text id="N03-2017"> <title>Word Alignment with Cohesion Constraint</title> <abstract> We present a <entity id="N03-2017.1">syntax-based constraint</entity> for <entity id="N03-2017.2">word alignment</entity> , known as the <entity id="N03-2017.3">cohesion constraint</entity> . It requires disjoint <entity id="N03-2017.4">English phrases</entity> to be mapped to non-overlapping intervals in the <entity id="N03-2017.5">French sentence</entity> . We evaluate the utility of this <entity id="N03-2017.6">constraint</entity> in two different algorithms. The results show that it can provide a significant improvement in <entity id="N03-2017.7">alignment quality</entity> . </abstract>


</text>

<text id="N03-2025"> <title>Bootstrapping for Named Entity Tagging Using Concept-based Seeds</title> <abstract> A novel <entity id="N03-2025.1">bootstrapping approach</entity> to <entity id="N03-2025.2">Named Entity (NE) tagging</entity> using <entity id="N03-2025.3">concept-based seeds</entity> and <entity id="N03-2025.4">successive learners</entity> is presented. This approach only requires a few <entity id="N03-2025.5">common noun</entity> or <entity id="N03-2025.6">pronoun</entity> <entity id="N03-2025.7">seeds</entity> that correspond to the <entity id="N03-2025.8">concept</entity> for the targeted <entity id="N03-2025.9">NE</entity> , e.g. he/she/man/woman for <entity id="N03-2025.10">PERSON NE</entity> . The <entity id="N03-2025.11">bootstrapping procedure</entity> is implemented as training two <entity id="N03-2025.12">successive learners</entity> . First, <entity id="N03-2025.13">decision list</entity> is used to learn the <entity id="N03-2025.14">parsing-based NE rules</entity> . Then, a <entity id="N03-2025.15">Hidden Markov Model</entity> is trained on a <entity id="N03-2025.16">corpus</entity> automatically tagged by the first <entity id="N03-2025.17">learner</entity> . The resulting <entity id="N03-2025.18">NE system</entity> approaches <entity id="N03-2025.19">supervised NE</entity> performance for some <entity id="N03-2025.20">NE types</entity> . </abstract>


</text>

<text id="N03-2036"> <title>A Phrase-Based Unigram Model for Statistical Machine Translation</title> <abstract> In this paper, we describe a <entity id="N03-2036.1">phrase-based unigram model</entity> for <entity id="N03-2036.2">statistical machine translation</entity> that uses a much simpler set of <entity id="N03-2036.3">model parameters</entity> than similar <entity id="N03-2036.4">phrase-based models</entity> . The <entity id="N03-2036.5">units of translation</entity> are <entity id="N03-2036.6">blocks</entity> - pairs of <entity id="N03-2036.7">phrases</entity> . During <entity id="N03-2036.8">decoding</entity> , we use a <entity id="N03-2036.9">block unigram model</entity> and a <entity id="N03-2036.10">word-based trigram language model</entity> . During <entity id="N03-2036.11">training</entity> , the <entity id="N03-2036.12">blocks</entity> are learned from <entity id="N03-2036.13">source interval projections</entity> using an underlying <entity id="N03-2036.14">word alignment</entity> . We show experimental results on <entity id="N03-2036.15">block selection criteria</entity> based on <entity id="N03-2036.16">unigram</entity> counts and <entity id="N03-2036.17">phrase</entity> length. </abstract>


</text>

<text id="N03-3010"> <title>Cooperative Model Based Language Understanding in Dialogue</title> <abstract> In this paper, we propose a novel <entity id="N03-3010.1">Cooperative Model</entity> for <entity id="N03-3010.2">natural language understanding</entity> in a <entity id="N03-3010.3">dialogue system</entity> . We build this based on both <entity id="N03-3010.4">Finite State Model (FSM)</entity> and <entity id="N03-3010.5">Statistical Learning Model (SLM)</entity> . <entity id="N03-3010.6">FSM</entity> provides two strategies for <entity id="N03-3010.7">language understanding</entity> and have a high accuracy but little robustness and flexibility. <entity id="N03-3010.8">Statistical approach</entity> is much more robust but less accurate. <entity id="N03-3010.9">Cooperative Model</entity> incorporates all the three strategies together and thus can suppress all the shortcomings of different strategies and has all the advantages of the three strategies. </abstract>


</text>

<text id="N03-4010"> <title>JAVELIN: A Flexible, Planner-Based Architecture for Question Answering</title> <abstract> The <entity id="N03-4010.1">JAVELIN system</entity> integrates a flexible, <entity id="N03-4010.2">planning-based architecture</entity> with a variety of <entity id="N03-4010.3">language processing modules</entity> to provide an <entity id="N03-4010.4">open-domain question answering capability</entity> on <entity id="N03-4010.5">free text</entity> . The demonstration will focus on how <entity id="N03-4010.6">JAVELIN</entity> processes <entity id="N03-4010.7">questions</entity> and retrieves the most likely <entity id="N03-4010.8">answer candidates</entity> from the given <entity id="N03-4010.9">text corpus</entity> . The operation of the system will be explained in depth through browsing the <entity id="N03-4010.10">repository</entity> of <entity id="N03-4010.11">data objects</entity> created by the system during each <entity id="N03-4010.12">question answering session</entity> . </abstract>


</text>

<text id="P03-1002"> <title>Using Predicate-Argument Structures for Information Extraction</title> <abstract> In this paper we present a novel, customizable : <entity id="P03-1002.1">IE paradigm</entity> that takes advantage of <entity id="P03-1002.2">predicate-argument structures</entity> . We also introduce a new way of automatically identifying <entity id="P03-1002.3">predicate argument structures</entity> , which is central to our <entity id="P03-1002.4">IE paradigm</entity> . It is based on: (1) an extended set of <entity id="P03-1002.5">features</entity> ; and (2) <entity id="P03-1002.6">inductive decision tree learning</entity> . The experimental results prove our claim that accurate <entity id="P03-1002.7">predicate-argument structures</entity> enable high quality <entity id="P03-1002.8">IE</entity> results. </abstract>


</text>

<text id="P03-1005"> <title>Hierarchical Directed Acyclic Graph Kernel: Methods for Structured Natural Language Data</title> <abstract> This paper proposes the <entity id="P03-1005.1">Hierarchical Directed Acyclic Graph (HDAG) Kernel</entity> for <entity id="P03-1005.2">structured natural language data</entity> . The <entity id="P03-1005.3">HDAG Kernel</entity> directly accepts several levels of both <entity id="P03-1005.4">chunks</entity> and their <entity id="P03-1005.5">relations</entity> , and then efficiently computes the <entity id="P03-1005.6">weighed sum</entity> of the number of common <entity id="P03-1005.7">attribute sequences</entity> of the <entity id="P03-1005.8">HDAGs</entity> . We applied the proposed method to <entity id="P03-1005.9">question classification</entity> and <entity id="P03-1005.10">sentence alignment tasks</entity> to evaluate its performance as a <entity id="P03-1005.11">similarity measure</entity> and a <entity id="P03-1005.12">kernel function</entity> . The results of the experiments demonstrate that the <entity id="P03-1005.13">HDAG Kernel</entity> is superior to other <entity id="P03-1005.14">kernel functions</entity> and <entity id="P03-1005.15">baseline methods</entity> . </abstract>


</text>

<text id="P03-1009"> <title>Clustering Polysemic Subcategorization Frame Distributions Semantically</title> <abstract> Previous research has demonstrated the utility of <entity id="P03-1009.1">clustering</entity> in inducing <entity id="P03-1009.2">semantic verb classes</entity> from undisambiguated <entity id="P03-1009.3">corpus data</entity> . We describe a new approach which involves clustering <entity id="P03-1009.4">subcategorization frame (SCF)</entity> distributions using the <entity id="P03-1009.5">Information Bottleneck</entity> and <entity id="P03-1009.6">nearest neighbour</entity> methods. In contrast to previous work, we particularly focus on clustering <entity id="P03-1009.7">polysemic verbs</entity> . A novel <entity id="P03-1009.8">evaluation scheme</entity> is proposed which accounts for the effect of <entity id="P03-1009.9">polysemy</entity> on the <entity id="P03-1009.10">clusters</entity> , offering us a good insight into the potential and limitations of <entity id="P03-1009.11">semantically classifying</entity> <entity id="P03-1009.12">undisambiguated SCF data</entity> . </abstract>


</text>

<text id="P03-1022"> <title>A Machine Learning Approach to Pronoun Resolution in Spoken Dialogue</title> <abstract> We apply a <entity id="P03-1022.1">decision tree based approach</entity> to <entity id="P03-1022.2">pronoun resolution</entity> in <entity id="P03-1022.3">spoken dialogue</entity> . Our system deals with <entity id="P03-1022.4">pronouns</entity> with <entity id="P03-1022.5">NP- and non-NP-antecedents</entity> . We present a set of <entity id="P03-1022.6">features</entity> designed for <entity id="P03-1022.7">pronoun resolution</entity> in <entity id="P03-1022.8">spoken dialogue</entity> and determine the most promising <entity id="P03-1022.9">features</entity> . We evaluate the system on twenty <entity id="P03-1022.10">Switchboard dialogues</entity> and show that it compares well to <entity id="P03-1022.11">Byron&apos;s (2002) manually tuned system</entity> . </abstract>


</text>

<text id="P03-1030"> <title>Optimizing Story Link Detection is not Equivalent to Optimizing New Event Detection</title> <abstract> <entity id="P03-1030.1">Link detection</entity> has been regarded as a core technology for the <entity id="P03-1030.2">Topic Detection and Tracking tasks</entity> of <entity id="P03-1030.3">new event detection</entity> . In this paper we formulate <entity id="P03-1030.4">story link detection</entity> and <entity id="P03-1030.5">new event detection</entity> as <entity id="P03-1030.6">information retrieval task</entity> and hypothesize on the impact of <entity id="P03-1030.7">precision</entity> and <entity id="P03-1030.8">recall</entity> on both systems. Motivated by these arguments, we introduce a number of new performance enhancing techniques including <entity id="P03-1030.9">part of speech tagging</entity> , new <entity id="P03-1030.10">similarity measures</entity> and expanded <entity id="P03-1030.11">stop lists</entity> . Experimental results validate our hypothesis. </abstract>


</text>

<text id="P03-1031"> <title>Corpus-based Discourse Understanding in Spoken Dialogue Systems</title> <abstract> This paper concerns the <entity id="P03-1031.1">discourse understanding process</entity> in <entity id="P03-1031.2">spoken dialogue systems</entity> . This process enables the system to understand <entity id="P03-1031.3">user utterances</entity> based on the <entity id="P03-1031.4">context</entity> of a <entity id="P03-1031.5">dialogue</entity> . Since multiple <entity id="P03-1031.6">candidates</entity> for the <entity id="P03-1031.7">understanding</entity> result can be obtained for a <entity id="P03-1031.8">user utterance</entity> due to the <entity id="P03-1031.9">ambiguity</entity> of <entity id="P03-1031.10">speech understanding</entity> , it is not appropriate to decide on a single <entity id="P03-1031.11">understanding</entity> result after each <entity id="P03-1031.12">user utterance</entity> . By holding multiple <entity id="P03-1031.13">candidates</entity> for <entity id="P03-1031.14">understanding</entity> results and resolving the <entity id="P03-1031.15">ambiguity</entity> as the <entity id="P03-1031.16">dialogue</entity> progresses, the <entity id="P03-1031.17">discourse understanding accuracy</entity> can be improved. This paper proposes a method for resolving this <entity id="P03-1031.18">ambiguity</entity> based on <entity id="P03-1031.19">statistical information</entity> obtained from <entity id="P03-1031.20">dialogue corpora</entity> . Unlike conventional methods that use <entity id="P03-1031.21">hand-crafted rules</entity> , the proposed method enables easy design of the <entity id="P03-1031.22">discourse understanding process</entity> . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple <entity id="P03-1031.23">candidates</entity> for <entity id="P03-1031.24">understanding</entity> results is effective. </abstract>


</text>

<text id="P03-1033"> <title>Flexible Guidance Generation using User Model in Spoken Dialogue Systems</title> <abstract> We address appropriate <entity id="P03-1033.1">user modeling</entity> in order to generate <entity id="P03-1033.2">cooperative responses</entity> to each <entity id="P03-1033.3">user</entity> in <entity id="P03-1033.4">spoken dialogue systems</entity> . Unlike previous studies that focus on <entity id="P03-1033.5">user</entity> &apos;s <entity id="P03-1033.6">knowledge</entity> or typical kinds of <entity id="P03-1033.7">users</entity> , the <entity id="P03-1033.8">user model</entity> we propose is more comprehensive. Specifically, we set up three dimensions of <entity id="P03-1033.9">user models</entity> : <entity id="P03-1033.10">skill level</entity> to the system, <entity id="P03-1033.11">knowledge level</entity> on the <entity id="P03-1033.12">target domain</entity> and the degree of <entity id="P03-1033.13">hastiness</entity> . Moreover, the <entity id="P03-1033.14">models</entity> are automatically derived by <entity id="P03-1033.15">decision tree learning</entity> using real <entity id="P03-1033.16">dialogue data</entity> collected by the system. We obtained reasonable <entity id="P03-1033.17">classification accuracy</entity> for all dimensions. <entity id="P03-1033.18">Dialogue strategies</entity> based on the <entity id="P03-1033.19">user modeling</entity> are implemented in <entity id="P03-1033.20">Kyoto city bus information system</entity> that has been developed at our laboratory. Experimental evaluation shows that the <entity id="P03-1033.21">cooperative responses</entity> adaptive to <entity id="P03-1033.22">individual users</entity> serve as good guidance for <entity id="P03-1033.23">novice users</entity> without increasing the <entity id="P03-1033.24">dialogue duration</entity> for <entity id="P03-1033.25">skilled users</entity> . </abstract>


</text>

<text id="P03-1050"> <title>Unsupervised Learning of Arabic Stemming using a Parallel Corpus</title> <abstract> This paper presents an <entity id="P03-1050.1">unsupervised learning approach</entity> to building a <entity id="P03-1050.2">non-English (Arabic) stemmer</entity> . The <entity id="P03-1050.3">stemming model</entity> is based on <entity id="P03-1050.4">statistical machine translation</entity> and it uses an <entity id="P03-1050.5">English stemmer</entity> and a small (10K sentences) <entity id="P03-1050.6">parallel corpus</entity> as its sole <entity id="P03-1050.7">training resources</entity> . No <entity id="P03-1050.8">parallel text</entity> is needed after the <entity id="P03-1050.9">training phase</entity> . <entity id="P03-1050.10">Monolingual, unannotated text</entity> can be used to further improve the <entity id="P03-1050.11">stemmer</entity> by allowing it to adapt to a desired <entity id="P03-1050.12">domain</entity> or <entity id="P03-1050.13">genre</entity> . Examples and results will be given for <entity id="P03-1050.14">Arabic</entity> , but the approach is applicable to any <entity id="P03-1050.15">language</entity> that needs <entity id="P03-1050.16">affix removal</entity> . Our <entity id="P03-1050.17">resource-frugal approach</entity> results in 87.5% <entity id="P03-1050.18">agreement</entity> with a state of the art, proprietary <entity id="P03-1050.19">Arabic stemmer</entity> built using <entity id="P03-1050.20">rules</entity> , <entity id="P03-1050.21">affix lists</entity> , and <entity id="P03-1050.22">human annotated text</entity> , in addition to an <entity id="P03-1050.23">unsupervised component</entity> . <entity id="P03-1050.24">Task-based evaluation</entity> using <entity id="P03-1050.25">Arabic information retrieval</entity> indicates an improvement of 22-38% in <entity id="P03-1050.26">average precision</entity> over <entity id="P03-1050.27">unstemmed text</entity> , and 96% of the performance of the proprietary <entity id="P03-1050.28">stemmer</entity> above. </abstract>


</text>

<text id="P03-1051"> <title>Language Model Based Arabic Word Segmentation</title> <abstract> We approximate <entity id="P03-1051.1">Arabic&apos;s rich morphology</entity> by a <entity id="P03-1051.2">model</entity> that a <entity id="P03-1051.3">word</entity> consists of a sequence of <entity id="P03-1051.4">morphemes</entity> in the <entity id="P03-1051.5">pattern</entity> <entity id="P03-1051.6">prefix*-stem-suffix*</entity> (* denotes zero or more occurrences of a <entity id="P03-1051.7">morpheme</entity> ). Our method is seeded by a small <entity id="P03-1051.8">manually segmented Arabic corpus</entity> and uses it to bootstrap an <entity id="P03-1051.9">unsupervised algorithm</entity> to build the <entity id="P03-1051.10">Arabic word segmenter</entity> from a large <entity id="P03-1051.11">unsegmented Arabic corpus</entity> . The algorithm uses a <entity id="P03-1051.12">trigram language model</entity> to determine the most probable <entity id="P03-1051.13">morpheme sequence</entity> for a given <entity id="P03-1051.14">input</entity> . The <entity id="P03-1051.15">language model</entity> is initially estimated from a small <entity id="P03-1051.16">manually segmented corpus</entity> of about 110,000 <entity id="P03-1051.17">words</entity> . To improve the <entity id="P03-1051.18">segmentation</entity> <entity id="P03-1051.19">accuracy</entity> , we use an <entity id="P03-1051.20">unsupervised algorithm</entity> for automatically acquiring new <entity id="P03-1051.21">stems</entity> from a 155 million <entity id="P03-1051.22">word</entity> <entity id="P03-1051.23">unsegmented corpus</entity> , and re-estimate the <entity id="P03-1051.24">model parameters</entity> with the expanded <entity id="P03-1051.25">vocabulary</entity> and <entity id="P03-1051.26">training corpus</entity> . The resulting <entity id="P03-1051.27">Arabic word segmentation system</entity> achieves around 97% <entity id="P03-1051.28">exact match accuracy</entity> on a <entity id="P03-1051.29">test corpus</entity> containing 28,449 <entity id="P03-1051.30">word tokens</entity> . We believe this is a state-of-the-art performance and the algorithm can be used for many <entity id="P03-1051.31">highly inflected languages</entity> provided that one can create a small <entity id="P03-1051.32">manually segmented corpus</entity> of the <entity id="P03-1051.33">language</entity> of interest. </abstract>


</text>

<text id="P03-1058"> <title>Exploiting Parallel Texts for Word Sense Disambiguation: An Empirical Study</title> <abstract> 
A central problem of <entity id="P03-1058.1">word sense disambiguation (WSD)</entity> is the lack of <entity id="P03-1058.2">manually sense-tagged data</entity> required for <entity id="P03-1058.3">supervised learning</entity> . In this paper, we evaluate an approach to automatically acquire <entity id="P03-1058.4">sense-tagged training data</entity> from <entity id="P03-1058.5">English-Chinese parallel corpora</entity> , which are then used for disambiguating the <entity id="P03-1058.6">nouns</entity> in the <entity id="P03-1058.7">SENSEVAL-2 English lexical sample task</entity> . Our investigation reveals that this <entity id="P03-1058.8">method of acquiring sense-tagged data</entity> is promising. On a subset of the most difficult <entity id="P03-1058.9">SENSEVAL-2 nouns</entity> , the <entity id="P03-1058.10">accuracy</entity> difference between the two approaches is only 14.0%, and the difference could narrow further to 6.5% if we disregard the advantage that <entity id="P03-1058.11">manually sense-tagged data</entity> have in their <entity id="P03-1058.12">sense coverage</entity> . Our analysis also highlights the importance of the issue of <entity id="P03-1058.13">domain dependence</entity> in evaluating <entity id="P03-1058.14">WSD programs</entity> . </abstract>


</text>

<text id="P03-1068"> <title>Towards a Resource for Lexical Semantics: A Large German Corpus with Extensive Semantic Annotation</title> <abstract> We describe the ongoing construction of a large, <entity id="P03-1068.1">semantically annotated corpus</entity> resource as reliable basis for the large-scale <entity id="P03-1068.2">acquisition of word-semantic information</entity> , e.g. the construction of <entity id="P03-1068.3">domain-independent lexica</entity> . The backbone of the <entity id="P03-1068.4">annotation</entity> are <entity id="P03-1068.5">semantic roles</entity> in the <entity id="P03-1068.6">frame semantics paradigm</entity> . We report experiences and evaluate the <entity id="P03-1068.7">annotated data</entity> from the first project stage. On this basis, we discuss the problems of <entity id="P03-1068.8">vagueness</entity> and <entity id="P03-1068.9">ambiguity</entity> in <entity id="P03-1068.10">semantic annotation</entity> . </abstract>


</text>

<text id="P03-1070"> <title>Towards a Model of Face-to-Face Grounding</title> <abstract> We investigate the <entity id="P03-1070.1">verbal and nonverbal means</entity> for <entity id="P03-1070.2">grounding</entity> , and propose a design for <entity id="P03-1070.3">embodied conversational agents</entity> that relies on both kinds of <entity id="P03-1070.4">signals</entity> to establish <entity id="P03-1070.5">common ground</entity> in <entity id="P03-1070.6">human-computer interaction</entity> . We analyzed <entity id="P03-1070.7">eye gaze</entity> , <entity id="P03-1070.8">head nods</entity> and <entity id="P03-1070.9">attentional focus</entity> in the context of a <entity id="P03-1070.10">direction-giving task</entity> . The distribution of <entity id="P03-1070.11">nonverbal behaviors</entity> differed depending on the type of <entity id="P03-1070.12">dialogue move</entity> being grounded, and the overall pattern reflected a monitoring of lack of <entity id="P03-1070.13">negative feedback</entity> . Based on these results, we present an <entity id="P03-1070.14">ECA</entity> that uses <entity id="P03-1070.15">verbal and nonverbal grounding acts</entity> to update <entity id="P03-1070.16">dialogue state</entity> . </abstract>


</text>

<text id="P03-2036"> <title>Comparison between CFG filtering techniques for LTAG and HPSG</title> <abstract> An empirical comparison of <entity id="P03-2036.1">CFG filtering techniques</entity> for <entity id="P03-2036.2">LTAG</entity> and <entity id="P03-2036.3">HPSG</entity> is presented. We demonstrate that an approximation of <entity id="P03-2036.4">HPSG</entity> produces a more effective <entity id="P03-2036.5">CFG filter</entity> than that of <entity id="P03-2036.6">LTAG</entity> . We also investigate the reason for that difference. </abstract>


</text>

<text id="C04-1106"> <title>Lower and higher estimates of the number of &amp;quot;true analogies&amp;quot; between sentences contained in a large multilingual corpus</title> <abstract> The reality of <entity id="C04-1106.1">analogies between words</entity> is refuted by noone (e.g., I walked is to to walk as I laughed is to to laugh, noted I walked : to walk :: I laughed : to laugh). But <entity id="C04-1106.2">computational linguists</entity> seem to be quite dubious about <entity id="C04-1106.3">analogies between sentences</entity> : they would not be enough numerous to be of any use. We report experiments conducted on a <entity id="C04-1106.4">multilingual corpus</entity> to estimate the number of <entity id="C04-1106.5">analogies</entity> among the <entity id="C04-1106.6">sentences</entity> that it contains. We give two estimates, a lower one and a higher one. As an <entity id="C04-1106.7">analogy</entity> must be valid on the level of <entity id="C04-1106.8">form</entity> as well as on the level of <entity id="C04-1106.9">meaning</entity> , we relied on the idea that <entity id="C04-1106.10">translation</entity> should preserve <entity id="C04-1106.11">meaning</entity> to test for similar <entity id="C04-1106.12">meanings</entity> . </abstract>


</text>

<text id="N04-1024"> <title>Evaluating Multiple Aspects of Coherence in Student Essays</title> <abstract> <entity id="N04-1024.1">CriterionSM Online Essay Evaluation Service</entity> includes a capability that labels <entity id="N04-1024.2">sentences</entity> in student <entity id="N04-1024.3">writing</entity> with <entity id="N04-1024.4">essay-based discourse elements</entity> (e.g., <entity id="N04-1024.5">thesis statements</entity> ). We describe a new system that enhances <entity id="N04-1024.6">Criterion</entity> &apos;s capability, by evaluating multiple aspects of <entity id="N04-1024.7">coherence</entity> in <entity id="N04-1024.8">essays</entity> . This system identifies <entity id="N04-1024.9">features</entity> of <entity id="N04-1024.10">sentences</entity> based on <entity id="N04-1024.11">semantic similarity measures</entity> and <entity id="N04-1024.12">discourse structure</entity> . A <entity id="N04-1024.13">support vector machine</entity> uses these <entity id="N04-1024.14">features</entity> to capture <entity id="N04-1024.15">breakdowns in coherence</entity> due to relatedness to the <entity id="N04-1024.16">essay question</entity> and relatedness between <entity id="N04-1024.17">discourse elements</entity> . <entity id="N04-1024.18">Intra-sentential quality</entity> is evaluated with <entity id="N04-1024.19">rule-based heuristics</entity> . Results indicate that the system yields higher performance than a <entity id="N04-1024.20">baseline</entity> on all three aspects. </abstract> 


</text>

<text id="H05-1005"> <title>Improving Multilingual Summarization: Using Redundancy in the Input to Correct MT errors</title> <abstract> In this paper, we use the <entity id="H05-1005.1">information redundancy</entity> in <entity id="H05-1005.2">multilingual input</entity> to correct errors in <entity id="H05-1005.3">machine translation</entity> and thus improve the quality of <entity id="H05-1005.4">multilingual summaries</entity> . We consider the case of <entity id="H05-1005.5">multi-document summarization</entity> , where the input <entity id="H05-1005.6">documents</entity> are in <entity id="H05-1005.7">Arabic</entity> , and the output <entity id="H05-1005.8">summary</entity> is in <entity id="H05-1005.9">English</entity> . Typically, information that makes it to a <entity id="H05-1005.10">summary</entity> appears in many different <entity id="H05-1005.11">lexical-syntactic forms</entity> in the input <entity id="H05-1005.12">documents</entity> . Further, the use of multiple <entity id="H05-1005.13">machine translation systems</entity> provides yet more <entity id="H05-1005.14">redundancy</entity> , yielding different ways to realize that <entity id="H05-1005.15">information</entity> in <entity id="H05-1005.16">English</entity> . We demonstrate how errors in the <entity id="H05-1005.17">machine translations</entity> of the input <entity id="H05-1005.18">Arabic documents</entity> can be corrected by identifying and generating from such <entity id="H05-1005.19">redundancy</entity> , focusing on <entity id="H05-1005.20">noun phrases</entity> . </abstract>


</text>

<text id="H05-1012"> <title>A Maximum Entropy Word Aligner for Arabic-English Machine Translation</title> <abstract> This paper presents a <entity id="H05-1012.1">maximum entropy word alignment algorithm</entity> for <entity id="H05-1012.2">Arabic-English</entity> based on <entity id="H05-1012.3">supervised training data</entity> . We demonstrate that it is feasible to create <entity id="H05-1012.4">training material</entity> for problems in <entity id="H05-1012.5">machine translation</entity> and that a mixture of <entity id="H05-1012.6">supervised and unsupervised methods</entity> yields superior <entity id="H05-1012.7">performance</entity> . The <entity id="H05-1012.8">probabilistic model</entity> used in the <entity id="H05-1012.9">alignment</entity> directly models the <entity id="H05-1012.10">link decisions</entity> . Significant improvement over traditional <entity id="H05-1012.11">word alignment techniques</entity> is shown as well as improvement on several <entity id="H05-1012.12">machine translation tests</entity> . Performance of the algorithm is contrasted with <entity id="H05-1012.13">human annotation performance</entity> . </abstract>


</text>

<text id="H05-1095"> <title>Translating with non-contiguous phrases</title> <abstract> This paper presents a <entity id="H05-1095.1">phrase-based statistical machine translation method</entity> , based on <entity id="H05-1095.2">non-contiguous phrases</entity> , i.e. <entity id="H05-1095.3">phrases</entity> with gaps. A method for producing such <entity id="H05-1095.4">phrases</entity> from a <entity id="H05-1095.5">word-aligned corpora</entity> is proposed. A <entity id="H05-1095.6">statistical translation model</entity> is also presented that deals such <entity id="H05-1095.7">phrases</entity> , as well as a <entity id="H05-1095.8">training method</entity> based on the maximization of <entity id="H05-1095.9">translation accuracy</entity> , as measured with the <entity id="H05-1095.10">NIST evaluation metric</entity> . <entity id="H05-1095.11">Translations</entity> are produced by means of a <entity id="H05-1095.12">beam-search decoder</entity> . Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the <entity id="H05-1095.13">training data</entity> . </abstract>


</text>

<text id="H05-1117"> <title>Automatically Evaluating Answers to Definition Questions</title> <abstract> Following recent developments in the <entity id="H05-1117.1">automatic evaluation</entity> of <entity id="H05-1117.2">machine translation</entity> and <entity id="H05-1117.3">document summarization</entity> , we present a similar approach, implemented in a measure called <entity id="H05-1117.4">POURPRE</entity> , for <entity id="H05-1117.5">automatically evaluating answers to definition questions</entity> . Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system&apos;s response. The lack of automatic methods for <entity id="H05-1117.6">scoring system output</entity> is an impediment to progress in the field, which we address with this work. Experiments with the <entity id="H05-1117.7">TREC 2003 and TREC 2004 QA tracks</entity> indicate that <entity id="H05-1117.8">rankings</entity> produced by our metric correlate highly with <entity id="H05-1117.9">official rankings</entity> , and that <entity id="H05-1117.10">POURPRE</entity> outperforms direct application of existing metrics. </abstract>


</text>

<text id="H05-2007"> <title>Pattern Visualization for Machine Translation Output</title> <abstract> We describe a method for identifying systematic <entity id="H05-2007.1">patterns</entity> in <entity id="H05-2007.2">translation data</entity> using <entity id="H05-2007.3">part-of-speech tag sequences</entity> . We incorporate this analysis into a <entity id="H05-2007.4">diagnostic tool</entity> intended for <entity id="H05-2007.5">developers</entity> of <entity id="H05-2007.6">machine translation systems</entity> , and demonstrate how our application can be used by <entity id="H05-2007.7">developers</entity> to explore <entity id="H05-2007.8">patterns</entity> in <entity id="H05-2007.9">machine translation output</entity> . </abstract>


</text>

<text id="I05-2021"> <title>Evaluating the Word Sense Disambiguation Performance of Statistical Machine Translation</title> <abstract> We present the first known <entity id="I05-2021.1">empirical test</entity> of an increasingly common speculative claim, by evaluating a representative <entity id="I05-2021.2">Chinese-to-English SMT model</entity> directly on <entity id="I05-2021.3">word sense disambiguation performance</entity> , using standard <entity id="I05-2021.4">WSD evaluation methodology</entity> and <entity id="I05-2021.5">datasets</entity> from the <entity id="I05-2021.6">Senseval-3 Chinese lexical sample task</entity> . Much effort has been put in designing and evaluating dedicated <entity id="I05-2021.7">word sense disambiguation (WSD) models</entity> , in particular with the <entity id="I05-2021.8">Senseval</entity> series of workshops. At the same time, the recent improvements in the <entity id="I05-2021.9">BLEU scores</entity> of <entity id="I05-2021.10">statistical machine translation (SMT)</entity> suggests that <entity id="I05-2021.11">SMT models</entity> are good at predicting the right <entity id="I05-2021.12">translation</entity> of the <entity id="I05-2021.13">words</entity> in <entity id="I05-2021.14">source language sentences</entity> . Surprisingly however, the <entity id="I05-2021.15">WSD</entity> <entity id="I05-2021.16">accuracy</entity> of <entity id="I05-2021.17">SMT models</entity> has never been evaluated and compared with that of the dedicated <entity id="I05-2021.18">WSD models</entity> . We present controlled experiments showing the <entity id="I05-2021.19">WSD</entity> <entity id="I05-2021.20">accuracy</entity> of current typical <entity id="I05-2021.21">SMT models</entity> to be significantly lower than that of all the dedicated <entity id="I05-2021.22">WSD models</entity> considered. This tends to support the view that despite recent speculative claims to the contrary, current <entity id="I05-2021.23">SMT models</entity> do have limitations in comparison with dedicated <entity id="I05-2021.24">WSD models</entity> , and that <entity id="I05-2021.25">SMT</entity> should benefit from the better predictions made by the <entity id="I05-2021.26">WSD models</entity> . </abstract>


</text>

<text id="I05-2048"> <title>Statistical Machine Translation Part I: Hands-On Introduction</title> <abstract> <entity id="I05-2048.1">Statistical machine translation (SMT)</entity> is currently one of the hot spots in <entity id="I05-2048.2">natural language processing</entity> . Over the last few years dramatic improvements have been made, and a number of comparative evaluations have shown, that <entity id="I05-2048.3">SMT</entity> gives competitive results to <entity id="I05-2048.4">rule-based translation systems</entity> , requiring significantly less development time. This is particularly important when building <entity id="I05-2048.5">translation systems</entity> for new <entity id="I05-2048.6">language pairs</entity> or new <entity id="I05-2048.7">domains</entity> . This workshop is intended to give an introduction to <entity id="I05-2048.8">statistical machine translation</entity> with a focus on practical considerations. Participants should be able, after attending this workshop, to set out building an <entity id="I05-2048.9">SMT system</entity> themselves and achieving good <entity id="I05-2048.10">baseline results</entity> in a short time. The tutorial will cover the basics of <entity id="I05-2048.11">SMT</entity> : Theory will be put into practice. <entity id="I05-2048.12">STTK</entity> , a <entity id="I05-2048.13">statistical machine translation tool kit</entity> , will be introduced and used to build a working <entity id="I05-2048.14">translation system</entity> . <entity id="I05-2048.15">STTK</entity> has been developed by the presenter and co-workers over a number of years and is currently used as the basis of <entity id="I05-2048.16">CMU&apos;s SMT system</entity> . It has also successfully been coupled with <entity id="I05-2048.17">rule-based and example based machine translation modules</entity> to build a <entity id="I05-2048.18">multi engine machine translation system</entity> . The <entity id="I05-2048.19">source code</entity> of the <entity id="I05-2048.20">tool kit</entity> will be made available. </abstract>


</text>

<text id="I05-4010"> <title>Harvesting the Bitexts of the Laws of Hong Kong From the Web</title> <abstract> In this paper we present our recent work on harvesting <entity id="I05-4010.1">English-Chinese bitexts</entity> of the laws of Hong Kong from the <entity id="I05-4010.2">Web</entity> and aligning them to the <entity id="I05-4010.3">subparagraph</entity> level via utilizing the <entity id="I05-4010.4">numbering system</entity> in the <entity id="I05-4010.5">legal text hierarchy</entity> . Basic methodology and practical techniques are reported in detail. The resultant <entity id="I05-4010.6">bilingual corpus</entity> , 10.4M <entity id="I05-4010.7">English words</entity> and 18.3M <entity id="I05-4010.8">Chinese characters</entity> , is an authoritative and comprehensive <entity id="I05-4010.9">text collection</entity> covering the specific and special domain of HK laws. It is particularly valuable to <entity id="I05-4010.10">empirical MT research</entity> . This piece of work has also laid a foundation for exploring and harvesting <entity id="I05-4010.11">English-Chinese bitexts</entity> in a larger volume from the <entity id="I05-4010.12">Web</entity> . </abstract>


</text>

<text id="I05-5003"> <title>Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence</title> <abstract> The task of <entity id="I05-5003.1">machine translation (MT) evaluation</entity> is closely related to the task of <entity id="I05-5003.2">sentence-level semantic equivalence classification</entity> . This paper investigates the utility of applying standard <entity id="I05-5003.3">MT evaluation methods (BLEU, NIST, WER and PER)</entity> to building <entity id="I05-5003.4">classifiers</entity> to predict <entity id="I05-5003.5">semantic equivalence</entity> and <entity id="I05-5003.6">entailment</entity> . We also introduce a novel <entity id="I05-5003.7">classification method</entity> based on <entity id="I05-5003.8">PER</entity> which leverages <entity id="I05-5003.9">part of speech information</entity> of the <entity id="I05-5003.10">words</entity> contributing to the <entity id="I05-5003.11">word matches and non-matches</entity> in the <entity id="I05-5003.12">sentence</entity> . Our results show that <entity id="I05-5003.13">MT evaluation techniques</entity> are able to produce useful <entity id="I05-5003.14">features</entity> for <entity id="I05-5003.15">paraphrase classification</entity> and to a lesser extent <entity id="I05-5003.16">entailment</entity> . Our <entity id="I05-5003.17">technique</entity> gives a substantial improvement in <entity id="I05-5003.18">paraphrase classification accuracy</entity> over all of the other <entity id="I05-5003.19">models</entity> used in the experiments. </abstract>


</text>

<text id="I05-5008"> <title>Automatic generation of paraphrases to be used as translation references in objective evaluation measures of machine translation</title> <abstract> We propose a method that automatically generates <entity id="I05-5008.1">paraphrase</entity> sets from <entity id="I05-5008.2">seed sentences</entity> to be used as <entity id="I05-5008.3">reference sets</entity> in objective <entity id="I05-5008.4">machine translation evaluation measures</entity> like <entity id="I05-5008.5">BLEU</entity> and <entity id="I05-5008.6">NIST</entity> . We measured the quality of the <entity id="I05-5008.7">paraphrases</entity> produced in an experiment, i.e., (i) their <entity id="I05-5008.8">grammaticality</entity> : at least 99% correct <entity id="I05-5008.9">sentences</entity> ; (ii) their <entity id="I05-5008.10">equivalence in meaning</entity> : at least 96% correct <entity id="I05-5008.11">paraphrases</entity> either by <entity id="I05-5008.12">meaning equivalence</entity> or <entity id="I05-5008.13">entailment</entity> ; and, (iii) the amount of internal <entity id="I05-5008.14">lexical and syntactical variation</entity> in a set of <entity id="I05-5008.15">paraphrases</entity> : slightly superior to that of <entity id="I05-5008.16">hand-produced sets</entity> . The <entity id="I05-5008.17">paraphrase</entity> sets produced by this method thus seem adequate as <entity id="I05-5008.18">reference sets</entity> to be used for <entity id="I05-5008.19">MT evaluation</entity> . </abstract>


</text>

<text id="I05-6011"> <title>Annotating Honorifics Denoting Social Ranking of Referents</title> <abstract> This paper proposes an <entity id="I05-6011.1">annotating scheme</entity> that encodes <entity id="I05-6011.2">honorifics</entity> (respectful words). <entity id="I05-6011.3">Honorifics</entity> are used extensively in <entity id="I05-6011.4">Japanese</entity> , reflecting the social relationship (e.g. social ranks and age) of the <entity id="I05-6011.5">referents</entity> . This <entity id="I05-6011.6">referential information</entity> is vital for resolving <entity id="I05-6011.7">zero pronouns</entity> and improving <entity id="I05-6011.8">machine translation outputs</entity> . Annotating <entity id="I05-6011.9">honorifics</entity> is a complex task that involves identifying a <entity id="I05-6011.10">predicate</entity> with <entity id="I05-6011.11">honorifics</entity> , assigning <entity id="I05-6011.12">ranks</entity> to <entity id="I05-6011.13">referents</entity> of the <entity id="I05-6011.14">predicate</entity> , calibrating the <entity id="I05-6011.15">ranks</entity> , and connecting <entity id="I05-6011.16">referents</entity> with their <entity id="I05-6011.17">predicates</entity> . </abstract>


</text>

<text id="J05-1003"> <title>Discriminative Reranking for Natural Language Parsing</title> <abstract> This article considers approaches which rerank the output of an existing <entity id="J05-1003.1">probabilistic parser</entity> . The base <entity id="J05-1003.2">parser</entity> produces a set of <entity id="J05-1003.3">candidate parses</entity> for each input <entity id="J05-1003.4">sentence</entity> , with associated <entity id="J05-1003.5">probabilities</entity> that define an initial <entity id="J05-1003.6">ranking</entity> of these <entity id="J05-1003.7">parses</entity> . A second <entity id="J05-1003.8">model</entity> then attempts to improve upon this initial <entity id="J05-1003.9">ranking</entity> , using additional <entity id="J05-1003.10">features</entity> of the <entity id="J05-1003.11">tree</entity> as evidence. The strength of our approach is that it allows a <entity id="J05-1003.12">tree</entity> to be represented as an arbitrary set of <entity id="J05-1003.13">features</entity> , without concerns about how these <entity id="J05-1003.14">features</entity> interact or overlap and without the need to define a <entity id="J05-1003.15">derivation</entity> or a <entity id="J05-1003.16">generative model</entity> which takes these <entity id="J05-1003.17">features</entity> into account . We introduce a new method for the <entity id="J05-1003.18">reranking task</entity> , based on the <entity id="J05-1003.19">boosting approach</entity> to <entity id="J05-1003.20">ranking problems</entity> described in Freund et al. (1998). We apply the <entity id="J05-1003.21">boosting method</entity> to <entity id="J05-1003.22">parsing</entity> the <entity id="J05-1003.23">Wall Street Journal treebank</entity> . The method combined the <entity id="J05-1003.24">log-likelihood</entity> under a <entity id="J05-1003.25">baseline model</entity> (that of Collins [1999]) with evidence from an additional 500,000 <entity id="J05-1003.26">features</entity> over <entity id="J05-1003.27">parse trees</entity> that were not included in the original <entity id="J05-1003.28">model</entity> . The new <entity id="J05-1003.29">model</entity> achieved 89.75% <entity id="J05-1003.30">F-measure</entity> , a 13% relative decrease in <entity id="J05-1003.31">F-measure</entity> error over the <entity id="J05-1003.32">baseline model's score</entity> of 88.2%. The article also introduces a new algorithm for the <entity id="J05-1003.33">boosting approach</entity> which takes advantage of the <entity id="J05-1003.34">sparsity of the feature space</entity> in the <entity id="J05-1003.35">parsing data</entity> . Experiments show significant efficiency gains for the new algorithm over the obvious <entity id="J05-1003.36">implementation</entity> of the <entity id="J05-1003.37">boosting approach</entity> . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on <entity id="J05-1003.38">feature selection methods</entity> within <entity id="J05-1003.39">log-linear (maximum-entropy) models</entity> . Although the experiments in this article are on <entity id="J05-1003.40">natural language parsing (NLP)</entity> , the approach should be applicable to many other <entity id="J05-1003.41">NLP problems</entity> which are naturally framed as <entity id="J05-1003.42">ranking tasks</entity> , for example, <entity id="J05-1003.43">speech recognition</entity> , <entity id="J05-1003.44">machine translation</entity> , or <entity id="J05-1003.45">natural language generation</entity> . </abstract>


</text>

<text id="J05-4003"> <title>Improving Machine Translation Performance by Exploiting Non-Parallel Corpora</title> <abstract> <SectionTitle></SectionTitle> We present a novel method for <entity id="J05-4003.1">discovering parallel sentences</entity> in <entity id="J05-4003.2">comparable, non-parallel corpora</entity> . We train a <entity id="J05-4003.3">maximum entropy classifier</entity> that, given a pair of <entity id="J05-4003.4">sentences</entity> , can reliably determine whether or not they are <entity id="J05-4003.5">translations</entity> of each other. Using this approach, we extract <entity id="J05-4003.6">parallel data</entity> from large <entity id="J05-4003.7">Chinese, Arabic, and English non-parallel newspaper corpora</entity> . We evaluate the <entity id="J05-4003.8">quality of the extracted data</entity> by showing that it improves the performance of a state-of-the-art <entity id="J05-4003.9">statistical machine translation system</entity> . We also show that a good-quality <entity id="J05-4003.10">MT system</entity> can be built from scratch by starting with a very small <entity id="J05-4003.11">parallel corpus</entity> (100,000 <entity id="J05-4003.12">words</entity> ) and exploiting a large <entity id="J05-4003.13">non-parallel corpus</entity> . Thus, our method can be applied with great benefit to <entity id="J05-4003.14">language pairs</entity> for which only scarce <entity id="J05-4003.15">resources</entity> are available. </abstract>


</text>

<text id="P05-1032"> <title>Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases</title> <abstract> In this paper we describe a novel <entity id="P05-1032.1">data structure</entity> for <entity id="P05-1032.2">phrase-based statistical machine translation</entity> which allows for the <entity id="P05-1032.3">retrieval</entity> of arbitrarily long <entity id="P05-1032.4">phrases</entity> while simultaneously using less <entity id="P05-1032.5">memory</entity> than is required by current <entity id="P05-1032.6">decoder</entity> implementations. We detail the <entity id="P05-1032.7">computational complexity</entity> and <entity id="P05-1032.8">average retrieval times</entity> for looking up <entity id="P05-1032.9">phrase translations</entity> in our <entity id="P05-1032.10">suffix array-based data structure</entity> . We show how <entity id="P05-1032.11">sampling</entity> can be used to reduce the <entity id="P05-1032.12">retrieval time</entity> by orders of magnitude with no loss in <entity id="P05-1032.13">translation quality</entity> . </abstract>


</text>

<text id="P05-1034"> <title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT</title> <abstract> We describe a novel approach to <entity id="P05-1034.1">statistical machine translation</entity> that combines <entity id="P05-1034.2">syntactic information</entity> in the <entity id="P05-1034.3">source language</entity> with recent advances in <entity id="P05-1034.4">phrasal translation</entity> . This method requires a <entity id="P05-1034.5">source-language</entity> <entity id="P05-1034.6">dependency parser</entity> , <entity id="P05-1034.7">target language</entity> <entity id="P05-1034.8">word segmentation</entity> and an <entity id="P05-1034.9">unsupervised word alignment component</entity> . We align a <entity id="P05-1034.10">parallel corpus</entity> , project the <entity id="P05-1034.11">source dependency parse</entity> onto the target <entity id="P05-1034.12">sentence</entity> , extract <entity id="P05-1034.13">dependency treelet translation pairs</entity> , and train a <entity id="P05-1034.14">tree-based ordering model</entity> . We describe an efficient <entity id="P05-1034.15">decoder</entity> and show that using these <entity id="P05-1034.16">tree-based models</entity> in combination with conventional <entity id="P05-1034.17">SMT models</entity> provides a promising approach that incorporates the power of <entity id="P05-1034.18">phrasal SMT</entity> with the linguistic generality available in a <entity id="P05-1034.19">parser</entity> . </abstract>


</text>

<text id="P05-1048"> <title>Word Sense Disambiguation vs. Statistical Machine Translation</title> <abstract> We directly investigate a subject of much recent debate: do <entity id="P05-1048.1">word sense disambigation models</entity> help <entity id="P05-1048.2">statistical machine translation</entity> <entity id="P05-1048.3">quality</entity> ? We present empirical results casting doubt on this common, but unproved, assumption. Using a state-of-the-art <entity id="P05-1048.4">Chinese word sense disambiguation model</entity> to choose <entity id="P05-1048.5">translation candidates</entity> for a typical <entity id="P05-1048.6">IBM statistical MT system</entity> , we find that <entity id="P05-1048.7">word sense disambiguation</entity> does not yield significantly better <entity id="P05-1048.8">translation quality</entity> than the <entity id="P05-1048.9">statistical machine translation system</entity> alone. <entity id="P05-1048.10">Error analysis</entity> suggests several key factors behind this surprising finding, including inherent limitations of current <entity id="P05-1048.11">statistical MT architectures</entity> . </abstract>


</text>

<text id="P05-1067"> <title>Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars</title> <abstract> <entity id="P05-1067.1">Syntax-based statistical machine translation (MT)</entity> aims at applying <entity id="P05-1067.2">statistical models</entity> to <entity id="P05-1067.3">structured data</entity> . In this paper, we present a <entity id="P05-1067.4">syntax-based statistical machine translation system</entity> based on a <entity id="P05-1067.5">probabilistic synchronous dependency insertion grammar</entity> . <entity id="P05-1067.6">Synchronous dependency insertion grammars</entity> are a version of <entity id="P05-1067.7">synchronous grammars</entity> defined on <entity id="P05-1067.8">dependency trees</entity> . We first introduce our approach to inducing such a <entity id="P05-1067.9">grammar</entity> from <entity id="P05-1067.10">parallel corpora</entity> . Second, we describe the <entity id="P05-1067.11">graphical model</entity> for the <entity id="P05-1067.12">machine translation task</entity> , which can also be viewed as a <entity id="P05-1067.13">stochastic tree-to-tree transducer</entity> . We introduce a <entity id="P05-1067.14">polynomial time decoding algorithm</entity> for the <entity id="P05-1067.15">model</entity> . We evaluate the outputs of our <entity id="P05-1067.16">MT system</entity> using the <entity id="P05-1067.17">NIST and Bleu automatic MT evaluation software</entity> . The result shows that our system outperforms the <entity id="P05-1067.18">baseline system</entity> based on the <entity id="P05-1067.19">IBM models</entity> in both <entity id="P05-1067.20">translation speed and quality</entity> . </abstract>


</text>

<text id="P05-1069"> <title>A Localized Prediction Model for Statistical Machine Translation</title> <abstract> In this paper, we present a novel <entity id="P05-1069.1">training method</entity> for a <entity id="P05-1069.2">localized phrase-based prediction model</entity> for <entity id="P05-1069.3">statistical machine translation (SMT)</entity> . The <entity id="P05-1069.4">model</entity> predicts <entity id="P05-1069.5">blocks</entity> with orientation to handle <entity id="P05-1069.6">local phrase re-ordering</entity> . We use a <entity id="P05-1069.7">maximum likelihood criterion</entity> to train a <entity id="P05-1069.8">log-linear block bigram model</entity> which uses <entity id="P05-1069.9">real-valued features</entity> (e.g. a <entity id="P05-1069.10">language model score</entity> ) as well as <entity id="P05-1069.11">binary features</entity> based on the <entity id="P05-1069.12">block</entity> identities themselves, e.g. block bigram features. Our <entity id="P05-1069.13">training algorithm</entity> can easily handle millions of <entity id="P05-1069.14">features</entity> . The best system obtains a 18.6% improvement over the <entity id="P05-1069.15">baseline</entity> on a standard <entity id="P05-1069.16">Arabic-English translation task</entity> . </abstract>


</text>

<text id="P05-1074"> <title>Paraphrasing with Bilingual Parallel Corpora</title> <abstract> Previous work has used <entity id="P05-1074.1">monolingual parallel corpora</entity> to extract and generate <entity id="P05-1074.2">paraphrases</entity> . We show that this task can be done using <entity id="P05-1074.3">bilingual parallel corpora</entity> , a much more commonly available <entity id="P05-1074.4">resource</entity> . Using <entity id="P05-1074.5">alignment techniques</entity> from <entity id="P05-1074.6">phrase-based statistical machine translation</entity> , we show how <entity id="P05-1074.7">paraphrases</entity> in one <entity id="P05-1074.8">language</entity> can be identified using a <entity id="P05-1074.9">phrase</entity> in another language as a pivot. We define a <entity id="P05-1074.10">paraphrase probability</entity> that allows <entity id="P05-1074.11">paraphrases</entity> extracted from a <entity id="P05-1074.12">bilingual parallel corpus</entity> to be ranked using <entity id="P05-1074.13">translation probabilities</entity> , and show how it can be refined to take <entity id="P05-1074.14">contextual information</entity> into account. We evaluate our <entity id="P05-1074.15">paraphrase extraction and ranking methods</entity> using a set of <entity id="P05-1074.16">manual word alignments</entity> , and contrast the <entity id="P05-1074.17">quality</entity> with <entity id="P05-1074.18">paraphrases</entity> extracted from <entity id="P05-1074.19">automatic alignments</entity> . </abstract>


</text>

<text id="P05-2016"> <title>Dependency-Based Statistical Machine Translation</title> <abstract> We present a <entity id="P05-2016.1">Czech-English statistical machine translation system</entity> which performs <entity id="P05-2016.2">tree-to-tree translation</entity> of <entity id="P05-2016.3">dependency structures</entity> . The only <entity id="P05-2016.4">bilingual resource</entity> required is a <entity id="P05-2016.5">sentence-aligned parallel corpus</entity> . All other <entity id="P05-2016.6">resources</entity> are <entity id="P05-2016.7">monolingual</entity> . We also refer to an <entity id="P05-2016.8">evaluation method</entity> and plan to compare our <entity id="P05-2016.9">system&apos;s output</entity> with a <entity id="P05-2016.10">benchmark system</entity> . </abstract>


</text>

<text id="E06-1018"> <title>Word Sense Induction: Triplet-Based Clustering and Automatic Evaluation</title> <abstract> In this paper a novel solution to automatic and <entity id="E06-1018.1">unsupervised word sense induction (WSI)</entity> is introduced. It represents an instantiation of the <entity id="E06-1018.2">one sense per collocation observation</entity> (Gale et al., 1992). Like most existing approaches it utilizes <entity id="E06-1018.3">clustering of word co-occurrences</entity> . This approach differs from other approaches to <entity id="E06-1018.4">WSI</entity> in that it enhances the effect of the <entity id="E06-1018.5">one sense per collocation observation</entity> by using triplets of <entity id="E06-1018.6">words</entity> instead of pairs. The combination with a <entity id="E06-1018.7">two-step clustering process</entity> using <entity id="E06-1018.8">sentence co-occurrences</entity> as <entity id="E06-1018.9">features</entity> allows for accurate results. Additionally, a novel and likewise automatic and <entity id="E06-1018.10">unsupervised evaluation method</entity> inspired by Schutze&apos;s (1992) idea of evaluation of <entity id="E06-1018.11">word sense disambiguation algorithms</entity> is employed. Offering advantages like reproducability and independency of a given biased <entity id="E06-1018.12">gold standard</entity> it also enables <entity id="E06-1018.13">automatic parameter optimization</entity> of the <entity id="E06-1018.14">WSI algorithm</entity> . </abstract>


</text>

<text id="E06-1022"> <title>Addressee Identification in Face-to-Face Meetings</title> <abstract> We present results on <entity id="E06-1022.1">addressee identification</entity> in <entity id="E06-1022.2">four-participants face-to-face meetings</entity> using <entity id="E06-1022.3">Bayesian Network</entity> and <entity id="E06-1022.4">Naive Bayes classifiers</entity> . First, we investigate how well the <entity id="E06-1022.5">addressee</entity> of a <entity id="E06-1022.6">dialogue act</entity> can be predicted based on <entity id="E06-1022.7">gaze</entity> , <entity id="E06-1022.8">utterance</entity> and <entity id="E06-1022.9">conversational context features</entity> . Then, we explore whether information about <entity id="E06-1022.10">meeting context</entity> can aid <entity id="E06-1022.11">classifiers</entity> &apos; <entity id="E06-1022.12">performances</entity> . Both <entity id="E06-1022.13">classifiers</entity> perform the best when <entity id="E06-1022.14">conversational context</entity> and <entity id="E06-1022.15">utterance features</entity> are combined with <entity id="E06-1022.16">speaker&apos;s gaze information</entity> . The <entity id="E06-1022.17">classifiers</entity> show little <entity id="E06-1022.18">gain</entity> from information about <entity id="E06-1022.19">meeting context</entity> . </abstract>


</text>

<text id="E06-1031"> <title>CDER: Efficient MT Evaluation Using Block Movements</title> <abstract> Most state-of-the-art <entity id="E06-1031.1">evaluation measures</entity> for <entity id="E06-1031.2">machine translation</entity> assign high <entity id="E06-1031.3">costs</entity> to movements of <entity id="E06-1031.4">word</entity> blocks. In many cases though such movements still result in correct or almost correct <entity id="E06-1031.5">sentences</entity> . In this paper, we will present a new <entity id="E06-1031.6">evaluation measure</entity> which explicitly models <entity id="E06-1031.7">block reordering</entity> as an <entity id="E06-1031.8">edit operation</entity> . Our <entity id="E06-1031.9">measure</entity> can be exactly calculated in <entity id="E06-1031.10">quadratic time</entity> . Furthermore, we will show how some <entity id="E06-1031.11">evaluation measures</entity> can be improved by the introduction of <entity id="E06-1031.12">word-dependent substitution costs</entity> . The correlation of the new <entity id="E06-1031.13">measure</entity> with <entity id="E06-1031.14">human judgment</entity> has been investigated systematically on two different <entity id="E06-1031.15">language pairs</entity> . The experimental results will show that it significantly outperforms state-of-the-art approaches in <entity id="E06-1031.16">sentence-level correlation</entity> . Results from experiments with <entity id="E06-1031.17">word dependent substitution costs</entity> will demonstrate an additional increase of correlation between <entity id="E06-1031.18">automatic evaluation measures</entity> and <entity id="E06-1031.19">human judgment</entity> . </abstract> 


</text>

<text id="E06-1035"> <title>Automatic Segmentation of Multiparty Dialogue</title> <abstract> In this paper, we investigate the problem of automatically predicting <entity id="E06-1035.1">segment boundaries</entity> in <entity id="E06-1035.2">spoken multiparty dialogue</entity> . We extend prior work in two ways. We first apply approaches that have been proposed for <entity id="E06-1035.3">predicting top-level topic shifts</entity> to the problem of <entity id="E06-1035.4">identifying subtopic boundaries</entity> . We then explore the impact on <entity id="E06-1035.5">performance</entity> of using <entity id="E06-1035.6">ASR output</entity> as opposed to <entity id="E06-1035.7">human transcription</entity> . Examination of the effect of <entity id="E06-1035.8">features</entity> shows that <entity id="E06-1035.9">predicting top-level and predicting subtopic boundaries</entity> are two distinct tasks: (1) for predicting <entity id="E06-1035.10">subtopic boundaries</entity> , the <entity id="E06-1035.11">lexical cohesion-based approach</entity> alone can achieve competitive results, (2) for <entity id="E06-1035.12">predicting top-level boundaries</entity> , the <entity id="E06-1035.13">machine learning approach</entity> that combines <entity id="E06-1035.14">lexical-cohesion and conversational features</entity> performs best, and (3) <entity id="E06-1035.15">conversational cues</entity> , such as <entity id="E06-1035.16">cue phrases</entity> and <entity id="E06-1035.17">overlapping speech</entity> , are better indicators for the top-level prediction task. We also find that the <entity id="E06-1035.18">transcription errors</entity> inevitable in <entity id="E06-1035.19">ASR output</entity> have a negative impact on models that combine <entity id="E06-1035.20">lexical-cohesion and conversational features</entity> , but do not change the general preference of approach for the two tasks. </abstract> 


</text>

<text id="P06-1013"> <title>Ensemble Methods for Unsupervised WSD</title> <abstract> <entity id="P06-1013.1">Combination methods</entity> are an effective way of improving <entity id="P06-1013.2">system performance</entity> . This paper examines the benefits of <entity id="P06-1013.3">system combination</entity> for <entity id="P06-1013.4">unsupervised WSD</entity> . We investigate several <entity id="P06-1013.5">voting- and arbiter-based combination strategies</entity> over a diverse pool of <entity id="P06-1013.6">unsupervised WSD systems</entity> . Our <entity id="P06-1013.7">combination methods</entity> rely on <entity id="P06-1013.8">predominant senses</entity> which are derived automatically from <entity id="P06-1013.9">raw text</entity> . Experiments using the <entity id="P06-1013.10">SemCor</entity> and <entity id="P06-1013.11">Senseval-3 data sets</entity> demonstrate that our ensembles yield significantly better results when compared with state-of-the-art. </abstract> 


</text>

<text id="P06-1052"> <title>An Improved Redundancy Elimination Algorithm for Underspecified Representations</title> <abstract> We present an efficient algorithm for the <entity id="P06-1052.1">redundancy elimination problem</entity> : Given an <entity id="P06-1052.2">underspecified semantic representation (USR)</entity> of a <entity id="P06-1052.3">scope ambiguity</entity> , compute an <entity id="P06-1052.4">USR</entity> with fewer mutually <entity id="P06-1052.5">equivalent readings</entity> . The algorithm operates on <entity id="P06-1052.6">underspecified chart representations</entity> which are derived from <entity id="P06-1052.7">dominance graphs</entity> ; it can be applied to the <entity id="P06-1052.8">USRs</entity> computed by <entity id="P06-1052.9">large-scale grammars</entity> . We evaluate the algorithm on a <entity id="P06-1052.10">corpus</entity> , and show that it reduces the degree of <entity id="P06-1052.11">ambiguity</entity> significantly while taking negligible runtime. </abstract>


</text>

<text id="P06-2001"> <title>Using Machine Learning Techniques to Build a Comma Checker for Basque</title> <abstract> In this paper, we describe the research using <entity id="P06-2001.1">machine learning techniques</entity> to build a <entity id="P06-2001.2">comma checker</entity> to be integrated in a <entity id="P06-2001.3">grammar checker</entity> for <entity id="P06-2001.4">Basque</entity> . After several experiments, and trained with a little <entity id="P06-2001.5">corpus</entity> of 100,000 <entity id="P06-2001.6">words</entity> , the system guesses correctly not placing <entity id="P06-2001.7">commas</entity> with a <entity id="P06-2001.8">precision</entity> of 96% and a <entity id="P06-2001.9">recall</entity> of 98%. It also gets a <entity id="P06-2001.10">precision</entity> of 70% and a <entity id="P06-2001.11">recall</entity> of 49% in the task of placing <entity id="P06-2001.12">commas</entity> . Finally, we have shown that these results can be improved using a bigger and a more homogeneous <entity id="P06-2001.13">corpus</entity> to train, that is, a bigger <entity id="P06-2001.14">corpus</entity> written by one unique <entity id="P06-2001.15">author</entity> . </abstract>


</text>

<text id="P06-2012"> <title>Unsupervised Relation Disambiguation Using Spectral Clustering</title> <abstract> This paper presents an <entity id="P06-2012.1">unsupervised learning approach</entity> to disambiguate various relations between <entity id="P06-2012.2">named entities</entity> by use of various <entity id="P06-2012.3">lexical and syntactic features</entity> from the <entity id="P06-2012.4">contexts</entity> . It works by calculating <entity id="P06-2012.5">eigenvectors</entity> of an <entity id="P06-2012.6">adjacency graph</entity> &apos;s <entity id="P06-2012.7">Laplacian</entity> to recover a <entity id="P06-2012.8">submanifold</entity> of data from a <entity id="P06-2012.9">high dimensionality space</entity> and then performing <entity id="P06-2012.10">cluster number estimation</entity> on the <entity id="P06-2012.11">eigenvectors</entity> . Experiment results on <entity id="P06-2012.12">ACE corpora</entity> show that this <entity id="P06-2012.13">spectral clustering based approach</entity> outperforms the other <entity id="P06-2012.14">clustering methods</entity> . </abstract>


</text>

<text id="P06-2059"> <title>Automatic Construction of Polarity-tagged Corpus from HTML Documents</title> <abstract> This paper proposes a novel method of building <entity id="P06-2059.1">polarity-tagged corpus</entity> from <entity id="P06-2059.2">HTML documents</entity> . The characteristics of this method is that it is fully automatic and can be applied to arbitrary <entity id="P06-2059.3">HTML documents</entity> . The idea behind our method is to utilize certain <entity id="P06-2059.4">layout structures</entity> and <entity id="P06-2059.5">linguistic pattern</entity> . By using them, we can automatically extract such <entity id="P06-2059.6">sentences</entity> that express opinion. In our experiment, the method could construct a <entity id="P06-2059.7">corpus</entity> consisting of 126,610 <entity id="P06-2059.8">sentences</entity> . </abstract>


</text>

<text id="H01-1040"> <title>Intelligent Access to Text: Integrating Information Extraction Technology into Text Browsers</title> <abstract> 
 In this paper we show how two standard outputs from <entity id="H01-1040.1">information extraction (IE) systems</entity> - <entity id="H01-1040.2">named entity annotations</entity> and <entity id="H01-1040.3">scenario templates</entity> - can be used to enhance access to <entity id="H01-1040.4">text collections</entity> via a standard <entity id="H01-1040.5">text browser</entity> . We describe how this information is used in a <entity id="H01-1040.6">prototype system</entity> designed to support <entity id="H01-1040.7">information workers</entity> &apos; access to a <entity id="H01-1040.8">pharmaceutical news archive</entity> as part of their <entity id="H01-1040.9">industry watch</entity> function. We also report results of a preliminary, <entity id="H01-1040.10">qualitative user evaluation</entity> of the system, which while broadly positive indicates further work needs to be done on the <entity id="H01-1040.11">interface</entity> to make <entity id="H01-1040.12">users</entity> aware of the increased potential of <entity id="H01-1040.13">IE-enhanced text browsers</entity> . </abstract>


</text>

<text id="H01-1055"> <title>Natural Language Generation in Dialog Systems</title> <abstract> 
 Recent advances in <entity id="H01-1055.1">Automatic Speech Recognition technology</entity> have put the goal of naturally sounding <entity id="H01-1055.2">dialog systems</entity> within reach. However, the improved <entity id="H01-1055.3">speech recognition</entity> has brought to light a new problem: as <entity id="H01-1055.4">dialog systems</entity> understand more of what the <entity id="H01-1055.5">user</entity> tells them, they need to be more sophisticated at responding to the <entity id="H01-1055.6">user</entity> . The issue of <entity id="H01-1055.7">system response</entity> to <entity id="H01-1055.8">users</entity> has been extensively studied by the <entity id="H01-1055.9">natural language generation community</entity> , though rarely in the context of <entity id="H01-1055.10">dialog systems</entity> . We show how research in <entity id="H01-1055.11">generation</entity> can be adapted to <entity id="H01-1055.12">dialog systems</entity> , and how the high cost of hand-crafting <entity id="H01-1055.13">knowledge-based generation systems</entity> can be overcome by employing <entity id="H01-1055.14">machine learning techniques</entity> . </abstract>


</text>

<text id="H01-1068"> <title>A Three-Tiered Evaluation Approach for Interactive Spoken Dialogue Systems</title> <abstract> 
 We describe a three-tiered approach for evaluation of <entity id="H01-1068.1">spoken dialogue systems</entity> . The three tiers measure <entity id="H01-1068.2">user satisfaction</entity> , <entity id="H01-1068.3">system support of mission success</entity> and <entity id="H01-1068.4">component performance</entity> . We describe our use of this approach in numerous fielded <entity id="H01-1068.5">user studies</entity> conducted with the U.S. military. </abstract>


</text>

<text id="N03-4004"> <title>TAP-XL: An Automated Analyst&apos;s Assistant</title> <abstract> 
 The <entity id="N03-4004.1">TAP-XL Automated Analyst&apos;s Assistant</entity> is an application designed to help an <entity id="N03-4004.2">English</entity> -speaking analyst write a <entity id="N03-4004.3">topical report</entity> , culling information from a large inflow of <entity id="N03-4004.4">multilingual, multimedia data</entity> . It gives users the ability to spend their time finding more data relevant to their task, and gives them translingual reach into other <entity id="N03-4004.5">languages</entity> by leveraging <entity id="N03-4004.6">human language technology</entity> . </abstract>


</text>

<text id="H05-1101"> <title>Some Computational Complexity Results for Synchronous Context-Free Grammars</title> <abstract> 
 This paper investigates some <entity id="H05-1101.1">computational problems</entity> associated with <entity id="H05-1101.2">probabilistic translation models</entity> that have recently been adopted in the literature on <entity id="H05-1101.3">machine translation</entity> . These <entity id="H05-1101.4">models</entity> can be viewed as pairs of <entity id="H05-1101.5">probabilistic context-free grammars</entity> working in a &apos;synchronous&apos; way. Two <entity id="H05-1101.6">hardness</entity> results for the class <entity id="H05-1101.7">NP</entity> are reported, along with an <entity id="H05-1101.8">exponential time lower-bound</entity> for certain classes of algorithms that are currently used in the literature. </abstract>


</text>

<text id="I05-2014"> <title>BLEU in characters: towards automatic MT evaluation in languages without word delimiters</title> <abstract> 
 Automatic <entity id="I05-2014.1">evaluation metrics</entity> for <entity id="I05-2014.2">Machine Translation (MT) systems</entity> , such as <entity id="I05-2014.3">BLEU</entity> or <entity id="I05-2014.4">NIST</entity> , are now well established. Yet, they are scarcely used for the assessment of <entity id="I05-2014.5">language pairs</entity> like <entity id="I05-2014.6">English-Chinese</entity> or <entity id="I05-2014.7">English-Japanese</entity> , because of the <entity id="I05-2014.8">word segmentation problem</entity> . This study establishes the equivalence between the standard use of <entity id="I05-2014.9">BLEU</entity> in <entity id="I05-2014.10">word n-grams</entity> and its application at the <entity id="I05-2014.11">character</entity> level. The use of <entity id="I05-2014.12">BLEU</entity> at the <entity id="I05-2014.13">character</entity> level eliminates the <entity id="I05-2014.14">word segmentation problem</entity> : it makes it possible to directly compare commercial systems outputting <entity id="I05-2014.15">unsegmented texts</entity> with, for instance, <entity id="I05-2014.16">statistical MT systems</entity> which usually segment their <entity id="I05-2014.17">outputs</entity> . </abstract>


</text>

<text id="P05-3025"> <title>Interactively Exploring a Machine Translation Model</title> <abstract> 
 This paper describes a method of <entity id="P05-3025.1">interactively visualizing and directing the process</entity> of <entity id="P05-3025.2">translating a sentence</entity> . The method allows a <entity id="P05-3025.3">user</entity> to explore a <entity id="P05-3025.4">model</entity> of <entity id="P05-3025.5">syntax-based statistical machine translation (MT)</entity> , to understand the <entity id="P05-3025.6">model</entity> &apos;s strengths and weaknesses, and to compare it to other <entity id="P05-3025.7">MT systems</entity> . Using this <entity id="P05-3025.8">visualization method</entity> , we can find and address conceptual and practical problems in an <entity id="P05-3025.9">MT system</entity> . In our demonstration at <entity id="P05-3025.10">ACL</entity> , new <entity id="P05-3025.11">users</entity> of our tool will drive a <entity id="P05-3025.12">syntax-based decoder</entity> for themselves. </abstract>


</text>

<text id="E06-1004"> <title>Computational Complexity of Statistical Machine Translation</title> <abstract> 
 In this paper we study a set of problems that are of considerable importance to <entity id="E06-1004.1">Statistical Machine Translation (SMT)</entity> but which have not been addressed satisfactorily by the <entity id="E06-1004.2">SMT research community</entity> . Over the last decade, a variety of <entity id="E06-1004.3">SMT algorithms</entity> have been built and empirically tested whereas little is known about the <entity id="E06-1004.4">computational complexity</entity> of some of the fundamental problems of <entity id="E06-1004.5">SMT</entity> . Our work aims at providing useful insights into the the <entity id="E06-1004.6">computational complexity</entity> of those problems. We prove that while <entity id="E06-1004.7">IBM Models 1-2</entity> are conceptually and computationally simple, computations involving the higher (and more useful) <entity id="E06-1004.8">models</entity> are <entity id="E06-1004.9">hard</entity> . Since it is unlikely that there exists a <entity id="E06-1004.10">polynomial time solution</entity> for any of these <entity id="E06-1004.11">hard problems</entity> (unless <entity id="E06-1004.12">P = NP</entity> and <entity id="E06-1004.13">P#P = P</entity> ), our results highlight and justify the need for developing <entity id="E06-1004.14">polynomial time approximations</entity> for these computations. We also discuss some practical ways of dealing with <entity id="E06-1004.15">complexity</entity> . </abstract>


</text>

<text id="E06-1041"> <title>Structuring Knowledge for Reference Generation: A Clustering Algorithm</title> <abstract> 
 This paper discusses two problems that arise in the <entity id="E06-1041.1">Generation</entity> of <entity id="E06-1041.2">Referring Expressions</entity> : (a) <entity id="E06-1041.3">numeric-valued attributes</entity> , such as size or location; (b) <entity id="E06-1041.4">perspective-taking</entity> in <entity id="E06-1041.5">reference</entity> . Both problems, it is argued, can be resolved if some structure is imposed on the available knowledge prior to <entity id="E06-1041.6">content determination</entity> . We describe a <entity id="E06-1041.7">clustering algorithm</entity> which is sufficiently general to be applied to these diverse problems, discuss its application, and evaluate its performance. </abstract>


</text>

<text id="N06-2009"> <title>Answering the Question You Wish They Had Asked: The Impact of Paraphrasing for Question Answering</title> <abstract> 
 State-of-the-art <entity id="N06-2009.1">Question Answering (QA) systems</entity> are very sensitive to variations in the phrasing of an <entity id="N06-2009.2">information need</entity> . Finding the preferred <entity id="N06-2009.3">language</entity> for such a <entity id="N06-2009.4">need</entity> is a valuable task. We investigate that claim by adopting a simple <entity id="N06-2009.5">MT-based paraphrasing technique</entity> and evaluating <entity id="N06-2009.6">QA system</entity> performance on <entity id="N06-2009.7">paraphrased questions</entity> . We found a potential increase of 35% in <entity id="N06-2009.8">MRR</entity> with respect to the original <entity id="N06-2009.9">question</entity> . </abstract>


</text>

<text id="N06-2038"> <title>A Comparison of Tagging Strategies for Statistical Information Extraction</title> <abstract> 
 There are several approaches that model <entity id="N06-2038.1">information extraction</entity> as a <entity id="N06-2038.2">token classification task</entity> , using various <entity id="N06-2038.3">tagging strategies</entity> to combine multiple <entity id="N06-2038.4">tokens</entity> . We describe the <entity id="N06-2038.5">tagging strategies</entity> that can be found in the literature and evaluate their relative performances. We also introduce a new strategy, called <entity id="N06-2038.6">Begin/After tagging</entity> or <entity id="N06-2038.7">BIA</entity> , and show that it is competitive to the best other strategies. </abstract> 


</text>

<text id="N06-4001"> <title>InfoMagnets: Making Sense of Corpus Data</title> <abstract> 
 We introduce a new <entity id="N06-4001.1">interactive corpus exploration tool</entity> called <entity id="N06-4001.2">InfoMagnets</entity> . <entity id="N06-4001.3">InfoMagnets</entity> aims at making <entity id="N06-4001.4">exploratory corpus analysis</entity> accessible to researchers who are not experts in <entity id="N06-4001.5">text mining</entity> . As evidence of its usefulness and usability, it has been used successfully in a research context to uncover relationships between <entity id="N06-4001.6">language</entity> and <entity id="N06-4001.7">behavioral patterns</entity> in two distinct domains: <entity id="N06-4001.8">tutorial dialogue </entity>(Kumar et al., submitted) and <entity id="N06-4001.9">on-line communities</entity> (Arguello et al., 2006). As an <entity id="N06-4001.10">educational tool</entity> , it has been used as part of a unit on <entity id="N06-4001.11">protocol analysis</entity> in an <entity id="N06-4001.12">Educational Research Methods course</entity> . </abstract>


</text>

<text id="P06-1018"> <title>Polarized Unification Grammars</title> <abstract> 
 This paper proposes a generic <entity id="P06-1018.1">mathematical formalism</entity> for the combination of various <entity id="P06-1018.2">structures</entity> : <entity id="P06-1018.3">strings</entity> , <entity id="P06-1018.4">trees</entity> , <entity id="P06-1018.5">dags</entity> , <entity id="P06-1018.6">graphs</entity> , and products of them. The <entity id="P06-1018.7">polarization</entity> of the objects of the <entity id="P06-1018.8">elementary structures</entity> controls the <entity id="P06-1018.9">saturation</entity> of the final <entity id="P06-1018.10">structure</entity> . This formalism is both elementary and powerful enough to strongly simulate many <entity id="P06-1018.11">grammar formalisms</entity> , such as <entity id="P06-1018.12">rewriting systems</entity> , <entity id="P06-1018.13">dependency grammars</entity> , <entity id="P06-1018.14">TAG</entity> , <entity id="P06-1018.15">HPSG</entity> and <entity id="P06-1018.16">LFG</entity> . </abstract> 


</text>

<text id="P06-2110"> <title>Word Vectors and Two Kinds of Similarity</title> <abstract> 
 This paper examines what kind of <entity id="P06-2110.1">similarity</entity> between <entity id="P06-2110.2">words</entity> can be represented by what kind of <entity id="P06-2110.3">word vectors</entity> in the <entity id="P06-2110.4">vector space model</entity> . Through two experiments, three <entity id="P06-2110.5">methods for constructing word vectors</entity> , i.e., <entity id="P06-2110.6">LSA-based, cooccurrence-based and dictionary-based methods</entity> , were compared in terms of the ability to represent two kinds of <entity id="P06-2110.7">similarity</entity> , i.e., <entity id="P06-2110.8">taxonomic similarity</entity> and <entity id="P06-2110.9">associative similarity</entity> . The result of the comparison was that the <entity id="P06-2110.10">dictionary-based word vectors</entity> better reflect <entity id="P06-2110.11">taxonomic similarity</entity> , while the <entity id="P06-2110.12">LSA-based and the cooccurrence-based word vectors</entity> better reflect <entity id="P06-2110.13">associative similarity</entity> . </abstract> 


</text>

<text id="P06-3007"> <title>Investigations on Event-Based Summarization</title> <abstract> 
 We investigate independent and relevant event-based extractive <entity id="P06-3007.1">mutli-document summarization approaches</entity> . In this paper, <entity id="P06-3007.2">events</entity> are defined as <entity id="P06-3007.3">event terms</entity> and <entity id="P06-3007.4">associated event elements</entity> . With independent approach, we identify important <entity id="P06-3007.5">contents</entity> by frequency of <entity id="P06-3007.6">events</entity> . With relevant approach, we identify important contents by <entity id="P06-3007.7">PageRank algorithm</entity> on the <entity id="P06-3007.8">event map</entity> constructed from <entity id="P06-3007.9">documents</entity> . Experimental results are encouraging. </abstract>


</text>

<text id="P06-4007"> <title>FERRET: Interactive Question-Answering for Real-World Environments</title> <abstract> 
 This paper describes <entity id="P06-4007.1">FERRET</entity> , an <entity id="P06-4007.2">interactive question-answering (Q/A) system</entity> designed to address the challenges of integrating <entity id="P06-4007.3">automatic Q/A</entity> applications into real-world environments. <entity id="P06-4007.4">FERRET</entity> utilizes a novel approach to <entity id="P06-4007.5">Q/A</entity> known as <entity id="P06-4007.6">predictive questioning</entity> which attempts to identify the <entity id="P06-4007.7">questions</entity> (and <entity id="P06-4007.8">answers</entity> ) that <entity id="P06-4007.9">users</entity> need by analyzing how a <entity id="P06-4007.10">user</entity> interacts with a system while gathering information related to a particular scenario. </abstract>


</text>

<text id="P06-4011"> <title>Computational Analysis of Move Structures in Academic Abstracts</title> <abstract> 
 This paper introduces a method for <entity id="P06-4011.1">computational analysis of move structures</entity> in <entity id="P06-4011.2">abstracts</entity> of <entity id="P06-4011.3">research articles</entity> . In our approach, <entity id="P06-4011.4">sentences</entity> in a given <entity id="P06-4011.5">abstract</entity> are analyzed and labeled with a specific <entity id="P06-4011.6">move</entity> in light of various <entity id="P06-4011.7">rhetorical functions</entity> . The method involves automatically gathering a large number of <entity id="P06-4011.8">abstracts</entity> from the <entity id="P06-4011.9">Web</entity> and building a <entity id="P06-4011.10">language model</entity> of <entity id="P06-4011.11">abstract moves</entity> . We also present a prototype <entity id="P06-4011.12">concordancer</entity> , <entity id="P06-4011.13">CARE</entity> , which exploits the <entity id="P06-4011.14">move-tagged abstracts</entity> for <entity id="P06-4011.15">digital learning</entity> . This system provides a promising approach to <entity id="P06-4011.16">Web-based computer-assisted academic writing</entity> . </abstract>


</text>

<text id="P06-4014"> <title>Re-Usable Tools for Precision Machine Translation</title> <abstract> 
 The <entity id="P06-4014.1">LOGON MT demonstrator</entity> assembles independently valuable <entity id="P06-4014.2">general-purpose NLP components</entity> into a <entity id="P06-4014.3">machine translation pipeline</entity> that capitalizes on <entity id="P06-4014.4">output quality</entity> . The demonstrator embodies an interesting combination of <entity id="P06-4014.5">hand-built, symbolic resources</entity> and <entity id="P06-4014.6">stochastic processes</entity> . </abstract> 


</text>

<text id="T78-1001"> <title>Testing The Psychological Reality of a Representational Model</title> <abstract> 
 A research program is described in which a particular <entity id="T78-1001.1">representational format for meaning</entity> is tested as broadly as possible. In this format, developed by the LNR research group at The University of California at San Diego, <entity id="T78-1001.2">verbs</entity> are represented as interconnected sets of <entity id="T78-1001.3">subpredicates</entity> . These <entity id="T78-1001.4">subpredicates</entity> may be thought of as the almost inevitable <entity id="T78-1001.5">inferences</entity> that a <entity id="T78-1001.6">listener</entity> makes when a <entity id="T78-1001.7">verb</entity> is used in a <entity id="T78-1001.8">sentence</entity> . They confer a <entity id="T78-1001.9">meaning structure</entity> on the <entity id="T78-1001.10">sentence</entity> in which the <entity id="T78-1001.11">verb</entity> is used. </abstract> 


</text>

<text id="T78-1028"> <title>Fragments of a Theory of Human Plausible Reasoning</title> <abstract> 
 The paper outlines a <entity id="T78-1028.1">computational theory</entity> of <entity id="T78-1028.2">human plausible reasoning</entity> constructed from analysis of people&apos;s answers to everyday questions. Like <entity id="T78-1028.3">logic</entity> , the <entity id="T78-1028.4">theory</entity> is expressed in a <entity id="T78-1028.5">content-independent formalism</entity> . Unlike <entity id="T78-1028.6">logic</entity> , the <entity id="T78-1028.7">theory</entity> specifies how different information in <entity id="T78-1028.8">memory</entity> affects the certainty of the conclusions drawn. The <entity id="T78-1028.9">theory</entity> consists of a <entity id="T78-1028.10">dimensionalized space</entity> of different <entity id="T78-1028.11">inference types</entity> and their <entity id="T78-1028.12">certainty conditions</entity> , including a variety of <entity id="T78-1028.13">meta-inference types</entity> where the <entity id="T78-1028.14">inference</entity> depends on the person&apos;s knowledge about his own knowledge. The protocols from people&apos;s answers to questions are analyzed in terms of the different <entity id="T78-1028.15">inference types</entity> . The paper also discusses how <entity id="T78-1028.16">memory</entity> is structured in multiple ways to support the different <entity id="T78-1028.17">inference types</entity> , and how the information found in <entity id="T78-1028.18">memory</entity> determines which <entity id="T78-1028.19">inference types</entity> are triggered. </abstract> 


</text>

<text id="T78-1031"> <title>PATH-BASED AND NODE-BASED INFERENCE IN SEMANTIC NETWORKS</title> <abstract> 
 Two styles of performing <entity id="T78-1031.1">inference</entity> in <entity id="T78-1031.2">semantic networks</entity> are presented and compared. <entity id="T78-1031.3">Path-based inference</entity> allows an <entity id="T78-1031.4">arc</entity> or a <entity id="T78-1031.5">path of arcs</entity> between two given <entity id="T78-1031.6">nodes</entity> to be inferred from the existence of another specified <entity id="T78-1031.7">path</entity> between the same two <entity id="T78-1031.8">nodes</entity> . <entity id="T78-1031.9">Path-based inference rules</entity> may be written using a <entity id="T78-1031.10">binary relational calculus notation</entity> . <entity id="T78-1031.11">Node-based inference</entity> allows a <entity id="T78-1031.12">structure</entity> of <entity id="T78-1031.13">nodes</entity> to be inferred from the existence of an instance of a pattern of <entity id="T78-1031.14">node structures</entity> . <entity id="T78-1031.15">Node-based inference rules</entity> can be constructed in a <entity id="T78-1031.16">semantic network</entity> using a variant of a <entity id="T78-1031.17">predicate calculus notation</entity> . <entity id="T78-1031.18">Path-based inference</entity> is more efficient, while <entity id="T78-1031.19">node-based inference</entity> is more general. A method is described of combining the two styles in a single system in order to take advantage of the strengths of each. Applications of <entity id="T78-1031.20">path-based inference rules</entity> to the representation of the <entity id="T78-1031.21">extensional equivalence</entity> of <entity id="T78-1031.22">intensional concepts</entity> , and to the <entity id="T78-1031.23">explication</entity> of <entity id="T78-1031.24">inheritance</entity> in <entity id="T78-1031.25">hierarchies</entity> are sketched. </abstract> 


</text>

<text id="C80-1039"> <title>ON FROFF: A TEXT PROCESSING SYSTEM FOR ENGLISH TEXTS AND FIGURES</title> <abstract> 
 In order to meet the needs of a publication of papers in English, many systems to run off texts have been developed. In this paper, we report a system <entity id="C80-1039.1">FROFF</entity> which can make a fair copy of not only texts but also graphs and tables indispensable to our papers. Its selection of <entity id="C80-1039.2">fonts</entity> , specification of <entity id="C80-1039.3">character</entity> size are dynamically changeable, and the <entity id="C80-1039.4">typing location</entity> can be also changed in lateral or longitudinal directions. Each <entity id="C80-1039.5">character</entity> has its own width and a line length is counted by the sum of each <entity id="C80-1039.6">character</entity> . By using commands or <entity id="C80-1039.7">rules</entity> which are defined to facilitate the construction of format expected or some <entity id="C80-1039.8">mathematical expressions</entity> , elaborate and pretty documents can be successfully obtained. </abstract> 


</text>

<text id="C80-1073"> <title>ATNS USED AS A PROCEDURAL DIALOG MODEL</title> <abstract> 
 An attempt has been made to use an <entity id="C80-1073.1">Augmented Transition Network</entity> as a procedural <entity id="C80-1073.2">dialog model</entity> . The development of such a <entity id="C80-1073.3">model</entity> appears to be important in several respects: as a device to represent and to use different <entity id="C80-1073.4">dialog schemata</entity> proposed in empirical <entity id="C80-1073.5">conversation analysis</entity> ; as a device to represent and to use <entity id="C80-1073.6">models of verbal interaction</entity> ; as a device combining knowledge about <entity id="C80-1073.7">dialog schemata</entity> and about <entity id="C80-1073.8">verbal interaction</entity> with knowledge about <entity id="C80-1073.9">task-oriented and goal-directed dialogs</entity> . A standard <entity id="C80-1073.10">ATN</entity> should be further developed in order to account for the <entity id="C80-1073.11">verbal interactions</entity> of <entity id="C80-1073.12">task-oriented dialogs</entity> . </abstract> 


</text>

<text id="P80-1004"> <title>Metaphor - A Key to Extensible Semantic Analysis</title> <abstract> 
 Interpreting <entity id="P80-1004.1">metaphors</entity> is an integral and inescapable process in <entity id="P80-1004.2">human understanding of natural language</entity> . This paper discusses a <entity id="P80-1004.3">method of analyzing metaphors</entity> based on the existence of a small number of <entity id="P80-1004.4">generalized metaphor mappings</entity> . Each <entity id="P80-1004.5">generalized metaphor</entity> contains a <entity id="P80-1004.6">recognition network</entity> , a <entity id="P80-1004.7">basic mapping</entity> , additional <entity id="P80-1004.8">transfer mappings</entity> , and an <entity id="P80-1004.9">implicit intention component</entity> . It is argued that the method reduces <entity id="P80-1004.10">metaphor interpretation</entity> from a <entity id="P80-1004.11">reconstruction</entity> to a <entity id="P80-1004.12">recognition task</entity> . Implications towards automating certain aspects of <entity id="P80-1004.13">language learning</entity> are also discussed. </abstract>


</text>

<text id="P80-1019"> <title>Expanding the Horizons of Natural Language Interfaces</title> <abstract> 
 Current <entity id="P80-1019.1">natural language interfaces</entity> have concentrated largely on determining the literal <entity id="P80-1019.2">meaning</entity> of <entity id="P80-1019.3">input</entity> from their <entity id="P80-1019.4">users</entity> . While such <entity id="P80-1019.5">decoding</entity> is an essential underpinning, much recent work suggests that <entity id="P80-1019.6">natural language interfaces</entity> will never appear cooperative or graceful unless they also incorporate numerous <entity id="P80-1019.7">non-literal aspects of communication</entity> , such as robust <entity id="P80-1019.8">communication procedures</entity> . This paper defends that view, but claims that direct imitation of human performance is not the best way to implement many of these <entity id="P80-1019.9">non-literal aspects of communication</entity> ; that the new technology of powerful <entity id="P80-1019.10">personal computers</entity> with integral <entity id="P80-1019.11">graphics displays</entity> offers techniques superior to those of humans for these aspects, while still satisfying <entity id="P80-1019.12">human communication needs</entity> . The paper proposes <entity id="P80-1019.13">interfaces</entity> based on a judicious mixture of these techniques and the still valuable methods of more traditional <entity id="P80-1019.14">natural language interfaces</entity>. </abstract> 


</text>

<text id="P80-1026"> <title>Flexiable Parsing</title> <abstract> 
When people use <entity id="P80-1026.1">natural language</entity> in natural settings, they often use it ungrammatically, missing out or repeating words, breaking-off and restarting, speaking in fragments, etc.. Their <entity id="P80-1026.2">human listeners</entity> are usually able to cope with these deviations with little difficulty. If a <entity id="P80-1026.3">computer system</entity> wishes to accept <entity id="P80-1026.4">natural language input</entity> from its <entity id="P80-1026.5">users</entity> on a routine basis, it must display a similar indifference. In this paper, we outline a set of <entity id="P80-1026.6">parsing flexibilities</entity> that such a system should provide. We go, on to describe <entity id="P80-1026.7">FlexP</entity> , a <entity id="P80-1026.8">bottom-up pattern-matching parser</entity> that we have designed and implemented to provide these flexibilities for <entity id="P80-1026.9">restricted natural language</entity> input to a limited-domain computer system. </abstract> 


</text>

<text id="C82-1054"> <title>AN IMPROVED LEFT-CORNER PARSING ALGORITHM</title> <abstract> 
 This paper proposes a series of modifications to the <entity id="C82-1054.1">left corner parsing algorithm</entity> for <entity id="C82-1054.2">context-free grammars</entity> . It is argued that the resulting algorithm is both efficient and flexible and is, therefore, a good choice for the <entity id="C82-1054.3">parser</entity> used in a <entity id="C82-1054.4">natural language interface</entity> . </abstract>


</text>

<text id="J82-3002"> <title>An Efficient Easily Adaptable System for Interpreting Natural Language Queries</title> <abstract> 
 This paper gives an overall account of a prototype <entity id="J82-3002.1">natural language question answering system</entity> , called <entity id="J82-3002.2">Chat-80</entity> . <entity id="J82-3002.3">Chat-80</entity> has been designed to be both efficient and easily adaptable to a variety of applications. The system is implemented entirely in <entity id="J82-3002.4">Prolog</entity> , a <entity id="J82-3002.5">programming language</entity> based on <entity id="J82-3002.6">logic</entity> . With the aid of a <entity id="J82-3002.7">logic-based grammar formalism</entity> called <entity id="J82-3002.8">extraposition grammars</entity> , <entity id="J82-3002.9">Chat-80</entity> translates <entity id="J82-3002.10">English questions</entity> into the <entity id="J82-3002.11">Prolog</entity> <entity id="J82-3002.12">subset of logic</entity> . The resulting <entity id="J82-3002.13">logical expression</entity> is then transformed by a <entity id="J82-3002.14">planning algorithm</entity> into efficient <entity id="J82-3002.15">Prolog</entity> , cf. <entity id="J82-3002.16">query optimisation</entity> in a <entity id="J82-3002.17">relational database</entity> . Finally, the <entity id="J82-3002.18">Prolog form</entity> is executed to yield the answer. </abstract>


</text>

<text id="P82-1035"> <title>Scruffy Text Understanding: Design and Implementation of &apos;Tolerant&apos; Understanders</title> <abstract> 
 Most large <entity id="P82-1035.1">text-understanding systems</entity> have been designed under the assumption that the input <entity id="P82-1035.2">text</entity> will be in reasonably neat form, e.g., <entity id="P82-1035.3">newspaper stories</entity> and other <entity id="P82-1035.4">edited texts</entity> . However, a great deal of <entity id="P82-1035.5">natural language texts</entity> e.g., <entity id="P82-1035.6">memos</entity> , rough <entity id="P82-1035.7">drafts</entity> , <entity id="P82-1035.8">conversation transcripts</entity> etc., have features that differ significantly from <entity id="P82-1035.9">neat texts</entity> , posing special problems for readers, such as <entity id="P82-1035.10">misspelled words</entity> , <entity id="P82-1035.11">missing words</entity> , <entity id="P82-1035.12">poor syntactic construction</entity> , <entity id="P82-1035.13">missing periods</entity> , etc. Our solution to these problems is to make use of <entity id="P82-1035.14">expectations</entity> , based both on knowledge of <entity id="P82-1035.15">surface English</entity> and on <entity id="P82-1035.16">world knowledge</entity> of the situation being described. These <entity id="P82-1035.17">syntactic and semantic expectations</entity> can be used to figure out <entity id="P82-1035.18">unknown words</entity> from <entity id="P82-1035.19">context</entity> , constrain the possible <entity id="P82-1035.20">word-senses</entity> of <entity id="P82-1035.21">words with multiple meanings</entity> ( <entity id="P82-1035.22">ambiguity</entity> ), fill in <entity id="P82-1035.23">missing words</entity> ( <entity id="P82-1035.24">ellipsis</entity> ), and resolve <entity id="P82-1035.25">referents</entity> ( <entity id="P82-1035.26">anaphora</entity> ). This method of using <entity id="P82-1035.27">expectations</entity> to aid the understanding of <entity id="P82-1035.28">scruffy texts</entity> has been incorporated into a working <entity id="P82-1035.29">computer program</entity> called <entity id="P82-1035.30">NOMAD</entity> , which understands <entity id="P82-1035.31">scruffy texts</entity> in the domain of Navy messages. </abstract>


</text>

<text id="P84-1020"> <title>LIMITED DOMAIN SYSTEMS FOR LANGUAGE TEACHING</title> <abstract> 
 This abstract describes a <entity id="P84-1020.1">natural language system</entity> which deals usefully with <entity id="P84-1020.2">ungrammatical input</entity> and describes some actual and potential applications of it in <entity id="P84-1020.3">computer aided second language learning</entity> . However, this is not the only area in which the principles of the system might be used, and the aim in building it was simply to demonstrate the workability of the general mechanism, and provide a framework for assessing developments of it. </abstract> 


</text>

<text id="P84-1034"> <title>A PROPER TREATMEMT OF SYNTAX AND SEMANTICS IN MACHINE TRANSLATION</title> <abstract> 
 A proper treatment of <entity id="P84-1034.1">syntax</entity> and <entity id="P84-1034.2">semantics</entity> in <entity id="P84-1034.3">machine translation</entity> is introduced and discussed from the empirical viewpoint. For <entity id="P84-1034.4">English-Japanese machine translation</entity> , the <entity id="P84-1034.5">syntax directed approach</entity> is effective where the <entity id="P84-1034.6">Heuristic Parsing Model (HPM)</entity> and the <entity id="P84-1034.7">Syntactic Role System</entity> play important roles. For <entity id="P84-1034.8">Japanese-English translation</entity> , the <entity id="P84-1034.9">semantics directed approach</entity> is powerful where the <entity id="P84-1034.10">Conceptual Dependency Diagram (CDD)</entity> and the <entity id="P84-1034.11">Augmented Case Marker System</entity> (which is a kind of <entity id="P84-1034.12">Semantic Role System</entity> ) play essential roles. Some examples of the difference between <entity id="P84-1034.13">Japanese sentence structure</entity> and <entity id="P84-1034.14">English sentence structure</entity> , which is vital to <entity id="P84-1034.15">machine translation</entity> are also discussed together with various interesting <entity id="P84-1034.16">ambiguities</entity> . </abstract> 


</text>

<text id="P84-1047"> <title>Entity-Oriented Parsing</title> <abstract> 
 An <entity id="P84-1047.1">entity-oriented approach to restricted-domain parsing</entity> is proposed. In this approach, the definitions of the <entity id="P84-1047.2">structure</entity> and <entity id="P84-1047.3">surface representation</entity> of <entity id="P84-1047.4">domain entities</entity> are grouped together. Like <entity id="P84-1047.5">semantic grammar</entity> , this allows easy exploitation of <entity id="P84-1047.6">limited domain semantics</entity> . In addition, it facilitates <entity id="P84-1047.7">fragmentary recognition</entity> and the use of <entity id="P84-1047.8">multiple parsing strategies</entity> , and so is particularly useful for robust <entity id="P84-1047.9">recognition of extra-grammatical input</entity> . Several advantages from the point of view of <entity id="P84-1047.10">language definition</entity> are also noted. Representative samples from an <entity id="P84-1047.11">entity-oriented language definition</entity> are presented, along with a <entity id="P84-1047.12">control structure</entity> for an <entity id="P84-1047.13">entity-oriented parser</entity> , some <entity id="P84-1047.14">parsing strategies</entity> that use the <entity id="P84-1047.15">control structure</entity> , and worked examples of <entity id="P84-1047.16">parses</entity> . A <entity id="P84-1047.17">parser</entity> incorporating the <entity id="P84-1047.18">control structure</entity> and the <entity id="P84-1047.19">parsing strategies</entity> is currently under <entity id="P84-1047.20">implementation</entity> . </abstract>


</text>

<text id="P84-1064"> <title>A COMPUTATIONAL THEORY OF DISPOSITIONS</title> <abstract> 
 Informally, a <entity id="P84-1064.1">disposition</entity> is a <entity id="P84-1064.2">proposition</entity> which is preponderantly, but not necessarily always, true. For example, birds can fly is a <entity id="P84-1064.3">disposition</entity> , as are the <entity id="P84-1064.4">propositions</entity> Swedes are blond and Spaniards are dark. An idea which underlies the theory described in this paper is that a <entity id="P84-1064.5">disposition</entity> may be viewed as a <entity id="P84-1064.6">proposition</entity> with implicit <entity id="P84-1064.7">fuzzy quantifiers</entity> which are approximations to all and always, e.g., almost all, almost always, most, frequently, etc. For example, birds can fly may be interpreted as the result of suppressing the <entity id="P84-1064.8">fuzzy quantifier</entity> most in the <entity id="P84-1064.9">proposition</entity> most birds can fly. Similarly, young men like young women may be read as most young men like mostly young women. The process of transforming a <entity id="P84-1064.10">disposition</entity> into a <entity id="P84-1064.11">proposition</entity> is referred to as <entity id="P84-1064.12">explicitation</entity> or <entity id="P84-1064.13">restoration</entity> . <entity id="P84-1064.14">Explicitation</entity> sets the stage for representing the <entity id="P84-1064.15">meaning</entity> of a <entity id="P84-1064.16">proposition</entity> through the use of <entity id="P84-1064.17">test-score semantics</entity> (Zadeh, 1978, 1982). In this approach to <entity id="P84-1064.18">semantics</entity> , the <entity id="P84-1064.19">meaning</entity> of a <entity id="P84-1064.20">proposition</entity> , p, is represented as a procedure which tests, scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to <entity id="P84-1064.21">reasoning with dispositions</entity> which is based on the concept of a <entity id="P84-1064.22">fuzzy syllogism</entity> . <entity id="P84-1064.23">Syllogistic reasoning with dispositions</entity> has an important bearing on <entity id="P84-1064.24">commonsense reasoning</entity> as well as on the <entity id="P84-1064.25">management of uncertainty</entity> in <entity id="P84-1064.26">expert systems</entity> . As a simple application of the techniques described in this paper, we formulate a definition of <entity id="P84-1064.27">typicality</entity> -- a concept which plays an important role in <entity id="P84-1064.28">human cognition</entity> and is of relevance to <entity id="P84-1064.29">default reasoning</entity> . </abstract>


</text>

<text id="P84-1078"> <title>Controlling Lexical Substitution in Computer Text Generation</title> <abstract> 
 This report describes <entity id="P84-1078.1">Paul</entity> , a <entity id="P84-1078.2">computer text generation system</entity> designed to create <entity id="P84-1078.3">cohesive text</entity> through the use of <entity id="P84-1078.4">lexical substitutions</entity> . Specifically, this system is designed to deterministically choose between <entity id="P84-1078.5">pronominalization</entity> , <entity id="P84-1078.6">superordinate substitution</entity> , and definite <entity id="P84-1078.7">noun phrase reiteration</entity> . The system identifies a strength of <entity id="P84-1078.8">antecedence recovery</entity> for each of the <entity id="P84-1078.9">lexical substitutions</entity> , and matches them against the <entity id="P84-1078.10">strength of potential antecedence</entity> of each element in the <entity id="P84-1078.11">text</entity> to select the proper <entity id="P84-1078.12">substitutions</entity> for these elements. </abstract>


</text>

<text id="C86-1081"> <title>A LOGICAL FORMALISM FOR THE REPRESENTATION OF DETERMINERS</title> <abstract> 
 <entity id="C86-1081.1">Determiners</entity> play an important role in conveying the <entity id="C86-1081.2">meaning</entity> of an <entity id="C86-1081.3">utterance</entity> , but they have often been disregarded, perhaps because it seemed more important to devise methods to grasp the <entity id="C86-1081.4">global meaning </entity> of a <entity id="C86-1081.5">sentence</entity> , even if not in a precise way. Another problem with <entity id="C86-1081.6">determiners</entity> is their inherent <entity id="C86-1081.7">ambiguity</entity> . In this paper we propose a <entity id="C86-1081.8">logical formalism</entity> , which, among other things, is suitable for representing <entity id="C86-1081.9">determiners</entity> without forcing a particular <entity id="C86-1081.10">interpretation</entity> when their <entity id="C86-1081.11">meaning</entity> is still not clear. </abstract>


</text>

<text id="C86-1132"> <title>SYNTHESIZING WEATHER FORECASTS FROM FORMATFED DATA</title> <abstract> 
 This paper describes a system ( <entity id="C86-1132.1">RAREAS</entity> ) which synthesizes marine weather forecasts directly from <entity id="C86-1132.2">formatted weather data</entity> . Such <entity id="C86-1132.3">synthesis</entity> appears feasible in certain <entity id="C86-1132.4">natural sublanguages</entity> with <entity id="C86-1132.5">stereotyped text structure</entity> . <entity id="C86-1132.6">RAREAS</entity> draws on several kinds of <entity id="C86-1132.7">linguistic and non-linguistic knowledge</entity> and mirrors a forecaster&apos;s apparent tendency to ascribe less precise <entity id="C86-1132.8">temporal adverbs</entity> to more remote meteorological events. The approach can easily be adapted to synthesize <entity id="C86-1132.9">bilingual or multi-lingual texts</entity> . </abstract>


</text>

<text id="J86-1002"> <title>THE CORRECTION OF ILL-FORMED INPUT USING HISTORY-BASED EXPECTATION WITH APPLICATIONS TO SPEECH UNDERSTANDING</title> <abstract> 
 A method for <entity id="J86-1002.1">error correction</entity> of <entity id="J86-1002.2">ill-formed input</entity> is described that acquires <entity id="J86-1002.3">dialogue patterns</entity> in typical usage and uses these <entity id="J86-1002.4">patterns</entity> to predict new inputs. <entity id="J86-1002.5">Error correction</entity> is done by strongly biasing <entity id="J86-1002.6">parsing</entity> toward expected <entity id="J86-1002.7">meanings</entity> unless clear evidence from the input shows the current <entity id="J86-1002.8">sentence</entity> is not expected. A <entity id="J86-1002.9">dialogue acquisition and tracking algorithm</entity> is presented along with a description of its <entity id="J86-1002.10">implementation</entity> in a <entity id="J86-1002.11">voice interactive system</entity> . A series of tests are described that show the power of the <entity id="J86-1002.12">error correction methodology</entity> when <entity id="J86-1002.13">stereotypic dialogue</entity> occurs. </abstract> 


</text>

<text id="J86-3001"> <title>Attention, Intentions, And The Structure Of Discourse</title> <abstract> 
 In this paper we explore a new <entity id="J86-3001.1">theory of discourse structure</entity> that stresses the role of <entity id="J86-3001.2">purpose</entity> and <entity id="J86-3001.3">processing</entity> in <entity id="J86-3001.4">discourse</entity> . In this theory, <entity id="J86-3001.5">discourse structure</entity> is composed of three separate but interrelated components: the structure of the sequence of <entity id="J86-3001.6">utterances</entity> (called the <entity id="J86-3001.7">linguistic structure</entity> ), a structure of <entity id="J86-3001.8">purposes</entity> (called the <entity id="J86-3001.9">intentional structure</entity> ), and the state of <entity id="J86-3001.10">focus of attention</entity> (called the <entity id="J86-3001.11">attentional state</entity> ). The <entity id="J86-3001.12">linguistic structure</entity> consists of segments of the <entity id="J86-3001.13">discourse</entity> into which the <entity id="J86-3001.14">utterances</entity> naturally aggregate. The <entity id="J86-3001.15">intentional structure</entity> captures the <entity id="J86-3001.16">discourse-relevant purposes</entity> , expressed in each of the <entity id="J86-3001.17">linguistic segments</entity> as well as relationships among them. The <entity id="J86-3001.18">attentional state</entity> is an abstraction of the <entity id="J86-3001.19">focus of attention</entity> of the <entity id="J86-3001.20">participants</entity> as the <entity id="J86-3001.21">discourse</entity> unfolds. The <entity id="J86-3001.22">attentional state</entity> , being dynamic, records the objects, properties, and relations that are salient at each point of the <entity id="J86-3001.23">discourse</entity> . The distinction among these components is essential to provide an adequate explanation of such <entity id="J86-3001.24">discourse phenomena</entity> as <entity id="J86-3001.25">cue phrases</entity> , <entity id="J86-3001.26">referring expressions</entity> , and <entity id="J86-3001.27">interruptions</entity> . The <entity id="J86-3001.28">theory of attention, intention, and aggregation of utterances</entity> is illustrated in the paper with a number of example <entity id="J86-3001.29">discourses</entity> . Various properties of <entity id="J86-3001.30">discourse</entity> are described, and explanations for the behaviour of <entity id="J86-3001.31">cue phrases</entity> , <entity id="J86-3001.32">referring expressions</entity> , and <entity id="J86-3001.33">interruptions</entity> are explored. This <entity id="J86-3001.34">theory</entity> provides a framework for describing the processing of <entity id="J86-3001.35">utterances</entity> in a <entity id="J86-3001.36">discourse</entity> . <entity id="J86-3001.37">Discourse processing</entity> requires recognizing how the <entity id="J86-3001.38">utterances</entity> of the <entity id="J86-3001.39">discourse</entity> aggregate into <entity id="J86-3001.40">segments</entity> , recognizing the <entity id="J86-3001.41">intentions</entity> expressed in the <entity id="J86-3001.42">discourse</entity> and the relationships among <entity id="J86-3001.43">intentions</entity> , and tracking the <entity id="J86-3001.44">discourse</entity> through the operation of the mechanisms associated with <entity id="J86-3001.45">attentional state</entity> . This processing description specifies in these <entity id="J86-3001.46">recognition tasks</entity> the role of information from the <entity id="J86-3001.47">discourse</entity> and from the <entity id="J86-3001.48">participants</entity> &apos; knowledge of the domain. </abstract>


</text>

<text id="J86-4002"> <title>REFERENCE IDENTIFICATION AND REFERENCE IDENTIFICATION FAILURES</title> <abstract> 
 The goal of this work is the enrichment of <entity id="J86-4002.1">human-machine interactions</entity> in a <entity id="J86-4002.2">natural language environment</entity> . Because a <entity id="J86-4002.3">speaker</entity> and <entity id="J86-4002.4">listener</entity> cannot be assured to have the same <entity id="J86-4002.5">beliefs</entity> , <entity id="J86-4002.6">contexts</entity> , <entity id="J86-4002.7">perceptions</entity> , <entity id="J86-4002.8">backgrounds</entity> , or <entity id="J86-4002.9">goals</entity> , at each point in a <entity id="J86-4002.10">conversation</entity> , difficulties and mistakes arise when a <entity id="J86-4002.11">listener</entity> interprets a <entity id="J86-4002.12">speaker&apos;s utterance</entity> . These mistakes can lead to various kinds of misunderstandings between <entity id="J86-4002.13">speaker</entity> and <entity id="J86-4002.14">listener</entity> , including <entity id="J86-4002.15">reference failures</entity> or failure to understand the <entity id="J86-4002.16">speaker&apos;s intention</entity> . We call these misunderstandings <entity id="J86-4002.17">miscommunication</entity> . Such mistakes can slow, and possibly break down, <entity id="J86-4002.18">communication</entity> . Our goal is to recognize and isolate such <entity id="J86-4002.19">miscommunications</entity> and circumvent them. This paper highlights a particular class of <entity id="J86-4002.20">miscommunication</entity> --- <entity id="J86-4002.21">reference problems</entity> --- by describing a case study and techniques for avoiding <entity id="J86-4002.22">failures of reference</entity> . We want to illustrate a framework less restrictive than earlier ones by allowing a <entity id="J86-4002.23">speaker</entity> leeway in forming an <entity id="J86-4002.24">utterance</entity> about a task and in determining the conversational vehicle to deliver it. The paper also promotes a new view for <entity id="J86-4002.25">extensional reference</entity> . </abstract>


</text>

<text id="P86-1011"> <title>The Relationship Between Tree Adjoining Grammars And Head Grammars</title> <abstract> 
 We examine the relationship between the two <entity id="P86-1011.1">grammatical formalisms</entity> : <entity id="P86-1011.2">Tree Adjoining Grammars</entity> and <entity id="P86-1011.3">Head Grammars</entity> . We briefly investigate the weak <entity id="P86-1011.4">equivalence</entity> of the two <entity id="P86-1011.5">formalisms</entity> . We then turn to a discussion comparing the <entity id="P86-1011.6">linguistic expressiveness</entity> of the two <entity id="P86-1011.7">formalisms</entity> . </abstract>


</text>

<text id="P86-1038"> <title>A LOGICAL SEMANTICS FOR FEATURE STRUCTURES</title> <abstract> 
 <entity id="P86-1038.1">Unification-based grammar formalisms</entity> use structures containing sets of <entity id="P86-1038.2">features</entity> to describe <entity id="P86-1038.3">linguistic objects</entity> . Although <entity id="P86-1038.4">computational algorithms for unification of feature structures</entity> have been worked out in experimental research, these algorithms become quite complicated, and a more precise description of <entity id="P86-1038.5">feature structures</entity> is desirable. We have developed a <entity id="P86-1038.6">model</entity> in which descriptions of <entity id="P86-1038.7">feature structures</entity> can be regarded as <entity id="P86-1038.8">logical formulas</entity> , and interpreted by sets of <entity id="P86-1038.9">directed graphs</entity> which satisfy them. These <entity id="P86-1038.10">graphs</entity> are, in fact, <entity id="P86-1038.11">transition graphs</entity> for a special type of <entity id="P86-1038.12">deterministic finite automaton</entity> . This <entity id="P86-1038.13">semantics</entity> for <entity id="P86-1038.14">feature structures</entity> extends the ideas of Pereira and Shieber [11], by providing an interpretation for values which are specified by <entity id="P86-1038.15">disjunctions</entity> and <entity id="P86-1038.16">path values</entity> embedded within <entity id="P86-1038.17">disjunctions</entity> . Our interpretation differs from that of Pereira and Shieber by using a <entity id="P86-1038.18">logical model</entity> in place of a <entity id="P86-1038.19">denotational semantics</entity> . This <entity id="P86-1038.20">logical model</entity> yields a calculus of <entity id="P86-1038.21">equivalences</entity> , which can be used to simplify <entity id="P86-1038.22">formulas</entity> . <entity id="P86-1038.23">Unification</entity> is attractive, because of its generality, but it is often computationally inefficient. Our <entity id="P86-1038.24">model</entity> allows a careful examination of the <entity id="P86-1038.25">computational complexity</entity> of <entity id="P86-1038.26">unification</entity> . We have shown that the <entity id="P86-1038.27">consistency problem</entity> for <entity id="P86-1038.28">formulas</entity> with <entity id="P86-1038.29">disjunctive values</entity> is <entity id="P86-1038.30">NP-complete</entity> . To deal with this <entity id="P86-1038.31">complexity</entity> , we describe how <entity id="P86-1038.32">disjunctive</entity> values can be specified in a way which delays <entity id="P86-1038.33">expansion</entity> to <entity id="P86-1038.34">disjunctive normal form</entity> . </abstract>


</text>

<text id="A88-1001"> <title>The Multimedia Articulation of Answers in a Natural Language Database Query System</title> <abstract> 
 This paper describes a domain independent strategy for the <entity id="A88-1001.1">multimedia articulation of answers</entity> elicited by a <entity id="A88-1001.2">natural language interface</entity> to <entity id="A88-1001.3">database query applications</entity> . <entity id="A88-1001.4">Multimedia answers</entity> include <entity id="A88-1001.5">videodisc images</entity> and heuristically-produced complete <entity id="A88-1001.6">sentences</entity> in <entity id="A88-1001.7">text</entity> or <entity id="A88-1001.8">text-to-speech form</entity> . <entity id="A88-1001.9">Deictic reference</entity> and <entity id="A88-1001.10">feedback</entity> about the <entity id="A88-1001.11">discourse</entity> are enabled. The <entity id="A88-1001.12">interface</entity> thus presents the application as cooperative and conversational. </abstract>


</text>

<text id="A88-1003"> <title>An Architecture for Anaphora Resolution</title> <abstract> 
 In this paper, we describe the <entity id="A88-1003.1">pronominal anaphora resolution module</entity> of <entity id="A88-1003.2">Lucy</entity> , a portable <entity id="A88-1003.3">English understanding system</entity> . The design of this module was motivated by the observation that, although there exist many theories of <entity id="A88-1003.4">anaphora resolution</entity> , no one of these theories is complete. Thus we have implemented a <entity id="A88-1003.5">blackboard-like architecture</entity> in which individual <entity id="A88-1003.6">partial theories</entity> can be encoded as separate modules that can interact to propose candidate <entity id="A88-1003.7">antecedents</entity> and to evaluate each other&apos;s proposals. </abstract>


</text>

<text id="C88-1007"> <title>Machine Translation Using Isomorphic UCGs</title> <abstract> 
 This paper discusses the application of <entity id="C88-1007.1">Unification Categorial Grammar (UCG)</entity> to the framework of <entity id="C88-1007.2">Isomorphic Grammars</entity> for <entity id="C88-1007.3">Machine Translation</entity> pioneered by Landsbergen. The <entity id="C88-1007.4">Isomorphic Grammars approach to MT</entity> involves developing the <entity id="C88-1007.5">grammars</entity> of the <entity id="C88-1007.6">Source and Target languages</entity> in parallel, in order to ensure that <entity id="C88-1007.7">SL</entity> and <entity id="C88-1007.8">TL</entity> expressions which stand in the <entity id="C88-1007.9">translation relation</entity> have <entity id="C88-1007.10">isomorphic derivations</entity> . The principle advantage of this approach is that knowledge concerning translation equivalence of expressions may be directly exploited, obviating the need for answers to <entity id="C88-1007.11">semantic questions</entity> that we do not yet have. <entity id="C88-1007.12">Semantic</entity> and other information may still be incorporated, but as constraints on the <entity id="C88-1007.13">translation relation</entity> , not as levels of <entity id="C88-1007.14">textual representation</entity> . After introducing this approach to <entity id="C88-1007.15">MT system</entity> design, and the basics of <entity id="C88-1007.16">monolingual UCG</entity> , we will show how the two can be integrated, and present an example from an implemented <entity id="C88-1007.17">bi-directional English-Spanish fragment</entity> . Finally we will present some outstanding problems with the approach. </abstract>


</text>

<text id="C88-1044"> <title>On the Generation and Interpretation of Demonstrative Expressions</title> <abstract> 
 This paper presents necessary and sufficient conditions for the use of <entity id="C88-1044.1">demonstrative expressions</entity> in <entity id="C88-1044.2">English</entity> and discusses implications for current <entity id="C88-1044.3">discourse processing algorithms</entity> . We examine a broad range of <entity id="C88-1044.4">texts</entity> to show how the distribution of <entity id="C88-1044.5">demonstrative forms and functions</entity> is <entity id="C88-1044.6">genre dependent</entity> . This research is part of a larger study of <entity id="C88-1044.7">anaphoric expressions</entity> , the results of which will be incorporated into a <entity id="C88-1044.8">natural language generation system</entity> . </abstract>


</text>

<text id="C88-1066"> <title>Parsing with Category Coocurrence Restrictions</title> <abstract> 
 This paper summarizes the formalism of <entity id="C88-1066.1">Category Cooccurrence Restrictions (CCRs)</entity> and describes two <entity id="C88-1066.2">parsing algorithms</entity> that interpret it. <entity id="C88-1066.3">CCRs</entity> are <entity id="C88-1066.4">Boolean conditions</entity> on the cooccurrence of <entity id="C88-1066.5">categories</entity> in <entity id="C88-1066.6">local trees</entity> which allow the <entity id="C88-1066.7">statement of generalizations</entity> which cannot be captured in other current <entity id="C88-1066.8">syntax formalisms</entity> . The use of <entity id="C88-1066.9">CCRs</entity> leads to <entity id="C88-1066.10">syntactic descriptions</entity> formulated entirely with <entity id="C88-1066.11">restrictive statements</entity> . The paper shows how conventional algorithms for the analysis of <entity id="C88-1066.12">context free languages</entity> can be adapted to the <entity id="C88-1066.13">CCR formalism</entity> . Special attention is given to the part of the <entity id="C88-1066.14">parser</entity> that checks the fulfillment of <entity id="C88-1066.15">logical well-formedness conditions</entity> on <entity id="C88-1066.16">trees</entity> . </abstract>


</text>

<text id="C88-2086"> <title>Solving Some Persistent Presupposition Problems</title> <abstract> 
 Soames 1979 provides some counterexamples to the <entity id="C88-2086.1">theory of natural language presuppositions</entity> that is presented in Gazdar 1979. Soames 1982 provides a theory which explains these counterexamples. Mercer 1987 rejects the solution found in Soames 1982 leaving these counterexamples unexplained. By reappraising these insightful counterexamples, the <entity id="C88-2086.2">inferential theory for natural language presuppositions</entity> described in Mercer 1987, 1988 gives a simple and straightforward explanation for the <entity id="C88-2086.3">presuppositional nature</entity> of these <entity id="C88-2086.4">sentences</entity> . </abstract>


</text>

<text id="C88-2130"> <title>Directing the Generation of Living Space Descriptions</title> <abstract> 
 We have developed a <entity id="C88-2130.1">computational model</entity> of the process of describing the layout of an apartment or house, a much-studied <entity id="C88-2130.2">discourse task</entity> first characterized linguistically by Linde (1974). The <entity id="C88-2130.3">model</entity> is embodied in a program, <entity id="C88-2130.4">APT</entity> , that can reproduce segments of actual tape-recorded descriptions, using <entity id="C88-2130.5">organizational and discourse strategies</entity> derived through analysis of our <entity id="C88-2130.6">corpus</entity> . </abstract>


</text>

<text id="C88-2132"> <title>Island Parsing and Bidirectional Charts</title> <abstract> 
 <entity id="C88-2132.1">Chart parsing</entity> is <entity id="C88-2132.2">directional</entity> in the sense that it works from the starting point (usually the beginning of the sentence) extending its activity usually in a rightward manner. We shall introduce the concept of a <entity id="C88-2132.3">chart</entity> that works outward from <entity id="C88-2132.4">islands</entity> and makes sense of as much of the <entity id="C88-2132.5">sentence</entity> as it is actually possible, and after that will lead to predictions of missing <entity id="C88-2132.6">fragments</entity> . So, for any place where the easily identifiable <entity id="C88-2132.7">fragments</entity> occur in the <entity id="C88-2132.8">sentence</entity> , the process will extend to both the left and the right of the <entity id="C88-2132.9">islands</entity> , until possibly completely missing <entity id="C88-2132.10">fragments</entity> are reached. At that point, by virtue of the fact that both a left and a right context were found, <entity id="C88-2132.11">heuristics</entity> can be introduced that predict the nature of the missing <entity id="C88-2132.12">fragments</entity> . </abstract>


</text>

<text id="C88-2160"> <title>Interactive Translation : a new approach</title> <abstract> 
 A new approach for <entity id="C88-2160.1">Interactive Machine Translation</entity> where the <entity id="C88-2160.2">author</entity> interacts during the creation or the modification of the <entity id="C88-2160.3">document</entity> is proposed. The explanation of an <entity id="C88-2160.4">ambiguity</entity> or an error for the purposes of correction does not use any concepts of the underlying <entity id="C88-2160.5">linguistic theory</entity> : it is a reformulation of the erroneous or ambiguous <entity id="C88-2160.6">sentence</entity> . The interaction is limited to the analysis step of the <entity id="C88-2160.7">translation process</entity> . This paper presents a new <entity id="C88-2160.8">interactive disambiguation scheme</entity> based on the <entity id="C88-2160.9">paraphrasing</entity> of a <entity id="C88-2160.10">parser</entity> &apos;s multiple output. Some examples of <entity id="C88-2160.11">paraphrasing</entity> ambiguous <entity id="C88-2160.12">sentences</entity> are presented. </abstract>


</text>

<text id="C88-2162"> <title>NETL: A System for Representing and Using Real-World Knowledge</title> <abstract> 
 Computer programs so far have not fared well in <entity id="C88-2162.1">modeling language acquisition</entity> . For one thing, <entity id="C88-2162.2">learning methodology</entity> applicable in <entity id="C88-2162.3">general domains</entity> does not readily lend itself in the <entity id="C88-2162.4">linguistic domain</entity> . For another, <entity id="C88-2162.5">linguistic representation</entity> used by <entity id="C88-2162.6">language processing systems</entity> is not geared to <entity id="C88-2162.7">learning</entity> . We introduced a new <entity id="C88-2162.8">linguistic representation</entity> , the <entity id="C88-2162.9">Dynamic Hierarchical Phrasal Lexicon (DHPL)</entity> [Zernik88], to facilitate <entity id="C88-2162.10">language acquisition</entity> . From this, a <entity id="C88-2162.11">language learning model</entity> was implemented in the program <entity id="C88-2162.12">RINA</entity> , which enhances its own <entity id="C88-2162.13">lexical hierarchy</entity> by processing examples in context. We identified two tasks: First, how <entity id="C88-2162.14">linguistic concepts</entity> are acquired from <entity id="C88-2162.15">training examples</entity> and organized in a <entity id="C88-2162.16">hierarchy</entity> ; this task was discussed in previous papers [Zernik87]. Second, we show in this paper how a <entity id="C88-2162.17">lexical hierarchy</entity> is used in predicting new <entity id="C88-2162.18">linguistic concepts</entity> . Thus, a <entity id="C88-2162.19">program</entity> does not stall even in the presence of a <entity id="C88-2162.20">lexical unknown</entity> , and a <entity id="C88-2162.21">hypothesis</entity> can be produced for covering that <entity id="C88-2162.22">lexical gap</entity> . </abstract>


</text>

<text id="C88-2166"> <title>COMPLEX: A Computational Lexicon for Natural Language Systems</title> <abstract> 
 Although every <entity id="C88-2166.1">natural language system</entity> needs a <entity id="C88-2166.2">computational lexicon</entity> , each system puts different amounts and types of information into its <entity id="C88-2166.3">lexicon</entity> according to its individual needs. However, some of the information needed across systems is shared or identical information. This paper presents our experience in planning and building <entity id="C88-2166.4">COMPLEX</entity> , a <entity id="C88-2166.5">computational lexicon</entity> designed to be a repository of <entity id="C88-2166.6">shared lexical information</entity> for use by <entity id="C88-2166.7">Natural Language Processing (NLP) systems</entity> . We have drawn primarily on explicit and implicit information from <entity id="C88-2166.8">machine-readable dictionaries (MRD&apos;s)</entity> to create a <entity id="C88-2166.9">broad coverage lexicon</entity> . </abstract>


</text>

<text id="J88-3002"> <title>MODELING THE USER IN NATURAL LANGUAGE SYSTEMS</title> <abstract> 
 For <entity id="J88-3002.1">intelligent interactive systems</entity> to communicate with <entity id="J88-3002.2">humans</entity> in a natural manner, they must have knowledge about the <entity id="J88-3002.3">system users</entity> . This paper explores the role of <entity id="J88-3002.4">user modeling</entity> in such <entity id="J88-3002.5">systems</entity> . It begins with a characterization of what a <entity id="J88-3002.6">user model</entity> is and how it can be used. The types of information that a <entity id="J88-3002.7">user model</entity> may be required to keep about a <entity id="J88-3002.8">user</entity> are then identified and discussed. <entity id="J88-3002.9">User models</entity> themselves can vary greatly depending on the requirements of the situation and the implementation, so several dimensions along which they can be classified are presented. Since acquiring the knowledge for a <entity id="J88-3002.10">user model</entity> is a fundamental problem in <entity id="J88-3002.11">user modeling</entity> , a section is devoted to this topic. Next, the benefits and costs of implementing a <entity id="J88-3002.12">user modeling component</entity> for a system are weighed in light of several aspects of the <entity id="J88-3002.13">interaction requirements</entity> that may be imposed by the system. Finally, the current state of research in <entity id="J88-3002.14">user modeling</entity> is summarized, and future research topics that must be addressed in order to achieve powerful, general <entity id="J88-3002.15">user modeling systems</entity> are assessed. </abstract>


</text>

<text id="C90-1013"> <title>Generation for Dialogue Translation Using Typed Feature Structure Unification</title> <abstract> 
 This article introduces a <entity id="C90-1013.1">bidirectional grammar generation system</entity> called <entity id="C90-1013.2">feature structure-directed generation</entity> , developed for a <entity id="C90-1013.3">dialogue translation system</entity> . The system utilizes <entity id="C90-1013.4">typed feature structures</entity> to control the <entity id="C90-1013.5">top-down derivation</entity> in a declarative way. This <entity id="C90-1013.6">generation system</entity> also uses <entity id="C90-1013.7">disjunctive feature structures</entity> to reduce the number of copies of the <entity id="C90-1013.8">derivation tree</entity> . The <entity id="C90-1013.9">grammar</entity> for this <entity id="C90-1013.10">generator</entity> is designed to properly generate the <entity id="C90-1013.11">speaker&apos;s intention</entity> in a <entity id="C90-1013.12">telephone dialogue</entity> . </abstract> 


</text>

<text id="C90-2032"> <title>Sentence disambiguation by document preference sets oriented</title> <abstract> 
 This paper proposes <entity id="C90-2032.1">document oriented preference sets(DoPS)</entity> for the disambiguation of the <entity id="C90-2032.2">dependency structure</entity> of <entity id="C90-2032.3">sentences</entity> . The <entity id="C90-2032.4">DoPS system</entity> extracts preference knowledge from a <entity id="C90-2032.5">target document</entity> or other <entity id="C90-2032.6">documents</entity> automatically. <entity id="C90-2032.7">Sentence ambiguities</entity> can be resolved by using domain targeted preference knowledge without using complicated large <entity id="C90-2032.8">knowledgebases</entity> . <entity id="C90-2032.9">Implementation</entity> and <entity id="C90-2032.10">empirical results</entity> are described for the the analysis of <entity id="C90-2032.11">dependency structures</entity> of <entity id="C90-2032.12">Japanese patent claim sentences</entity> . </abstract> 


</text>

<text id="C90-3014"> <title>A phonological knowledge base system using unification-based formalism: a case study of Korean phonology</title> <abstract> 
 This paper describes the framework of a <entity id="C90-3014.1">Korean phonological knowledge base system</entity> using the <entity id="C90-3014.2">unification-based grammar formalism</entity> : <entity id="C90-3014.3">Korean Phonology Structure Grammar (KPSG)</entity> . The approach of <entity id="C90-3014.4">KPSG</entity> provides an explicit development model for constructing a computational <entity id="C90-3014.5">phonological system</entity> : <entity id="C90-3014.6">speech recognition</entity> and <entity id="C90-3014.7">synthesis system</entity> . We show that the proposed approach is more describable than other approaches such as those employing a traditional <entity id="C90-3014.8">generative phonological approach</entity> . </abstract> 


</text>

<text id="C90-3045"> <title>Synchronous Tree-Adjoining Grammars</title> <abstract> 
 The unique properties of <entity id="C90-3045.1">tree-adjoining grammars (TAG)</entity> present a challenge for the application of <entity id="C90-3045.2">TAGs</entity> beyond the limited confines of <entity id="C90-3045.3">syntax</entity> , for instance, to the task of <entity id="C90-3045.4">semantic interpretation</entity> or <entity id="C90-3045.5">automatic translation of natural language</entity> . We present a variant of <entity id="C90-3045.6">TAGs</entity> , called <entity id="C90-3045.7">synchronous TAGs</entity> , which characterize correspondences between <entity id="C90-3045.8">languages</entity> . The formalism&apos;s intended usage is to relate <entity id="C90-3045.9">expressions of natural languages</entity> to their associated <entity id="C90-3045.10">semantics</entity> represented in a <entity id="C90-3045.11">logical form language</entity> , or to their <entity id="C90-3045.12">translates</entity> in another <entity id="C90-3045.13">natural language</entity> ; in summary, we intend it to allow <entity id="C90-3045.14">TAGs</entity> to be used beyond their role in <entity id="C90-3045.15">syntax proper</entity> . We discuss the application of <entity id="C90-3045.16">synchronous TAGs</entity> to concrete examples, mentioning primarily in passing some computational issues that arise in its interpretation. </abstract> 


</text>

<text id="C90-3046"> <title>Japanese Sentence Analysis as Argumentation</title> <abstract> 
 This paper proposes that <entity id="C90-3046.1">sentence analysis</entity> should be treated as <entity id="C90-3046.2">defeasible reasoning</entity> , and presents such a treatment for <entity id="C90-3046.3">Japanese sentence analyses</entity> using an <entity id="C90-3046.4">argumentation system</entity> by Konolige, which is a <entity id="C90-3046.5">formalization</entity> of <entity id="C90-3046.6">defeasible reasoning</entity> , that includes <entity id="C90-3046.7">arguments</entity> and <entity id="C90-3046.8">defeat rules</entity> that capture <entity id="C90-3046.9">defeasibility</entity> . </abstract> 


</text>

<text id="C90-3072"> <title>Spelling-checking for Highly Inflective Languages</title> <abstract> 
 <entity id="C90-3072.1">Spelling-checkers</entity> have become an integral part of most <entity id="C90-3072.2">text processing software</entity> . From different reasons among which the speed of processing prevails they are usually based on <entity id="C90-3072.3">dictionaries of word forms</entity> instead of <entity id="C90-3072.4">words</entity> . This approach is sufficient for languages with little <entity id="C90-3072.5">inflection</entity> such as <entity id="C90-3072.6">English</entity> , but fails for <entity id="C90-3072.7">highly inflective languages</entity> such as <entity id="C90-3072.8">Czech</entity> , <entity id="C90-3072.9">Russian</entity> , <entity id="C90-3072.10">Slovak</entity> or other <entity id="C90-3072.11">Slavonic languages</entity> . We have developed a special method for describing <entity id="C90-3072.12">inflection</entity> for the purpose of building <entity id="C90-3072.13">spelling-checkers</entity> for such languages. The speed of the resulting program lies somewhere in the middle of the scale of existing <entity id="C90-3072.14">spelling-checkers</entity> for <entity id="C90-3072.15">English</entity> and the main <entity id="C90-3072.16">dictionary</entity> fits into the standard <entity id="C90-3072.17">360K floppy</entity> , whereas the number of recognized <entity id="C90-3072.18">word forms</entity> exceeds 6 million (for <entity id="C90-3072.19">Czech</entity> ). Further, a special method has been developed for easy <entity id="C90-3072.20">word classification</entity> . </abstract> 


</text>

<text id="H90-1016"> <title>Toward a Real-Time Spoken Language System Using Commercial Hardware</title> <abstract> 
 We describe the methods and hardware that we are using to produce a real-time demonstration of an <entity id="H90-1016.1">integrated Spoken Language System</entity> . We describe algorithms that greatly reduce the computation needed to compute the <entity id="H90-1016.2">N-Best sentence hypotheses</entity> . To avoid <entity id="H90-1016.3">grammar coverage problems</entity> we use a <entity id="H90-1016.4">fully-connected first-order statistical class grammar</entity> . The <entity id="H90-1016.5">speech-search algorithm</entity> is implemented on a <entity id="H90-1016.6">board</entity> with a single <entity id="H90-1016.7">Intel i860 chip</entity> , which provides a factor of 5 speed-up over a <entity id="H90-1016.8">SUN 4</entity> for <entity id="H90-1016.9">straight C code</entity> . The <entity id="H90-1016.10">board</entity> plugs directly into the <entity id="H90-1016.11">VME bus</entity> of the <entity id="H90-1016.12">SUN4</entity> , which controls the system and contains the <entity id="H90-1016.13">natural language system</entity> and <entity id="H90-1016.14">application back end</entity> . </abstract> 


</text>

<text id="H90-1060"> <title>A New Paradigm for Speaker-Independent Training and Speaker Adaptation</title> <abstract> 
 This paper reports on two contributions to <entity id="H90-1060.1">large vocabulary continuous speech recognition</entity> . First, we present a new paradigm for <entity id="H90-1060.2">speaker-independent (SI) training</entity> of <entity id="H90-1060.3">hidden Markov models (HMM)</entity> , which uses a large amount of <entity id="H90-1060.4">speech</entity> from a few <entity id="H90-1060.5">speakers</entity> instead of the traditional practice of using a little <entity id="H90-1060.6">speech</entity> from many <entity id="H90-1060.7">speakers</entity> . In addition, combination of the <entity id="H90-1060.8">training speakers</entity> is done by averaging the <entity id="H90-1060.9">statistics</entity> of <entity id="H90-1060.10">independently trained models</entity> rather than the usual pooling of all the <entity id="H90-1060.11">speech data</entity> from many <entity id="H90-1060.12">speakers</entity> prior to <entity id="H90-1060.13">training</entity> . With only 12 <entity id="H90-1060.14">training speakers</entity> for <entity id="H90-1060.15">SI recognition</entity> , we achieved a 7.5% <entity id="H90-1060.16">word error rate</entity> on a standard <entity id="H90-1060.17">grammar</entity> and <entity id="H90-1060.18">test set</entity> from the <entity id="H90-1060.19">DARPA Resource Management corpus</entity> . This <entity id="H90-1060.20">performance</entity> is comparable to our best condition for this test suite, using 109 <entity id="H90-1060.21">training speakers</entity> . Second, we show a significant improvement for <entity id="H90-1060.22">speaker adaptation (SA)</entity> using the new <entity id="H90-1060.23">SI corpus</entity> and a small amount of <entity id="H90-1060.24">speech</entity> from the new (target) <entity id="H90-1060.25">speaker</entity> . A <entity id="H90-1060.26">probabilistic spectral mapping</entity> is estimated independently for each <entity id="H90-1060.27">training (reference) speaker</entity> and the <entity id="H90-1060.28">target speaker</entity> . Each <entity id="H90-1060.29">reference model</entity> is transformed to the <entity id="H90-1060.30">space</entity> of the <entity id="H90-1060.31">target speaker</entity> and combined by <entity id="H90-1060.32">averaging</entity> . Using only 40 <entity id="H90-1060.33">utterances</entity> from the <entity id="H90-1060.34">target speaker</entity> for <entity id="H90-1060.35">adaptation</entity> , the <entity id="H90-1060.36">error rate</entity> dropped to 4.1% --- a 45% reduction in error compared to the <entity id="H90-1060.37">SI</entity> result. </abstract> 


</text>

<text id="J90-3002"> <title>AN EDITOR FOR THE EXPLANATORY AND COMBINATORY DICTIONARY OF CONTEMPORARY FRENCH (DECFC)</title> <abstract> 
 This paper presents a specialized <entity id="J90-3002.1">editor</entity> for a highly structured <entity id="J90-3002.2">dictionary</entity> . The basic goal in building that <entity id="J90-3002.3">editor</entity> was to provide an adequate tool to help <entity id="J90-3002.4">lexicologists</entity> produce a valid and coherent <entity id="J90-3002.5">dictionary</entity> on the basis of a <entity id="J90-3002.6">linguistic theory</entity> . If we want valuable <entity id="J90-3002.7">lexicons</entity> and <entity id="J90-3002.8">grammars</entity> to achieve complex <entity id="J90-3002.9">natural language processing</entity> , we must provide very powerful tools to help create and ensure the validity of such complex <entity id="J90-3002.10">linguistic databases</entity> . Our most important task in building the <entity id="J90-3002.11">editor</entity> was to define a set of <entity id="J90-3002.12">coherence rules</entity> that could be computationally applied to ensure the validity of <entity id="J90-3002.13">lexical entries</entity> . A customized <entity id="J90-3002.14">interface</entity> for browsing and editing was also designed and implemented. </abstract>


</text>

<text id="P90-1014"> <title>Free Indexation: Combinatorial Analysis and A Compositional Algorithm</title> <abstract> 
 The principle known as <entity id="P90-1014.1">free indexation</entity> plays an important role in the determination of the <entity id="P90-1014.2">referential properties of noun phrases</entity> in the <entity id="P90-1014.3">principle-and-parameters language framework</entity> . First, by investigating the combinatorics of <entity id="P90-1014.4">free indexation</entity> , we show that the problem of enumerating all possible <entity id="P90-1014.5">indexings</entity> requires <entity id="P90-1014.6">exponential time</entity> . Secondly, we exhibit a provably optimal <entity id="P90-1014.7">free indexation algorithm</entity> . </abstract> 


</text>

<text id="A92-1026"> <title>Robust Processing of Real-World Natural-Language Texts</title> <abstract> 
 It is often assumed that when <entity id="A92-1026.1">natural language processing</entity> meets the real world, the ideal of aiming for complete and correct interpretations has to be abandoned. However, our experience with <entity id="A92-1026.2">TACITUS</entity> ; especially in the <entity id="A92-1026.3">MUC-3 evaluation</entity> , has shown that principled techniques for <entity id="A92-1026.4">syntactic and pragmatic analysis</entity> can be bolstered with methods for achieving robustness. We describe three techniques for making <entity id="A92-1026.5">syntactic analysis</entity> more robust---an <entity id="A92-1026.6">agenda-based scheduling parser</entity> , a <entity id="A92-1026.7">recovery technique for failed parses</entity> , and a new technique called <entity id="A92-1026.8">terminal substring parsing</entity> . For <entity id="A92-1026.9">pragmatics processing</entity> , we describe how the method of <entity id="A92-1026.10">abductive inference</entity> is inherently robust, in that an interpretation is always possible, so that in the absence of the required <entity id="A92-1026.11">world knowledge</entity> , performance degrades gracefully. Each of these techniques have been evaluated and the results of the evaluations are presented. </abstract> 


</text>

<text id="A92-1027"> <title>An Efficient Chart-based Algorithm for Partial-Parsing of Unrestricted Texts</title> <abstract> 
 We present an efficient algorithm for <entity id="A92-1027.1">chart-based phrase structure parsing</entity> of <entity id="A92-1027.2">natural language</entity> that is tailored to the problem of extracting specific information from <entity id="A92-1027.3">unrestricted texts</entity> where many of the <entity id="A92-1027.4">words</entity> are unknown and much of the <entity id="A92-1027.5">text</entity> is irrelevant to the task. The <entity id="A92-1027.6">parser</entity> gains algorithmic efficiency through a <entity id="A92-1027.7">reduction</entity> of its <entity id="A92-1027.8">search space</entity> . As each new <entity id="A92-1027.9">edge</entity> is added to the <entity id="A92-1027.10">chart</entity> , the algorithm checks only the topmost of the <entity id="A92-1027.11">edges</entity> adjacent to it, rather than all such <entity id="A92-1027.12">edges</entity> as in conventional treatments. The resulting <entity id="A92-1027.13">spanning edges</entity> are insured to be the correct ones by carefully controlling the order in which <entity id="A92-1027.14">edges</entity> are introduced so that every final <entity id="A92-1027.15">constituent</entity> covers the longest possible <entity id="A92-1027.16">span</entity> . This is facilitated through the use of <entity id="A92-1027.17">phrase boundary heuristics</entity> based on the placement of <entity id="A92-1027.18">function words</entity> , and by <entity id="A92-1027.19">heuristic rules</entity> that permit certain kinds of <entity id="A92-1027.20">phrases</entity> to be deduced despite the presence of <entity id="A92-1027.21">unknown words</entity> . A further <entity id="A92-1027.22">reduction in the search space</entity> is achieved by using <entity id="A92-1027.23">semantic</entity> rather than <entity id="A92-1027.24">syntactic categories</entity> on the <entity id="A92-1027.25">terminal and non-terminal edges</entity> , thereby reducing the amount of <entity id="A92-1027.26">ambiguity</entity> and thus the number of <entity id="A92-1027.27">edges</entity> , since only <entity id="A92-1027.28">edges</entity> with a valid <entity id="A92-1027.29">semantic</entity> interpretation are ever introduced. </abstract> 


</text>

<text id="C92-1052"> <title>Temporal Structure of Discourse</title> <abstract> 
 In this paper <entity id="C92-1052.1">discourse segments</entity> are defined and a method for <entity id="C92-1052.2">discourse segmentation</entity> primarily based on <entity id="C92-1052.3">abduction</entity> of <entity id="C92-1052.4">temporal relations</entity> between <entity id="C92-1052.5">segments</entity> is proposed. This method is precise and <entity id="C92-1052.6">computationally feasible</entity> and is supported by previous work in the area of <entity id="C92-1052.7">temporal anaphora resolution</entity> . </abstract> 


</text>

<text id="C92-1055"> <title>Syntactic Ambiguity Resolution Using A Discrimination and Robustness Oriented Adaptive Learning Algorithm</title> <abstract> 
 In this paper, a discrimination and robustness oriented <entity id="C92-1055.1">adaptive learning procedure</entity> is proposed to deal with the task of <entity id="C92-1055.2">syntactic ambiguity resolution</entity> . Owing to the problem of <entity id="C92-1055.3">insufficient training data</entity> and <entity id="C92-1055.4">approximation error</entity> introduced by the <entity id="C92-1055.5">language model</entity> , traditional <entity id="C92-1055.6">statistical approaches</entity> , which resolve <entity id="C92-1055.7">ambiguities</entity> by indirectly and implicitly using <entity id="C92-1055.8">maximum likelihood method</entity> , fail to achieve high <entity id="C92-1055.9">performance</entity> in real applications. The proposed method remedies these problems by adjusting the parameters to maximize the <entity id="C92-1055.10">accuracy rate</entity> directly. To make the proposed algorithm robust, the possible variations between the <entity id="C92-1055.11">training corpus</entity> and the real tasks are also taken into consideration by enlarging the <entity id="C92-1055.12">separation margin</entity> between the correct candidate and its competing members. Significant improvement has been observed in the test. The <entity id="C92-1055.13">accuracy rate</entity> of <entity id="C92-1055.14">syntactic disambiguation</entity> is raised from 46.0% to 60.62% by using this novel approach. </abstract> 


</text>

<text id="C92-2068"> <title>Quasi-Destructive Graph Unification with Structure-Sharing</title> <abstract> 
 <entity id="C92-2068.1">Graph unification</entity> remains the most expensive part of <entity id="C92-2068.2">unification-based grammar parsing</entity> . We focus on one speed-up element in the design of <entity id="C92-2068.3">unification algorithms</entity> : avoidance of <entity id="C92-2068.4">copying</entity> of <entity id="C92-2068.5">unmodified subgraphs</entity> . We propose a method of attaining such a design through a method of <entity id="C92-2068.6">structure-sharing</entity> which avoids <entity id="C92-2068.7">log(d) overheads</entity> often associated with <entity id="C92-2068.8">structure-sharing of graphs</entity> without any use of costly <entity id="C92-2068.9">dependency pointers</entity> . The proposed scheme eliminates <entity id="C92-2068.10">redundant copying</entity> while maintaining the <entity id="C92-2068.11">quasi-destructive scheme&apos;s ability</entity> to avoid <entity id="C92-2068.12">over copying</entity> and <entity id="C92-2068.13">early copying</entity> combined with its ability to handle <entity id="C92-2068.14">cyclic structures</entity> without algorithmic additions. </abstract> 


</text>

<text id="C92-2115"> <title>A Similarity-Driven Transfer System</title> <abstract> 
 The <entity id="C92-2115.1">transfer phase</entity> in <entity id="C92-2115.2">machine translation (MT) systems</entity> has been considered to be more complicated than <entity id="C92-2115.3">analysis</entity> and <entity id="C92-2115.4">generation</entity> , since it is inherently a conglomeration of individual <entity id="C92-2115.5">lexical rules</entity> . Currently some attempts are being made to use <entity id="C92-2115.6">case-based reasoning</entity> in <entity id="C92-2115.7">machine translation</entity> , that is, to make decisions on the basis of <entity id="C92-2115.8">translation examples</entity> at appropriate pints in <entity id="C92-2115.9">MT</entity> . This paper proposes a new type of <entity id="C92-2115.10">transfer system</entity> , called a <entity id="C92-2115.11">Similarity-driven Transfer System (SimTran)</entity> , for use in such <entity id="C92-2115.12">case-based MT (CBMT)</entity> . </abstract>


</text>

<text id="C92-3165"> <title>Interactive Speech Understanding</title> <abstract> 
 This paper introduces a robust <entity id="C92-3165.1">interactive method for speech understanding</entity> . The <entity id="C92-3165.2">generalized LR parsing</entity> is enhanced in this approach. <entity id="C92-3165.3">Parsing</entity> proceeds from left to right correcting minor errors. When a very noisy <entity id="C92-3165.4">portion</entity> is detected, the <entity id="C92-3165.5">parser</entity> skips that <entity id="C92-3165.6">portion</entity> using a fake <entity id="C92-3165.7">non-terminal symbol</entity> . The unidentified <entity id="C92-3165.8">portion</entity> is resolved by <entity id="C92-3165.9">re-utterance</entity> of that <entity id="C92-3165.10">portion</entity> which is parsed very efficiently by using the <entity id="C92-3165.11">parse record</entity> of the first <entity id="C92-3165.12">utterance</entity> . The <entity id="C92-3165.13">user</entity> does not have to speak the whole <entity id="C92-3165.14">sentence</entity> again. This method is also capable of handling <entity id="C92-3165.15">unknown words</entity> , which is important in practical systems. Detected <entity id="C92-3165.16">unknown words</entity> can be incrementally incorporated into the <entity id="C92-3165.17">dictionary</entity> after the interaction with the <entity id="C92-3165.18">user</entity> . A <entity id="C92-3165.19">pilot system</entity> has shown great effectiveness of this approach. </abstract>


</text>

<text id="C92-4199"> <title>Recognizing Unregistered Names for Mandarin Word Identification</title> <abstract> 
 <entity id="C92-4199.1">Word Identification</entity> has been an important and active issue in <entity id="C92-4199.2">Chinese Natural Language Processing</entity> . In this paper, a new mechanism, based on the concept of <entity id="C92-4199.3">sublanguage</entity> , is proposed for identifying <entity id="C92-4199.4">unknown words</entity> , especially <entity id="C92-4199.5">personal names</entity> , in <entity id="C92-4199.6">Chinese newspapers</entity> . The proposed mechanism includes <entity id="C92-4199.7">title-driven name recognition</entity> , <entity id="C92-4199.8">adaptive dynamic word formation</entity> , <entity id="C92-4199.9">identification of 2-character and 3-character Chinese names without title</entity> . We will show the experimental results for two <entity id="C92-4199.10">corpora</entity> and compare them with the results by the <entity id="C92-4199.11">NTHU&apos;s statistic-based system</entity> , the only system that we know has attacked the same problem. The experimental results have shown significant improvements over the <entity id="C92-4199.12">WI systems</entity> without the <entity id="C92-4199.13">name identification</entity> capability. </abstract>


</text>

<text id="C92-4207"> <title>Reconstructing Spatial Image from Natural Language Texts</title> <abstract> 
 This paper describes the understanding process of the <entity id="C92-4207.1">spatial descriptions</entity> in <entity id="C92-4207.2">Japanese</entity> . In order to understand the described <entity id="C92-4207.3">world</entity> , the authors try to reconstruct the geometric model of the global scene from the scenic descriptions drawing a space. It is done by an experimental <entity id="C92-4207.4">computer program</entity> <entity id="C92-4207.5">SPRINT</entity> , which takes <entity id="C92-4207.6">natural language texts</entity> and produces a <entity id="C92-4207.7">model</entity> of the described <entity id="C92-4207.8">world</entity> . To reconstruct the <entity id="C92-4207.9">model</entity> , the authors extract the <entity id="C92-4207.10">qualitative spatial constraints</entity> from the <entity id="C92-4207.11">text</entity> , and represent them as the <entity id="C92-4207.12">numerical constraints</entity> on the <entity id="C92-4207.13">spatial attributes</entity> of the <entity id="C92-4207.14">entities</entity> . This makes it possible to express the vagueness of the <entity id="C92-4207.15">spatial concepts</entity> and to derive the maximally plausible interpretation from a chunk of information accumulated as the constraints. The interpretation reflects the <entity id="C92-4207.16">temporary belief</entity> about the <entity id="C92-4207.17">world</entity> . </abstract> 


</text>

<text id="H92-1003"> <title>Multi-Site Data Collection for a Spoken Language Corpus: MADCOW</title> <abstract> 
 This paper describes a recently collected <entity id="H92-1003.1">spoken language corpus</entity> for the <entity id="H92-1003.2">ATIS (Air Travel Information System) domain</entity> . This data collection effort has been co-ordinated by <entity id="H92-1003.3">MADCOW (Multi-site ATIS Data COllection Working group)</entity> . We summarize the motivation for this effort, the goals, the implementation of a <entity id="H92-1003.4">multi-site data collection paradigm</entity> , and the accomplishments of <entity id="H92-1003.5">MADCOW</entity> in monitoring the <entity id="H92-1003.6">collection</entity> and distribution of 12,000 <entity id="H92-1003.7">utterances</entity> of <entity id="H92-1003.8">spontaneous speech</entity> from five sites for use in a <entity id="H92-1003.9">multi-site common evaluation of speech, natural language and spoken language</entity> .</abstract>


</text>

<text id="H92-1010"> <title>Spoken Language Processing in the Framework of Human-Machine Communication at LIMSI</title> <abstract> 
 The paper provides an overview of the research conducted at <entity id="H92-1010.1">LIMSI</entity> in the field of <entity id="H92-1010.2">speech processing</entity> , but also in the related areas of <entity id="H92-1010.3">Human-Machine Communication</entity> , including <entity id="H92-1010.4">Natural Language Processing</entity> , <entity id="H92-1010.5">Non Verbal and Multimodal Communication</entity> . Also presented are the commercial applications of some of the research projects. When applicable, the discussion is placed in the framework of international collaborations. </abstract>


</text>

<text id="H92-1016"> <title>The MIT ATIS System: February 1992 Progress Report</title> <abstract> 
 This paper describes the status of the <entity id="H92-1016.1">MIT ATIS system</entity> as of February 1992, focusing especially on the changes made to the <entity id="H92-1016.2">SUMMIT recognizer</entity> . These include <entity id="H92-1016.3">context-dependent phonetic modelling</entity> , the use of a <entity id="H92-1016.4">bigram language model</entity> in conjunction with a <entity id="H92-1016.5">probabilistic LR parser</entity> , and refinements made to the <entity id="H92-1016.6">lexicon</entity> . Together with the use of a larger <entity id="H92-1016.7">training set</entity> , these modifications combined to reduce the <entity id="H92-1016.8">speech recognition word and sentence error rates</entity> by a factor of 2.5 and 1.6, respectively, on the <entity id="H92-1016.9">October &apos;91 test set</entity> . The weighted error for the entire <entity id="H92-1016.10">spoken language system</entity> on the same <entity id="H92-1016.11">test set</entity> is 49.3%. Similar results were also obtained on the <entity id="H92-1016.12">February &apos;92 benchmark evaluation</entity> . </abstract>


</text>

<text id="H92-1017"> <title>Recent Improvements and Benchmark Results for Paramax ATIS System</title> <abstract> 
 This paper describes three relatively <entity id="H92-1017.1">domain-independent capabilities</entity> recently added to the <entity id="H92-1017.2">Paramax spoken language understanding system</entity> : <entity id="H92-1017.3">non-monotonic reasoning</entity> , <entity id="H92-1017.4">implicit reference resolution</entity> , and <entity id="H92-1017.5">database query paraphrase</entity> . In addition, we discuss the results of the <entity id="H92-1017.6">February 1992 ATIS benchmark tests</entity> . We describe a variation on the <entity id="H92-1017.7">standard evaluation metric</entity> which provides a more tightly controlled measure of progress. Finally, we briefly describe an experiment which we have done in extending the <entity id="H92-1017.8">n-best speech/language integration architecture</entity> to improving <entity id="H92-1017.9">OCR</entity> <entity id="H92-1017.10">accuracy</entity> . </abstract> 


</text>

<text id="H92-1026"> <title>Towards History-based Grammars: Using Richer Models for Probabilistic Parsing</title> <abstract> 
 We describe a <entity id="H92-1026.1">generative probabilistic model of natural language</entity> , which we call <entity id="H92-1026.2">HBG</entity> , that takes advantage of detailed <entity id="H92-1026.3">linguistic information</entity> to resolve <entity id="H92-1026.4">ambiguity</entity> .  <entity id="H92-1026.5">HBG</entity> incorporates <entity id="H92-1026.6">lexical, syntactic, semantic, and structural information</entity> from the <entity id="H92-1026.7">parse tree</entity> into the <entity id="H92-1026.8">disambiguation process</entity> in a novel way. We use a <entity id="H92-1026.9">corpus of bracketed sentences</entity> , called a <entity id="H92-1026.10">Treebank</entity> , in combination with <entity id="H92-1026.11">decision tree building</entity> to tease out the relevant aspects of a <entity id="H92-1026.12">parse tree</entity> that will determine the correct <entity id="H92-1026.13">parse</entity> of a <entity id="H92-1026.14">sentence</entity> . This stands in contrast to the usual approach of further <entity id="H92-1026.15">grammar</entity> tailoring via the usual <entity id="H92-1026.16">linguistic introspection</entity> in the hope of generating the correct <entity id="H92-1026.17">parse</entity> . In <entity id="H92-1026.18">head-to-head tests</entity> against one of the best existing robust <entity id="H92-1026.19">probabilistic parsing models</entity> , which we call <entity id="H92-1026.20">P-CFG</entity>, the <entity id="H92-1026.21">HBG model</entity> significantly outperforms <entity id="H92-1026.22">P-CFG</entity> , increasing the <entity id="H92-1026.23">parsing accuracy</entity> rate from 60% to 75%, a 37% reduction in error. </abstract> 


</text>

<text id="H92-1036"> <title>MAP Estimation of Continuous Density HMM: Theory and Applications</title> <abstract> 
 We discuss <entity id="H92-1036.1">maximum a posteriori estimation</entity> of <entity id="H92-1036.2">continuous density hidden Markov models (CDHMM)</entity> . The classical <entity id="H92-1036.3">MLE reestimation algorithms</entity> , namely the <entity id="H92-1036.4">forward-backward algorithm</entity> and the <entity id="H92-1036.5">segmental k-means algorithm</entity> , are expanded and <entity id="H92-1036.6">reestimation formulas</entity> are given for <entity id="H92-1036.7">HMM with Gaussian mixture observation densities</entity> . Because of its adaptive nature, <entity id="H92-1036.8">Bayesian learning</entity> serves as a unified approach for the following four <entity id="H92-1036.9">speech recognition</entity> applications, namely <entity id="H92-1036.10">parameter smoothing</entity> , <entity id="H92-1036.11">speaker adaptation</entity> , <entity id="H92-1036.12">speaker group modeling</entity> and <entity id="H92-1036.13">corrective training</entity> . New experimental results on all four applications are provided to show the effectiveness of the <entity id="H92-1036.14">MAP estimation approach</entity> . </abstract>


</text>

<text id="H92-1045"> <title>One Sense Per Discourse</title> <abstract> 
 It is well-known that there are <entity id="H92-1045.1">polysemous words</entity> like <entity id="H92-1045.2">sentence</entity> whose <entity id="H92-1045.3">meaning</entity> or <entity id="H92-1045.4">sense</entity> depends on the context of use. We have recently reported on two new <entity id="H92-1045.5">word-sense disambiguation systems</entity> , one trained on <entity id="H92-1045.6">bilingual material</entity> (the <entity id="H92-1045.7">Canadian Hansards</entity> ) and the other trained on <entity id="H92-1045.8">monolingual material</entity> ( <entity id="H92-1045.9">Roget&apos;s Thesaurus</entity> and <entity id="H92-1045.10">Grolier&apos;s Encyclopedia</entity> ). As this work was nearing completion, we observed a very strong <entity id="H92-1045.11">discourse</entity> effect. That is, if a <entity id="H92-1045.12">polysemous word</entity> such as <entity id="H92-1045.13">sentence</entity> appears two or more times in a <entity id="H92-1045.14">well-written discourse</entity> , it is extremely likely that they will all share the same <entity id="H92-1045.15">sense</entity> . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share <entity id="H92-1045.16">sense</entity> in the same <entity id="H92-1045.17">discourse</entity> is extremely strong (98%). This result can be used as an additional source of <entity id="H92-1045.18">constraint</entity> for improving the performance of the <entity id="H92-1045.19">word-sense disambiguation algorithm</entity> . In addition, it could also be used to help evaluate <entity id="H92-1045.20">disambiguation algorithms</entity> that did not make use of the <entity id="H92-1045.21">discourse constraint</entity> . </abstract> 


</text>

<text id="P06-1112">
<title>Exploring Correlation Of Dependency Relation Paths For Answer Extraction </title>
<abstract>
In this paper, we explore correlation of <entity id="P06-1112.1">dependency relation paths</entity> to rank candidate answers in <entity id="P06-1112.2">answer extraction</entity>. Using the <entity id="P06-1112.3">correlation measure</entity>, we compare <entity id="P06-1112.4">dependency relations</entity> of a candidate answer and mapped <entity id="P06-1112.5">question phrases</entity> in <entity id="P06-1112.6">sentence</entity> with the corresponding <entity id="P06-1112.7">relations</entity> in question. Different from previous studies, we propose an <entity id="P06-1112.8">approximate phrase mapping algorithm</entity> and incorporate the <entity id="P06-1112.9">mapping score</entity> into the <entity id="P06-1112.10">correlation measure</entity>. The correlations are further incorporated into a <entity id="P06-1112.11">Maximum Entropy-based ranking model</entity> which estimates <entity id="P06-1112.12">path weights</entity> from training. Experimental results show that our method significantly outperforms state-of-the-art <entity id="P06-1112.13">syntactic relation-based methods</entity> by up to 20% in <entity id="P06-1112.14">MRR</entity>.
</abstract>



</text>

<text id="C90-3063">
<title> Automatic Processing Of Large Corpora For The Resolution Of Anaphora References </title>
<abstract>
<entity id="C90-3063.1">Manual acquisition</entity> of <entity id="C90-3063.2">semantic constraints</entity> in broad domains is very expensive. This paper presents an automatic scheme for collecting statistics on <entity id="C90-3063.3">cooccurrence patterns</entity> in a large <entity id="C90-3063.4">corpus</entity>. To a large extent, these statistics reflect <entity id="C90-3063.5">semantic constraints</entity> and thus are used to disambiguate <entity id="C90-3063.6">anaphora references</entity> and <entity id="C90-3063.7">syntactic ambiguities</entity>. The scheme was implemented by gathering statistics on the output of other linguistic tools. An experiment was performed to resolve <entity id="C90-3063.8">references</entity> of the <entity id="C90-3063.9">pronoun "it"</entity> in <entity id="C90-3063.10">sentences</entity> that were randomly selected from the <entity id="C90-3063.11">corpus</entity>. The results of the experiment show that in most of the cases the <entity id="C90-3063.12">cooccurrence statistics</entity> indeed reflect the <entity id="C90-3063.13">semantic constraints</entity> and thus provide a basis for a useful <entity id="C90-3063.14">disambiguation tool</entity>.
</abstract>



</text>

<text id="C04-1011">
<title> Kullback-Leibler Distance Between Probabilistic Context-Free Grammars And Probabilistic Finite Automata </title>
<abstract>
We consider the problem of computing the <entity id="C04-1011.1">Kullback-Leibler distance</entity>, also called the <entity id="C04-1011.2">relative entropy</entity>, between a <entity id="C04-1011.3">probabilistic context-free grammar</entity> and a <entity id="C04-1011.4">probabilistic finite automaton</entity>. We show that there is a <entity id="C04-1011.5">closed-form (analytical) solution</entity> for one part of the <entity id="C04-1011.6">Kullback-Leibler distance</entity>, viz. the <entity id="C04-1011.7">cross-entropy</entity>. We discuss several applications of the result to the problem of <entity id="C04-1011.8">distributional approximation</entity> of <entity id="C04-1011.9">probabilistic context-free grammars</entity> by means of <entity id="C04-1011.10">probabilistic finite automata</entity>.
</abstract>



</text>

<text id="A94-1037">
<title> Spelling Correction In Agglutinative Languages </title>
<abstract>
Methods developed for <entity id="A94-1037.1">spelling correction</entity> for <entity id="A94-1037.2">languages</entity> like <entity id="A94-1037.3">English</entity> (see the review by Kukich (Kukich, 1992)) are not readily applicable to <entity id="A94-1037.4">agglutinative languages</entity>. This poster presents an approach to <entity id="A94-1037.5">spelling correction</entity> in <entity id="A94-1037.6">agglutinative languages</entity> that is based on <entity id="A94-1037.7">two-level morphology</entity> and a <entity id="A94-1037.8">dynamic-programming based search algorithm</entity>. After an overview of our approach, we present results from experiments with <entity id="A94-1037.9">spelling correction</entity> in <entity id="A94-1037.10">Turkish</entity>.
</abstract>



</text>

<text id="H94-1102">
<title>Robust Continuous Speech Recognition Technology Program Summary</title>
<abstract>
The major objective of this program is to develop and demonstrate robust, high performance <entity id="H94-1102.1">continuous speech recognition (CSR) techniques</entity> focussed on application in <entity id="H94-1102.2">Spoken Language Systems (SLS)</entity> which will enhance the effectiveness of <entity id="H94-1102.3">military and civilian computer-based systems</entity>. A key complementary objective is to define and develop applications of robust <entity id="H94-1102.4">speech recognition and understanding systems</entity>, and to help catalyze the transition of <entity id="H94-1102.5">spoken language technology</entity> into <entity id="H94-1102.6">military and civilian systems</entity>, with particular focus on application of robust <entity id="H94-1102.7">CSR</entity> to <entity id="H94-1102.8">mobile military command and control</entity>. The research effort focusses on developing advanced <entity id="H94-1102.9">acoustic modelling</entity>, rapid search, and <entity id="H94-1102.10">recognition-time adaptation techniques</entity> for robust <entity id="H94-1102.11">large-vocabulary CSR</entity>, and on applying these techniques to the new <entity id="H94-1102.12">ARPA large-vocabulary CSR corpora</entity> and to military application tasks.
</abstract>



</text>

<text id="P98-1118">
<title>A Framework for Customizable Generation of Hypertext Presentations </title>
<abstract>
In this paper, we present a framework, <entity id="P98-1118.1">Presentor</entity>, for the development and customization of <entity id="P98-1118.2">hypertext presentation generators</entity>. <entity id="P98-1118.3">Presentor</entity> offers intuitive and powerful <entity id="P98-1118.4">declarative languages</entity> specifying the presentation at different levels: macro-planning, micro-planning, realization, and formatting. <entity id="P98-1118.5">Presentor</entity> is implemented and is portable cross-platform and cross-domain. It has been used with success in several application domains including weather forecasting, <entity id="P98-1118.6">object modeling</entity>, system description and <entity id="P98-1118.7">requirements summarization</entity>.
</abstract>



</text>

<text id="A92-1023">
<title>A Practical Methodology For The Evaluation Of Spoken Language Systems </title>
<abstract>
A meaningful evaluation methodology can advance the state-of-the-art by encouraging mature, practical applications rather than "toy" implementations. Evaluation is also crucial to assessing competing claims and identifying promising technical approaches. While work in <entity id="A92-1023.1">speech recognition (SR)</entity> has a history of evaluation methodologies that permit comparison among various systems, until recently no methodology existed for either developers of <entity id="A92-1023.2">natural language (NL) interfaces</entity> or researchers in <entity id="A92-1023.3">speech understanding (SU)</entity> to evaluate and compare the systems they developed. Recently considerable progress has been made by a number of groups involved in the <entity id="A92-1023.4">DARPA Spoken Language Systems (SLS) program</entity> to agree on a methodology for comparative evaluation of <entity id="A92-1023.5">SLS systems</entity>, and that methodology has been put into practice several times in comparative tests of several <entity id="A92-1023.6">SLS systems</entity>. These evaluations are probably the only <entity id="A92-1023.7">NL evaluations</entity> other than the series of <entity id="A92-1023.8">Message Understanding Conferences</entity> (Sundheim, 1989; Sundheim, 1991) to have been developed and used by a group of researchers at different sites, although several excellent workshops have been held to study some of these problems (Palmer et al., 1989; Neal et al., 1991). This paper describes a practical <entity id="A92-1023.9">"black-box" methodology</entity> for automatic evaluation of <entity id="A92-1023.10">question-answering NL systems</entity>. While each new application domain will require some development of special resources, the heart of the methodology is domain-independent, and it can be used with either <entity id="A92-1023.11">speech or text input</entity>. The particular characteristics of the approach are described in the following section: subsequent sections present its implementation in the <entity id="A92-1023.12">DARPA SLS community</entity>, and some problems and directions for future development.
</abstract>



</text>

<text id="P06-1053">
<title>Integrating Syntactic Priming Into An Incremental Probabilistic Parser, With An Application To Psycholinguistic Modeling </title>
<abstract>
The <entity id="P06-1053.1">psycholinguistic literature</entity> provides evidence for <entity id="P06-1053.2">syntactic priming</entity>, i.e., the tendency to repeat structures. This paper describes a method for incorporating <entity id="P06-1053.3">priming</entity> into an <entity id="P06-1053.4">incremental probabilistic parser</entity>. Three models are compared, which involve <entity id="P06-1053.5">priming</entity> of <entity id="P06-1053.6">rules</entity> between <entity id="P06-1053.7">sentences</entity>, within <entity id="P06-1053.8">sentences</entity>, and within <entity id="P06-1053.9">coordinate structures</entity>. These models simulate the reading time advantage for <entity id="P06-1053.10">parallel structures</entity> found in <entity id="P06-1053.11">human data</entity>, and also yield a small increase in overall <entity id="P06-1053.12">parsing accuracy</entity>.
</abstract>



</text>

<text id="P06-1012">
<title>
Estimating Class Priors In Domain Adaptation For Word Sense Disambiguation </title>
<abstract>
Instances of a <entity id="P06-1012.1">word</entity> drawn from different <entity id="P06-1012.2">domains</entity> may have different <entity id="P06-1012.3">sense priors</entity> (the proportions of the different <entity id="P06-1012.4">senses</entity> of a <entity id="P06-1012.5">word</entity>). This in turn affects the accuracy of <entity id="P06-1012.6">word sense disambiguation (WSD) systems</entity> trained and applied on different <entity id="P06-1012.7">domains</entity>. This paper presents a method to estimate the <entity id="P06-1012.8">sense priors</entity> of <entity id="P06-1012.9">words</entity> drawn from a new <entity id="P06-1012.10">domain</entity>, and highlights the importance of using <entity id="P06-1012.11">well calibrated probabilities</entity> when performing these <entity id="P06-1012.12">estimations</entity>. By using <entity id="P06-1012.13">well calibrated probabilities</entity>, we are able to estimate the <entity id="P06-1012.14">sense priors</entity> effectively to achieve significant improvements in <entity id="P06-1012.15">WSD accuracy</entity>.
</abstract>



</text>

<text id="C86-1105">
<title>An Attempt To Automatic Thesaurus Construction From An Ordinary Japanese Language Dictionary </title>
<abstract>
How to obtain <entity id="C86-1105.1">hierarchical relations</entity>(e.g. <entity id="C86-1105.2">superordinate -hyponym relation</entity>, <entity id="C86-1105.3">synonym relation</entity>) is one of the most important problems for <entity id="C86-1105.4">thesaurus construction</entity>. A pilot system for extracting these <entity id="C86-1105.5">relations</entity> automatically from an ordinary <entity id="C86-1105.6">Japanese language dictionary</entity> (Shinmeikai Kokugojiten, published by Sansei-do, in machine readable form) is given. The features of the <entity id="C86-1105.7">definition sentences</entity> in the <entity id="C86-1105.8">dictionary</entity>, the mechanical extraction of the <entity id="C86-1105.9">hierarchical relations</entity> and the estimation of the results are discussed.
</abstract>



</text>

<text id="C86-1021">
<title>The Transfer Phase Of Mu Machine Translation System</title>
<abstract>
The <entity id="C86-1021.1">interlingual approach to MT</entity> has been repeatedly advocated by researchers originally interested in <entity id="C86-1021.2">natural language understanding</entity> who take <entity id="C86-1021.3">machine translation</entity> to be one possible application. However, not only the <entity id="C86-1021.4">ambiguity</entity> but also the vagueness which every <entity id="C86-1021.5">natural language</entity> inevitably has leads this approach into essential difficulties. In contrast, our project, the <entity id="C86-1021.6">Mu-project</entity>, adopts the <entity id="C86-1021.7">transfer approach</entity> as the basic framework of <entity id="C86-1021.8">MT</entity>. This paper describes the detailed construction of the <entity id="C86-1021.9">transfer phase</entity> of our system from <entity id="C86-1021.10">Japanese</entity> to <entity id="C86-1021.11">English</entity>, and gives some examples of problems which seem difficult to treat in the <entity id="C86-1021.12">interlingual approach</entity>. The basic design principles of the <entity id="C86-1021.13">transfer phase</entity> of our system have already been mentioned in (1) (2). Some of the principles which are relevant to the topic of this paper are: (a) <entity id="C86-1021.14">Multiple Layer of Grammars</entity> (b) <entity id="C86-1021.15">Multiple Layer Presentation</entity> (c) <entity id="C86-1021.16">Lexicon Driven Processing</entity> (d) <entity id="C86-1021.17">Form-Oriented Dictionary Description</entity>. This paper also shows how these principles are realized in the current system.
</abstract>



</text>

<text id="C04-1058">
<title>
Why Nitpicking Works: Evidence For Occam's Razor In Error Correctors 
</title>
<abstract>
Empirical experience and observations have shown us when powerful and highly tunable <entity id="C04-1058.1">classifiers</entity> such as <entity id="C04-1058.2">maximum entropy classifiers</entity>, <entity id="C04-1058.3">boosting</entity> and <entity id="C04-1058.4">SVMs</entity> are applied to <entity id="C04-1058.5">language processing tasks</entity>, it is possible to achieve high accuracies, but eventually their performances all tend to plateau out at around the same point. To further improve performance, various <entity id="C04-1058.6">error correction mechanisms</entity> have been developed, but in practice, most of them cannot be relied on to predictably improve performance on <entity id="C04-1058.7">unseen data</entity>; indeed, depending upon the <entity id="C04-1058.8">test set</entity>, they are as likely to degrade accuracy as to improve it. This problem is especially severe if the <entity id="C04-1058.9">base classifier</entity> has already been finely tuned. In recent work, we introduced <entity id="C04-1058.10">N-fold Templated Piped Correction, or NTPC ("nitpick")</entity>, an intriguing <entity id="C04-1058.11">error corrector</entity> that is designed to work in these extreme operating conditions. Despite its simplicity, it consistently and robustly improves the accuracy of existing highly accurate <entity id="C04-1058.12">base models</entity>. This paper investigates some of the more surprising claims made by <entity id="C04-1058.13">NTPC</entity>, and presents experiments supporting an <entity id="C04-1058.14">Occam's Razor argument</entity> that more complex models are damaging or unnecessary in practice.
</abstract>



</text>

<text id="N06-1007">
<title> Acquisition Of Verb Entailment From Text </title>
<abstract>
The study addresses the problem of <entity id="N06-1007.1">automatic acquisition</entity> of <entity id="N06-1007.2">entailment relations</entity> between <entity id="N06-1007.3">verbs</entity>. While this task has much in common with <entity id="N06-1007.4">paraphrases acquisition</entity> which aims to discover <entity id="N06-1007.5">semantic equivalence</entity> between <entity id="N06-1007.6">verbs</entity>, the main challenge of <entity id="N06-1007.7">entailment acquisition</entity> is to capture <entity id="N06-1007.8">asymmetric, or directional, relations</entity>. Motivated by the intuition that it often underlies the <entity id="N06-1007.9">local structure</entity> of <entity id="N06-1007.10">coherent text</entity>, we develop a method that discovers <entity id="N06-1007.11">verb entailment</entity> using evidence about <entity id="N06-1007.12">discourse relations</entity> between <entity id="N06-1007.13">clauses</entity> available in a <entity id="N06-1007.14">parsed corpus</entity>. In comparison with earlier work, the proposed method covers a much wider range of <entity id="N06-1007.15">verb entailment types</entity> and learns the <entity id="N06-1007.16">mapping</entity> between <entity id="N06-1007.17">verbs</entity> with highly varied <entity id="N06-1007.18">argument structures</entity>.
</abstract>



</text>

<text id="A00-2023">
<title> Forest-Based Statistical Sentence Generation </title>
<abstract>
This paper presents a new approach to <entity id="A00-2023.1">statistical sentence generation</entity> in which alternative <entity id="A00-2023.2">phrases</entity> are represented as packed sets of <entity id="A00-2023.3">trees</entity>, or <entity id="A00-2023.4">forests</entity>, and then ranked statistically to choose the best one. This representation offers advantages in compactness and in the ability to represent <entity id="A00-2023.5">syntactic information</entity>. It also facilitates more efficient <entity id="A00-2023.6">statistical ranking</entity> than a previous approach to <entity id="A00-2023.7">statistical generation</entity>. An efficient <entity id="A00-2023.8">ranking algorithm</entity> is described, together with experimental results showing significant improvements over simple enumeration or a <entity id="A00-2023.9">lattice-based approach</entity>.
</abstract>



</text>

<text id="X98-1022">
<title>An NTU-Approach To Automatic Sentence Extraction For Summary Generation </title>

<abstract>
<entity id="X98-1022.1">Automatic summarization</entity> and <entity id="X98-1022.2">information extraction</entity> are two important Internet services. <entity id="X98-1022.3">MUC</entity> and <entity id="X98-1022.4">SUMMAC</entity> play their appropriate roles in the next generation Internet. This paper focuses on the <entity id="X98-1022.5">automatic summarization</entity> and proposes two different models to extract <entity id="X98-1022.6">sentences</entity> for <entity id="X98-1022.7">summary generation</entity> under two tasks initiated by <entity id="X98-1022.8">SUMMAC-1</entity>. For <entity id="X98-1022.9">categorization task</entity>, <entity id="X98-1022.10">positive feature vectors</entity> and <entity id="X98-1022.11">negative feature vectors</entity> are used cooperatively to construct generic, indicative <entity id="X98-1022.12">summaries</entity>. For adhoc task, a <entity id="X98-1022.13">text model</entity> based on relationship between <entity id="X98-1022.14">nouns</entity> and <entity id="X98-1022.15">verbs</entity> is used to filter out irrelevant <entity id="X98-1022.16">discourse segment</entity>, to rank relevant <entity id="X98-1022.17">sentences</entity>, and to generate the <entity id="X98-1022.18">user-directed summaries</entity>. The result shows that the <entity id="X98-1022.19">NormF</entity> of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0. 447. The <entity id="X98-1022.20">NormF</entity> of the best summary and that of the fixed summary for <entity id="X98-1022.21">categorization task</entity> are 0.4090 and 0.4023. Our system outperforms the average system in <entity id="X98-1022.22">categorization task</entity> but does a common job in adhoc task.
</abstract>



</text>

<text id="P98-2213">
<title>
A Method for Relating Multiple Newspaper Articles by Using Graphs, and Its Application to Webcasting 
</title>
<abstract>
This paper describes methods for relating (threading) multiple newspaper articles, and for visualizing various characteristics of them by using a <entity id="P98-2213.1">directed graph</entity>. A set of articles is represented by a set of <entity id="P98-2213.2">word vectors</entity>, and the <entity id="P98-2213.3">similarity</entity> between the <entity id="P98-2213.4">vectors</entity> is then calculated. The <entity id="P98-2213.5">graph</entity> is constructed from the <entity id="P98-2213.6">similarity matrix</entity>. By applying some <entity id="P98-2213.7">constraints</entity> on the chronological ordering of articles, an efficient <entity id="P98-2213.8">threading algorithm</entity> that runs in <entity id="P98-2213.9">0(n) time</entity> (where n is the number of articles) is obtained. The constructed <entity id="P98-2213.10">graph</entity> is visualized with <entity id="P98-2213.11">words</entity> that represent the <entity id="P98-2213.12">topics</entity> of the <entity id="P98-2213.13">threads</entity>, and <entity id="P98-2213.14">words</entity> that represent new <entity id="P98-2213.15">information</entity> in each article. The <entity id="P98-2213.16">threading technique</entity> is suitable for Webcasting (push) applications. A <entity id="P98-2213.17">threading server</entity> determines relationships among articles from various news sources, and creates files containing their <entity id="P98-2213.18">threading information</entity>. This information is represented in <entity id="P98-2213.19">eXtended Markup Language (XML)</entity>, and can be visualized on most Web browsers. The <entity id="P98-2213.20">XML-based representation</entity> and a current prototype are described in this paper.
</abstract>



</text>

<text id="P98-1113">
<title>
A Flexible Example-Based Parser Based on the SSTC
</title>
<abstract>

In this paper we sketch an approach for <entity id="P98-1113.1">Natural Language parsing</entity>. Our approach is an <entity id="P98-1113.2">example-based approach</entity>, which relies mainly on examples that already parsed to their <entity id="P98-1113.3">representation structure</entity>, and on the knowledge that we can get from these examples the required information to parse a new <entity id="P98-1113.4">input sentence</entity>. In our approach, examples are annotated with the <entity id="P98-1113.5">Structured String Tree Correspondence (SSTC) annotation schema</entity> where each <entity id="P98-1113.6">SSTC</entity> describes a <entity id="P98-1113.7">sentence</entity>, a <entity id="P98-1113.8">representation tree</entity> as well as the correspondence between <entity id="P98-1113.9">substrings</entity> in the <entity id="P98-1113.10">sentence</entity> and <entity id="P98-1113.11">subtrees</entity> in the <entity id="P98-1113.12">representation tree</entity>. In the process of <entity id="P98-1113.13">parsing</entity>, we first try to build <entity id="P98-1113.14">subtrees</entity> for <entity id="P98-1113.15">phrases</entity> in the <entity id="P98-1113.16">input sentence</entity> which have been successfully found in the <entity id="P98-1113.17">example-base</entity> - a bottom up approach. These <entity id="P98-1113.18">subtrees</entity> will then be combined together to form a <entity id="P98-1113.19">single rooted representation tree</entity> based on an example with similar <entity id="P98-1113.20">representation structure</entity> - a top down approach.Keywords:

</abstract>



</text>

<text id="P02-1060">
<title>Named Entity Recognition Using An HMM-Based Chunk Tagger</title>

<abstract>
This paper proposes a <entity id="P02-1060.1">Hidden Markov Model (HMM)</entity> and an <entity id="P02-1060.2">HMM-based chunk tagger</entity>, from which a <entity id="P02-1060.3">named entity (NE) recognition (NER) system</entity> is built to recognize and classify <entity id="P02-1060.4">names</entity>, <entity id="P02-1060.5">times and numerical quantities</entity>. Through the <entity id="P02-1060.6">HMM</entity>, our system is able to apply and integrate four types of internal and external evidences : 1) simple deterministic internal feature of the <entity id="P02-1060.7">words</entity>, such as <entity id="P02-1060.8">capitalization</entity> and digitalization ; 2) <entity id="P02-1060.9">internal semantic feature</entity> of important triggers ; 3) <entity id="P02-1060.10">internal gazetteer feature</entity>; 4) <entity id="P02-1060.11">external macro context feature</entity>. In this way, the <entity id="P02-1060.12">NER problem</entity> can be resolved effectively. Evaluation of our <entity id="P02-1060.13">system</entity> on <entity id="P02-1060.14">MUC-6 and MUC-7 English NE tasks</entity> achieves <entity id="P02-1060.15">F-measures</entity> of 96.6% and 94.1% respectively. It shows that the performance is significantly better than reported by any other <entity id="P02-1060.16">machine-learning system</entity>. Moreover, the <entity id="P02-1060.17">performance</entity> is even consistently better than those based on <entity id="P02-1060.18">handcrafted rules</entity>.
</abstract>



</text>

<text id="C96-2213">
<title>Using A Hybrid System Of Corpus- And Knowledge-Based Techniques To Automate The Induction Of A Lexical Sublanguage Grammar </title>
<abstract>
Porting a <entity id="C96-2213.1">Natural Language Processing (NLP) system</entity> to a <entity id="C96-2213.2">new domain</entity> remains one of the bottlenecks in <entity id="C96-2213.3">syntactic parsing</entity>, because of the amount of effort required to fix gaps in the <entity id="C96-2213.4">lexicon</entity>, and to attune the <entity id="C96-2213.5">existing grammar</entity> to the idiosyncracies of the <entity id="C96-2213.6">new sublanguage</entity>. This paper shows how the process of fitting a <entity id="C96-2213.7">lexicalized grammar</entity> to a <entity id="C96-2213.8">domain</entity> can be automated to a great extent by using a <entity id="C96-2213.9">hybrid system</entity> that combines <entity id="C96-2213.10">traditional knowledge-based techniques</entity> with a <entity id="C96-2213.11">corpus-based approach</entity>.
</abstract>



</text>

<text id="C04-1102">
<title>Detecting Transliterated Orthographic Variants Via Two Similarity Metrics </title>
<abstract>
We propose a <entity id="C04-1102.1">detection method</entity> for orthographic variants caused by <entity id="C04-1102.2">transliteration</entity> in a large <entity id="C04-1102.3">corpus</entity>. The method employs two <entity id="C04-1102.4">similarities</entity>. One is <entity id="C04-1102.5">string similarity</entity> based on <entity id="C04-1102.6">edit distance</entity>. The other is <entity id="C04-1102.7">contextual similarity</entity> by a <entity id="C04-1102.8">vector space model</entity>. Experimental results show that the method performed a 0.889 <entity id="C04-1102.9">F-measure</entity> in an open test.
</abstract>



</text>

<text id="N06-1018">
<title>Understanding Temporal Expressions In Emails </title>
<abstract>
Recent years have seen increasing research on extracting and using temporal information in <entity id="N06-1018.1">natural language applications</entity>. However most of the works found in the literature have focused on identifying and understanding <entity id="N06-1018.2">temporal expressions</entity> in <entity id="N06-1018.3">newswire texts</entity>. In this paper we report our work on anchoring <entity id="N06-1018.4">temporal expressions</entity> in a novel <entity id="N06-1018.5">genre</entity>, emails. The highly under-specified nature of these <entity id="N06-1018.6">expressions</entity> fits well with our <entity id="N06-1018.7">constraint-based representation</entity> of time, <entity id="N06-1018.8">Time Calculus for Natural Language (TCNL)</entity>. We have developed and evaluated a <entity id="N06-1018.9">Temporal Expression Anchoror (TEA)</entity>, and the result shows that it performs significantly better than the <entity id="N06-1018.10">baseline</entity>, and compares favorably with some of the closely related work.</abstract>



</text>

<text id="H89-2066">
<title>Research And Development For Spoken Language Systems </title>
<abstract>
The goal of this research is to develop a <entity id="H89-2066.1">spoken language system</entity> that will demonstrate the usefulness of <entity id="H89-2066.2">voice input</entity> for <entity id="H89-2066.3">interactive problem solving</entity>. The system will accept <entity id="H89-2066.4">continuous speech</entity>, and will handle <entity id="H89-2066.5">multiple speakers</entity> without <entity id="H89-2066.6">explicit speaker enrollment</entity>. Combining <entity id="H89-2066.7">speech recognition</entity> and <entity id="H89-2066.8">natural language processing</entity> to achieve <entity id="H89-2066.9">speech understanding</entity>, the system will be demonstrated in an <entity id="H89-2066.10">application domain</entity> relevant to the DoD. The objective of this project is to develop a <entity id="H89-2066.11">robust and high-performance speech recognition system</entity> using a <entity id="H89-2066.12">segment-based approach</entity> to <entity id="H89-2066.13">phonetic recognition</entity>. The <entity id="H89-2066.14">recognition system</entity> will eventually be integrated with <entity id="H89-2066.15">natural language processing</entity> to achieve <entity id="H89-2066.16">spoken language understanding</entity>.
</abstract>



</text>

<text id="J81-1002">
<title> Computer Generation Of Multiparagraph English Text </title>
<abstract>
This paper reports recent research into methods for <entity id="J81-1002.1">creating natural language text</entity>. A new <entity id="J81-1002.2">processing paradigm</entity> called <entity id="J81-1002.3">Fragment-and-Compose</entity> has been created and an experimental system implemented in it. The <entity id="J81-1002.4">knowledge</entity> to be expressed in <entity id="J81-1002.5">text</entity> is first divided into small <entity id="J81-1002.6">propositional units</entity>, which are then composed into appropriate combinations and converted into <entity id="J81-1002.7">text</entity>.<entity id="J81-1002.8">KDS (Knowledge Delivery System)</entity>, which embodies this paradigm, has distinct parts devoted to creation of the <entity id="J81-1002.9">propositional units</entity>, to organization of the <entity id="J81-1002.10">text</entity>, to prevention of <entity id="J81-1002.11">excess redundancy</entity>, to creation of combinations of units, to evaluation of these combinations as potential <entity id="J81-1002.12">sentences</entity>, to selection of the best among competing combinations, and to creation of the <entity id="J81-1002.13">final text</entity>. The <entity id="J81-1002.14">Fragment-and-Compose paradigm</entity> and the <entity id="J81-1002.15">computational methods</entity> of <entity id="J81-1002.16">KDS</entity> are described.
</abstract>



</text>

<text id="C90-1002">
<title> Design Of A Hybrid Deterministic Parser </title>
<abstract>
<entity id="C90-1002.1">A deterministic parser</entity> is under development which represents a departure from <entity id="C90-1002.2">traditional deterministic parsers</entity> in that it combines both <entity id="C90-1002.3">symbolic and connectionist components</entity>. The connectionist component is trained either from <entity id="C90-1002.4">patterns</entity> derived from the <entity id="C90-1002.5">rules</entity> of a <entity id="C90-1002.6">deterministic grammar</entity>. The development and evolution of such a <entity id="C90-1002.7">hybrid architecture</entity> has lead to a <entity id="C90-1002.8">parser</entity> which is superior to any <entity id="C90-1002.9">known deterministic parser</entity>. Experiments are described and powerful <entity id="C90-1002.10">training techniques</entity> are demonstrated that permit <entity id="C90-1002.11">decision-making</entity> by the <entity id="C90-1002.12">connectionist component</entity> in the <entity id="C90-1002.13">parsing process</entity>. This approach has permitted some simplifications to the <entity id="C90-1002.14">rules</entity> of other <entity id="C90-1002.15">deterministic parsers</entity>, including the elimination of <entity id="C90-1002.16">rule packets</entity> and priorities. Furthermore, <entity id="C90-1002.17">parsing</entity> is performed more robustly and with more tolerance for error. Data are presented which show how a <entity id="C90-1002.18">connectionist (neural) network</entity> trained with <entity id="C90-1002.19">linguistic rules</entity> can parse both <entity id="C90-1002.20">expected (grammatical) sentences</entity> as well as some novel (ungrammatical or lexically ambiguous) sentences.
</abstract>



</text>

<text id="P02-1059">
<title> Supervised Ranking In Open-Domain Text Summarization </title>
<abstract>
The paper proposes and empirically motivates an integration of <entity id="P02-1059.1">supervised learning</entity> with <entity id="P02-1059.2">unsupervised learning</entity> to deal with human biases in <entity id="P02-1059.3">summarization</entity>. In particular, we explore the use of <entity id="P02-1059.4">probabilistic decision tree</entity> within the clustering framework to account for the variation as well as regularity in <entity id="P02-1059.5">human created summaries</entity>. The <entity id="P02-1059.6">corpus</entity> of human created extracts is created from a <entity id="P02-1059.7">newspaper corpus</entity> and used as a test set. We build <entity id="P02-1059.8">probabilistic decision trees</entity> of different flavors and integrate each of them with the clustering framework. Experiments with the <entity id="P02-1059.9">corpus</entity> demonstrate that the mixture of the two paradigms generally gives a significant boost in performance compared to cases where either ofthe two is considered alone.
</abstract>



</text>

<text id="P02-1002">
<title> Sequential Conditional Generalized Iterative Scaling </title>
<abstract>
We describe a speedup for training <entity id="P02-1002.1">conditional maximum entropy models</entity>. The algorithm is a simple variation on <entity id="P02-1002.2">Generalized Iterative Scaling</entity>, but converges roughly an order of magnitude faster, depending on the number of constraints, and the way speed is measured. Rather than attempting to train all model parameters simultaneously, the algorithm trains them sequentially. The algorithm is easy to implement, typically uses only slightly more memory, and will lead to improvements for most <entity id="P02-1002.3">maximum entropy problems</entity>.
</abstract>



</text>

<text id="C00-2123">
<title> Word Re-Ordering And DP-Based Search In Statistical Machine Translation </title>
<abstract>
In this paper, we describe a search procedure for <entity id="C00-2123.1">statistical machine translation (MT)</entity> based on <entity id="C00-2123.2">dynamic programming (DP)</entity>. Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible <entity id="C00-2123.3">word reordering</entity> between <entity id="C00-2123.4">source and target language</entity> in order to achieve an efficient search algorithm. A search restriction especially useful for the translation direction from German to English is presented. The experimental tests are carried out on the <entity id="C00-2123.5">Verbmobil task</entity> (German-English, 8000-word vocabulary), which is a <entity id="C00-2123.6">limited-domain spoken-language task</entity>.
</abstract>



</text>

<text id="C96-1055">
<title>Role Of Word Sense Disambiguation In Lexical Acquisition : Predicting Semantics From Syntactic Cues</title>
<abstract>
This paper addresses the issue of <entity id="C96-1055.1">word-sense ambiguity</entity> in extraction from <entity id="C96-1055.2">machine-readable resources</entity> for the construction of <entity id="C96-1055.3">large-scale knowledge sources</entity>. We describe two experiments: one which ignored <entity id="C96-1055.4">word-sense distinctions</entity>, resulting in 6.3% accuracy for <entity id="C96-1055.5">semantic classification</entity> of <entity id="C96-1055.6">verbs</entity> based on (Levin, 1993); and one which exploited <entity id="C96-1055.7">word-sense distinctions</entity>, resulting in 97.9% accuracy. These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that <entity id="C96-1055.8">verb semantics</entity> and <entity id="C96-1055.9">syntactic behavior</entity> are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving <entity id="C96-1055.10">semantic information</entity> from <entity id="C96-1055.11">syntactic cues</entity> if we first divide the <entity id="C96-1055.12">syntactic cues</entity> into distinct groupings that correlate with different <entity id="C96-1055.13">word senses</entity>. Finally, we show that we can provide effective acquisition techniques for novel <entity id="C96-1055.14">word senses</entity> using a combination of online sources.
</abstract>



</text>

<text id="M91-1029">
<title>PRC Inc: Description Of The PAKTUS System Used For MUC-3 </title>
<abstract>
The <entity id="M91-1029.1">PRC Adaptive Knowledge-based Text Understanding System (PAKTUS)</entity> has been under development as an Independent Research and Development project at PRC since 1984. The objective is a generic system of tools, including a <entity id="M91-1029.2">core English lexicon</entity>, <entity id="M91-1029.3">grammar</entity>, and concept representations, for building <entity id="M91-1029.4">natural language processing (NLP) systems</entity> for <entity id="M91-1029.5">text understanding</entity>. Systems built with <entity id="M91-1029.6">PAKTUS</entity> are intended to generate input to knowledge based systems ordata base systems. Input to the <entity id="M91-1029.7">NLP system</entity> is typically derived from an existing <entity id="M91-1029.8">electronic message stream</entity>, such as a news wire. <entity id="M91-1029.9">PAKTUS</entity> supports the adaptation of the generic core to a variety of domains: <entity id="M91-1029.10">JINTACCS messages</entity>, <entity id="M91-1029.11">RAINFORM messages</entity>, <entity id="M91-1029.12">news reports</entity> about a specific type of event, such as financial transfers or terrorist acts, etc., by acquiring <entity id="M91-1029.13">sublanguage and domain-specific grammar</entity>, <entity id="M91-1029.14">words, conceptual mappings</entity>, and <entity id="M91-1029.15">discourse patterns</entity>. The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success.
</abstract>



</text>

<text id="P98-1088">
<title>Memorisation for Glue Language Deduction and Categorial Parsing </title>
<abstract>
The multiplicative fragment of <entity id="P98-1088.1">linear logic</entity> has found a number of applications in <entity id="P98-1088.2">computational linguistics</entity>: in the <entity id="P98-1088.3">"glue language"</entity> approach to <entity id="P98-1088.4">LFG semantics</entity>, and in the formulation and <entity id="P98-1088.5">parsing</entity> of various <entity id="P98-1088.6">categorial grammars</entity>. These applications call for efficient deduction methods. Although a number of deduction methods for <entity id="P98-1088.7">multiplicative linear logic</entity> are known, none of them are tabular methods, which bring a substantial efficiency gain by avoiding redundant computation (cf. chart methods in <entity id="P98-1088.8">CFG parsing</entity>): this paper presents such a method, and discusses its use in relation to the above applications.
</abstract>



</text>

<text id="E06-1045">
<title>Data-Driven Generation Of Emphatic Facial Displays</title>
<abstract>
We describe an implementation of data-driven selection of emphatic facial displays for an <entity id="E06-1045.1">embodied conversational agent</entity> in a <entity id="E06-1045.2">dialogue system</entity>. A <entity id="E06-1045.3">corpus of sentences</entity> in the domain of the <entity id="E06-1045.4">target dialogue system</entity> was recorded, and the facial displays used by the <entity id="E06-1045.5">speaker</entity> were annotated. The data from those recordings was used in a range of models for generating facial displays, each model making use of a different amount of <entity id="E06-1045.6">context</entity> or choosing displays differently within a <entity id="E06-1045.7">context</entity>. The models were evaluated in two ways: by <entity id="E06-1045.8">cross-validation</entity> against the <entity id="E06-1045.9">corpus</entity>, and by asking users to rate the output. The predictions of the <entity id="E06-1045.10">cross-validation</entity> study differed from the actual user ratings. While the <entity id="E06-1045.11">cross-validation</entity> gave the highest scores to models making a majority choice within a context, the user study showed a significant preference for models that produced more variation. This preference was especially strong among the female subjects.
</abstract>



</text>

<text id="P04-1030">
<title> Head-Driven Parsing For Word Lattices </title>
<abstract>
We present the first application of the <entity id="P04-1030.1">head-driven statistical parsing model</entity> of Collins (1999) as a <entity id="P04-1030.2">simultaneous language model</entity> and <entity id="P04-1030.3">parser</entity> for <entity id="P04-1030.4">large-vocabulary speech recognition</entity>. The model is adapted to an <entity id="P04-1030.5">online left to right chart-parser</entity> for <entity id="P04-1030.6">word lattices</entity>, integrating acoustic, n-gram, and parser probabilities. The <entity id="P04-1030.7">parser</entity> uses <entity id="P04-1030.8">structural and lexical dependencies</entity> not considered by <entity id="P04-1030.9">n-gram models</entity>, conditioning recognition on more linguistically-grounded relationships. Experiments on the <entity id="P04-1030.10">Wall Street Journal treebank</entity> and lattice corpora show <entity id="P04-1030.11">word error rates</entity> competitive with the <entity id="P04-1030.12">standard n-gram language model</entity> while extracting additional <entity id="P04-1030.13">structural information</entity> useful for <entity id="P04-1030.14">speech understanding</entity>.
</abstract>



</text>

<text id="P02-1023">
<title>Improving Language Model Size Reduction Using Better Pruning Criteria </title>

<abstract>
Reducing <entity id="P02-1023.1">language model (LM) size</entity> is a critical issue when applying a <entity id="P02-1023.2">LM</entity> to realistic applications which have memory constraints. In this paper, three measures are studied for the purpose of <entity id="P02-1023.3">LM pruning</entity>. They are probability, <entity id="P02-1023.4">rank</entity>, and <entity id="P02-1023.5">entropy</entity>. We evaluated the performance of the three <entity id="P02-1023.6">pruning criteria</entity> in a real application of <entity id="P02-1023.7">Chinese text input</entity> in terms of <entity id="P02-1023.8">character error rate (CER)</entity>. We first present an empirical comparison, showing that <entity id="P02-1023.9">rank</entity> performs the best in most cases. We also show that the high-performance of <entity id="P02-1023.10">rank</entity> lies in its strong correlation with <entity id="P02-1023.11">error rate</entity>. We then present a novel method of combining two criteria in <entity id="P02-1023.12">model pruning</entity>. Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately, at the same <entity id="P02-1023.13">CER</entity>.
</abstract>



</text>

<text id="C00-1054">
<title> Finite-State Multimodal Parsing And Understanding </title>

<abstract>
 Multimodal interfaces require effective <entity id="C00-1054.1">parsing</entity> and understanding of <entity id="C00-1054.2">utterances</entity> whose content is distributed across multiple input modes. Johnston 1998 presents an approach in which strategies for <entity id="C00-1054.3">multimodal integration</entity> are stated declaratively using a <entity id="C00-1054.4">unification-based grammar</entity> that is used by a <entity id="C00-1054.5">multidimensional chart parser</entity> to compose inputs. This approach is highly expressive and supports a broad class of <entity id="C00-1054.6">interfaces</entity>, but offers only limited potential for mutual compensation among the input modes, is subject to significant concerns in terms of computational complexity, and complicates selection among alternative multimodal interpretations of the input. In this paper, we present an alternative approach in which <entity id="C00-1054.7">multimodal parsing and understanding</entity> are achieved using a <entity id="C00-1054.8">weighted finite-state device</entity> which takes <entity id="C00-1054.9">speech and gesture streams</entity> as inputs and outputs their joint interpretation. This approach is significantly more efficient, enables tight-coupling of multimodal understanding with <entity id="C00-1054.10">speech recognition</entity>, and provides a general probabilistic framework for <entity id="C00-1054.11">multimodal ambiguity resolution</entity>.
</abstract>



</text>

<text id="N06-1037">
<title>Exploring Syntactic Features For Relation Extraction Using A Convolution Tree Kernel </title>

<abstract>
This paper proposes to use a <entity id="N06-1037.1">convolution kernel</entity> over <entity id="N06-1037.2">parse trees</entity> to model <entity id="N06-1037.3">syntactic structure information</entity> for <entity id="N06-1037.4">relation extraction</entity>. Our study reveals that the <entity id="N06-1037.5">syntactic structure features</entity> embedded in a <entity id="N06-1037.6">parse tree</entity> are very effective for <entity id="N06-1037.7">relation extraction</entity> and these features can be well captured by the <entity id="N06-1037.8">convolution tree kernel</entity>. Evaluation on the <entity id="N06-1037.9">ACE 2003 corpus</entity> shows that the <entity id="N06-1037.10">convolution kernel</entity> over <entity id="N06-1037.11">parse trees</entity> can achieve comparable performance with the previous best-reported feature-based methods on the 24 <entity id="N06-1037.12">ACE relation subtypes</entity>. It also shows that our method significantly outperforms the previous two <entity id="N06-1037.13">dependency tree kernels</entity> on the 5 <entity id="N06-1037.14">ACE relation major types</entity>.
</abstract>



</text>

<text id="H90-1011">
<title>Performing Integrated Syntactic And Semantic Parsing Using Classification </title>

<abstract>
This paper describes a particular approach to <entity id="H90-1011.1">parsing</entity> that utilizes recent advances in <entity id="H90-1011.2">unification-based parsing</entity> and in <entity id="H90-1011.3">classification-based knowledge representation</entity>. As <entity id="H90-1011.4">unification-based grammatical frameworks</entity> are extended to handle richer descriptions of <entity id="H90-1011.5">linguistic information</entity>, they begin to share many of the properties that have been developed in <entity id="H90-1011.6">KL-ONE-like knowledge representation systems</entity>. This commonality suggests that some of the <entity id="H90-1011.7">classification-based representation techniques</entity> can be applied to <entity id="H90-1011.8">unification-based linguistic descriptions</entity>. This merging supports the integration of <entity id="H90-1011.9">semantic and syntactic information</entity> into the same system, simultaneously subject to the same types of processes, in an efficient manner. The result is expected to be more <entity id="H90-1011.10">efficient parsing</entity> due to the increased organization of knowledge. The use of a <entity id="H90-1011.11">KL-ONE style representation</entity> for <entity id="H90-1011.12">parsing</entity> and <entity id="H90-1011.13">semantic interpretation</entity> was first explored in the <entity id="H90-1011.14">PSI-KLONE system</entity> [2], in which <entity id="H90-1011.15">parsing</entity> is characterized as an inference process called <entity id="H90-1011.16">incremental description refinement</entity>.
</abstract>



</text>

<text id="J97-1004">
<title>Developing And Empirically Evaluating Robust Explanation Generators : The KNIGHT Experiments</title>

<abstract>
"To explain complex phenomena, an <entity id="J97-1004.1">explanation system</entity> must be able to select information from a formal representation of <entity id="J97-1004.2">domain knowledge</entity>, organize the selected information into <entity id="J97-1004.3">multisentential discourse plans</entity>, and realize the <entity id="J97-1004.4">discourse plans</entity> in text. Although recent years have witnessed significant progress in the development of sophisticated computational mechanisms for <entity id="J97-1004.5">explanation</entity>, empirical results have been limited. This paper reports on a seven-year effort to empirically study <entity id="J97-1004.6">explanation generation</entity> from <entity id="J97-1004.7">semantically rich, large-scale knowledge bases</entity>. In particular, it describes a <entity id="J97-1004.8">robust explanation system</entity> that constructs <entity id="J97-1004.9">multisentential and multi-paragraph explanations</entity> from the a <entity id="J97-1004.10">large-scale knowledge base</entity> in the domain of botanical anatomy, physiology, and development. We introduce the evaluation methodology and describe how performance was assessed with this methodology in the most extensive empirical evaluation conducted on an explanation system. In this evaluation, scored within ""half a grade"" of domain experts, and its performance exceeded that of one of the domain experts."
</abstract>



</text>

<text id="P04-2005">
<title> Automatic Acquisition Of English Topic Signatures Based On A Second Language </title><abstract>
We present a novel approach for automatically acquiring <entity id="P04-2005.1">English topic signatures</entity>. Given a particular <entity id="P04-2005.2">concept</entity>, or <entity id="P04-2005.3">word sense</entity>, a <entity id="P04-2005.4">topic signature</entity> is a set of <entity id="P04-2005.5">words</entity> that tend to co-occur with it. <entity id="P04-2005.6">Topic signatures</entity> can be useful in a number of <entity id="P04-2005.7">Natural Language Processing (NLP) applications</entity>, such as <entity id="P04-2005.8">Word Sense Disambiguation (WSD)</entity> and <entity id="P04-2005.9">Text Summarisation</entity>. Our method takes advantage of the different way in which <entity id="P04-2005.10">word senses</entity> are lexicalised in <entity id="P04-2005.11">English</entity> and <entity id="P04-2005.12">Chinese</entity>, and also exploits the large amount of <entity id="P04-2005.13">Chinese text</entity> available in <entity id="P04-2005.14">corpora</entity> and on the Web. We evaluated the <entity id="P04-2005.15">topic signatures</entity> on a <entity id="P04-2005.16">WSD task</entity>, where we trained a <entity id="P04-2005.17">second-order vector cooccurrence algorithm</entity> on <entity id="P04-2005.18">standard WSD datasets</entity>, with promising results.
</abstract>



</text>

<text id="P04-2010">
<title>
A Machine Learning Approach To German Pronoun Resolution </title><abstract>
This paper presents a novel <entity id="P04-2010.1">ensemble learning approach</entity> to resolving <entity id="P04-2010.2">German pronouns</entity>. <entity id="P04-2010.3">Boosting</entity>, the method in question, combines the moderately accurate <entity id="P04-2010.4">hypotheses</entity> of several <entity id="P04-2010.5">classifiers</entity> to form a highly accurate one. Experiments show that this approach is superior to a single <entity id="P04-2010.6">decision-tree classifier</entity>. Furthermore, we present a <entity id="P04-2010.7">standalone system</entity> that resolves <entity id="P04-2010.8">pronouns</entity> in <entity id="P04-2010.9">unannotated text</entity> by using a fully automatic sequence of <entity id="P04-2010.10">preprocessing modules</entity> that mimics the <entity id="P04-2010.11">manual annotation process</entity>. Although the system performs well within a limited <entity id="P04-2010.12">textual domain</entity>, further research is needed to make it effective for <entity id="P04-2010.13">open-domain question answering</entity> and <entity id="P04-2010.14">text summarisation</entity>.
</abstract>



</text>

<text id="C04-1035">
<title>
Classifying Ellipsis In Dialogue : A Machine Learning Approach </title><abstract>
This paper presents a <entity id="C04-1035.1">machine learning approach</entity> to <entity id="C04-1035.2">bare slice disambiguation</entity> in <entity id="C04-1035.3">dialogue</entity>. We extract a set of <entity id="C04-1035.4">heuristic principles</entity> from a <entity id="C04-1035.5">corpus-based sample</entity> and formulate them as <entity id="C04-1035.6">probabilistic Horn clauses</entity>. We then use the predicates of such <entity id="C04-1035.7">clauses</entity> to create a set of <entity id="C04-1035.8">domain independent features</entity> to annotate an <entity id="C04-1035.9">input dataset</entity>, and run two different <entity id="C04-1035.10">machine learning algorithms</entity> : SLIPPER, a <entity id="C04-1035.11">rule-based learning algorithm</entity>, and TiMBL, a <entity id="C04-1035.12">memory-based system</entity>. Both learners perform well, yielding similar <entity id="C04-1035.13">success rates</entity> of approx 90%. The results show that the <entity id="C04-1035.14">features</entity> in terms of which we formulate our <entity id="C04-1035.15">heuristic principles</entity> have significant predictive power, and that <entity id="C04-1035.16">rules</entity> that closely resemble our <entity id="C04-1035.17">Horn clauses</entity> can be learnt automatically from these <entity id="C04-1035.18">features</entity>.
</abstract>



</text>

<text id="C04-1036">
<title> Feature Vector Quality And Distributional Similarity </title><abstract>
We suggest a new goal and <entity id="C04-1036.1">evaluation criterion</entity> for <entity id="C04-1036.2">word similarity measures</entity>. The new criterion  <entity id="C04-1036.3">meaning-entailing substitutability</entity>  fits the needs of <entity id="C04-1036.4">semantic-oriented NLP applications</entity> and can be evaluated directly (independent of an application) at a good level of <entity id="C04-1036.5">human agreement</entity>. Motivated by this <entity id="C04-1036.6">semantic criterion</entity> we analyze the empirical quality of <entity id="C04-1036.7">distributional word feature vectors</entity> and its impact on <entity id="C04-1036.8">word similarity results</entity>, proposing an objective measure for evaluating <entity id="C04-1036.9">feature vector quality</entity>. Finally, a novel <entity id="C04-1036.10">feature weighting and selection function</entity> is presented, which yields superior <entity id="C04-1036.11">feature vectors</entity> and better <entity id="C04-1036.12">word similarity performance</entity>.</abstract>



</text>

<text id="C04-1068">
<title>
Filtering Speaker-Specific Words From Electronic Discussions</title><abstract>
The work presented in this paper is the first step in a project which aims to cluster and summarise <entity id="C04-1068.1">electronic discussions</entity> in the context of <entity id="C04-1068.2">help-desk applications</entity>. The eventual objective of this project is to use these <entity id="C04-1068.3">summaries</entity> to assist help-desk users and operators. In this paper, we identify <entity id="C04-1068.4">features</entity> of <entity id="C04-1068.5">electronic discussions</entity> that influence the <entity id="C04-1068.6">clustering process</entity>, and offer a <entity id="C04-1068.7">filtering mechanism</entity> that removes undesirable <entity id="C04-1068.8">influences</entity>. We tested the <entity id="C04-1068.9">clustering and filtering processes</entity> on <entity id="C04-1068.10">electronic newsgroup discussions</entity>, and evaluated their <entity id="C04-1068.11">performance</entity> by means of two experiments : <entity id="C04-1068.12">coarse-level clustering</entity> simple <entity id="C04-1068.13">information retrieval</entity>.
</abstract>



</text>

<text id="C04-1080">
<title> Part- Of- Speech Tagging In Context </title><abstract>
We present a new HMM tagger that exploits <entity id="C04-1080.1">context</entity> on both sides of a <entity id="C04-1080.2">word</entity> to be tagged, and evaluate it in both the <entity id="C04-1080.3">unsupervised and supervised case</entity>. Along the way, we present the first comprehensive comparison of <entity id="C04-1080.4">unsupervised methods for part-of-speech tagging</entity>, noting that published results to date have not been comparable across <entity id="C04-1080.5">corpora</entity> or <entity id="C04-1080.6">lexicons</entity>. Observing that the <entity id="C04-1080.7">quality</entity> of the <entity id="C04-1080.8">lexicon</entity> greatly impacts the <entity id="C04-1080.9">accuracy</entity> that can be achieved by the <entity id="C04-1080.10">algorithms</entity>, we present a method of <entity id="C04-1080.11">HMM training</entity> that improves <entity id="C04-1080.12">accuracy</entity> when <entity id="C04-1080.13">training</entity> of <entity id="C04-1080.14">lexical probabilities</entity> is unstable. Finally, we show how this new tagger achieves state-of-the-art results in a <entity id="C04-1080.15">supervised, non-training intensive framework</entity>.
</abstract>



</text>

<text id="C04-1096">
<title> Generation Of Relative Referring Expressions Based On Perceptual Grouping</title><abstract>
Past work of generating <entity id="C04-1096.1">referring expressions</entity> mainly utilized attributes of <entity id="C04-1096.2">objects</entity> and <entity id="C04-1096.3">binary relations</entity> between <entity id="C04-1096.4">objects</entity>. However, such an approach does not work well when there is no distinctive attribute among <entity id="C04-1096.5">objects</entity>. To overcome this limitation, this paper proposes a method utilizing the perceptual groups of <entity id="C04-1096.6">objects</entity> and <entity id="C04-1096.7">n-ary relations</entity> among them. The key is to identify groups of <entity id="C04-1096.8">objects</entity> that are naturally recognized by humans. We conducted psychological experiments with 42 subjects to collect <entity id="C04-1096.9">referring expressions</entity> in such situations, and built a <entity id="C04-1096.10">generation algorithm</entity> based on the results. The evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions.
</abstract>



</text>

<text id="C04-1103">
<title>
Direct Orthographical Mapping For Machine Transliteration </title><abstract>
<entity id="C04-1103.1">Machine transliteration/back-transliteration</entity> plays an important role in many <entity id="C04-1103.2">multilingual speech and language applications</entity>. In this paper, a novel framework for <entity id="C04-1103.3">machine transliteration/backtransliteration</entity> that allows us to carry out <entity id="C04-1103.4">direct orthographical mapping (DOM)</entity> between two different <entity id="C04-1103.5">languages</entity> is presented. Under this framework, a <entity id="C04-1103.6">joint source-channel transliteration model</entity>, also called <entity id="C04-1103.7">n-gram transliteration model (n-gram TM)</entity>, is further proposed to model the <entity id="C04-1103.8">transliteration process</entity>. We evaluate the proposed methods through several <entity id="C04-1103.9">transliteration/backtransliteration experiments</entity> for <entity id="C04-1103.10">English/Chinese and English/Japanese language pairs</entity>. Our study reveals that the proposed method not only reduces an extensive <entity id="C04-1103.11">system development effort</entity> but also improves the <entity id="C04-1103.12">transliteration accuracy</entity> significantly.
</abstract>



</text>

<text id="C04-1112">
<title>
A Lemma- Based Approach To A Maximum Entropy Word Sense Disambiguation System For Dutch</title><abstract>
In this paper, we present a <entity id="C04-1112.1">corpus-based supervised word sense disambiguation (WSD) system</entity> for <entity id="C04-1112.2">Dutch</entity> which combines <entity id="C04-1112.3">statistical classification</entity> (<entity id="C04-1112.4">maximum entropy</entity>) with <entity id="C04-1112.5">linguistic information</entity>. Instead of building individual <entity id="C04-1112.6">classifiers</entity> per <entity id="C04-1112.7">ambiguous wordform</entity>, we introduce a <entity id="C04-1112.8">lemma-based approach</entity>. The advantage of this novel method is that it clusters all <entity id="C04-1112.9">inflected forms</entity> of an <entity id="C04-1112.10">ambiguous word</entity> in one <entity id="C04-1112.11">classifier</entity>, therefore augmenting the <entity id="C04-1112.12">training material</entity> available to the <entity id="C04-1112.13">algorithm</entity>. Testing the <entity id="C04-1112.14">lemma-based model</entity> on the <entity id="C04-1112.15">Dutch Senseval-2 test data</entity>, we achieve a significant increase in <entity id="C04-1112.16">accuracy</entity> over the <entity id="C04-1112.17">wordform model</entity>. Also, the <entity id="C04-1112.18">WSD system based on lemmas</entity> is smaller and more robust.
</abstract>



</text>

<text id="C04-1116">
<title> Term Aggregation : Mining Synonymous Expressions Using Personal Stylistic Variations</title><abstract>
We present a <entity id="C04-1116.1">text mining method</entity> for finding <entity id="C04-1116.2">synonymous expressions</entity> based on the <entity id="C04-1116.3">distributional hypothesis</entity> in a set of coherent <entity id="C04-1116.4">corpora</entity>. This paper proposes a new methodology to improve the <entity id="C04-1116.5">accuracy</entity> of a <entity id="C04-1116.6">term aggregation system</entity> using each author's <entity id="C04-1116.7">text</entity> as a coherent <entity id="C04-1116.8">corpus</entity>. Our approach is based on the idea that one person tends to use one <entity id="C04-1116.9">expression</entity> for one <entity id="C04-1116.10">meaning</entity>. According to our assumption, most of the <entity id="C04-1116.11">words</entity> with <entity id="C04-1116.12">similar context features</entity> in each author's <entity id="C04-1116.13">corpus</entity> tend not to be <entity id="C04-1116.14">synonymous expressions</entity>. Our proposed method improves the <entity id="C04-1116.15">accuracy</entity> of our <entity id="C04-1116.16">term aggregation system</entity>, showing that our approach is successful.
</abstract>



</text>

<text id="C04-1128">
<title> Detection Of Question- Answer Pairs In Email Conversations</title><abstract>
While <entity id="C04-1128.1">sentence extraction</entity> as an approach to <entity id="C04-1128.2">summarization</entity> has been shown to work in <entity id="C04-1128.3">documents</entity> of certain <entity id="C04-1128.4">genres</entity>, because of the conversational nature of <entity id="C04-1128.5">email communication</entity> where <entity id="C04-1128.6">utterances</entity> are made in relation to one made previously, <entity id="C04-1128.7">sentence extraction</entity> may not capture the necessary <entity id="C04-1128.8">segments</entity> of <entity id="C04-1128.9">dialogue</entity> that would make a <entity id="C04-1128.10">summary</entity> coherent. In this paper, we present our work on the detection of <entity id="C04-1128.11">question-answer pairs</entity> in an <entity id="C04-1128.12">email conversation</entity> for the task of <entity id="C04-1128.13">email summarization</entity>. We show that various <entity id="C04-1128.14">features</entity> based on the structure of email-threads can be used to improve upon <entity id="C04-1128.15">lexical similarity</entity> of <entity id="C04-1128.16">discourse segments</entity> for <entity id="C04-1128.17">question-answer pairing</entity>.
</abstract>



</text>

<text id="C04-1147">
<title>
Fast Computation Of Lexical Affinity Models </title><abstract>
We present a framework for the fast <entity id="C04-1147.1">computation</entity> of <entity id="C04-1147.2">lexical affinity models</entity>. The framework is composed of a novel algorithm to efficiently compute the <entity id="C04-1147.3">co-occurrence distribution</entity> between pairs of <entity id="C04-1147.4">terms</entity>, an <entity id="C04-1147.5">independence model</entity>, and a <entity id="C04-1147.6">parametric affinity model</entity>. In comparison with previous <entity id="C04-1147.7">models</entity>, which either use arbitrary windows to compute <entity id="C04-1147.8">similarity</entity> between <entity id="C04-1147.9">words</entity> or use <entity id="C04-1147.10">lexical affinity</entity> to create <entity id="C04-1147.11">sequential models</entity>, in this paper we focus on <entity id="C04-1147.12">models</entity> intended to capture the <entity id="C04-1147.13">co-occurrence patterns</entity> of any pair of <entity id="C04-1147.14">words</entity> or <entity id="C04-1147.15">phrases</entity> at any distance in the <entity id="C04-1147.16">corpus</entity>. The framework is flexible, allowing fast <entity id="C04-1147.17">adaptation</entity> to <entity id="C04-1147.18">applications</entity> and it is scalable. We apply it in combination with a <entity id="C04-1147.19">terabyte corpus</entity> to answer <entity id="C04-1147.20">natural language tests</entity>, achieving encouraging results.
</abstract>



</text>

<text id="C04-1192">
<title>
Fine-Grained Word Sense Disambiguation Based On Parallel Corpora, Word Alignment, Word Clustering And Aligned Wordnets</title><abstract>
The paper presents a method for <entity id="C04-1192.1">word sense disambiguation</entity> based on <entity id="C04-1192.2">parallel corpora</entity>. The method exploits recent advances in <entity id="C04-1192.3">word alignment</entity> and <entity id="C04-1192.4">word clustering</entity> based on <entity id="C04-1192.5">automatic extraction</entity> of <entity id="C04-1192.6">translation equivalents</entity> and being supported by available aligned <entity id="C04-1192.7">wordnets</entity> for the <entity id="C04-1192.8">languages</entity> in the <entity id="C04-1192.9">corpus</entity>. The <entity id="C04-1192.10">wordnets</entity> are aligned to the <entity id="C04-1192.11">Princeton Wordnet</entity>, according to the principles established by <entity id="C04-1192.12">EuroWordNet</entity>. The evaluation of the <entity id="C04-1192.13">WSD system</entity>, implementing the method described herein showed very encouraging results. The same system used in a validation mode, can be used to check and spot <entity id="C04-1192.14">alignment errors</entity> in <entity id="C04-1192.15">multilingually aligned wordnets</entity> as <entity id="C04-1192.16">BalkaNet</entity> and <entity id="C04-1192.17">EuroWordNet</entity>.
</abstract>



</text>

<text id="N04-1022">
<title> Minimum Bayes-Risk Decoding For Statistical Machine Translation </title><abstract>
We present <entity id="N04-1022.1">Minimum Bayes-Risk (MBR) decoding</entity> for <entity id="N04-1022.2">statistical machine translation</entity>. This statistical approach aims to minimize <entity id="N04-1022.3">expected loss</entity> of <entity id="N04-1022.4">translation errors</entity> under <entity id="N04-1022.5">loss functions</entity> that measure <entity id="N04-1022.6">translation performance</entity>. We describe a hierarchy of <entity id="N04-1022.7">loss functions</entity> that incorporate different levels of <entity id="N04-1022.8">linguistic information</entity> from <entity id="N04-1022.9">word strings</entity>, <entity id="N04-1022.10">word-to-word alignments</entity> from an <entity id="N04-1022.11">MT system</entity>, and <entity id="N04-1022.12">syntactic structure</entity> from <entity id="N04-1022.13">parse-trees</entity> of <entity id="N04-1022.14">source and target language sentences</entity>. We report the <entity id="N04-1022.15">performance</entity> of the <entity id="N04-1022.16">MBR decoders</entity> on a <entity id="N04-1022.17">Chinese-to-English translation task</entity>. Our results show that <entity id="N04-1022.18">MBR decoding</entity> can be used to tune <entity id="N04-1022.19">statistical MT performance</entity> for specific <entity id="N04-1022.20">loss functions</entity>.
</abstract>



</text>

<text id="N04-4028">
<title> Confidence Estimation For Information Extraction </title><abstract>
<entity id="N04-4028.1">Information extraction techniques</entity> automatically create <entity id="N04-4028.2">structured databases</entity> from <entity id="N04-4028.3">unstructured data sources</entity>, such as the Web or <entity id="N04-4028.4">newswire documents</entity>. Despite the successes of these systems, <entity id="N04-4028.5">accuracy</entity> will always be imperfect. For many reasons, it is highly desirable to accurately estimate the <entity id="N04-4028.6">confidence</entity> the system has in the correctness of each <entity id="N04-4028.7">extracted field</entity>. The <entity id="N04-4028.8">information extraction system</entity> we evaluate is based on a <entity id="N04-4028.9">linear-chain conditional random field (CRF)</entity>, a <entity id="N04-4028.10">probabilistic model</entity> which has performed well on <entity id="N04-4028.11">information extraction tasks</entity> because of its ability to capture arbitrary, overlapping <entity id="N04-4028.12">features</entity> of the <entity id="N04-4028.13">input</entity> in a <entity id="N04-4028.14"> Markov model</entity>. We implement several techniques to estimate the <entity id="N04-4028.15">confidence</entity> of both <entity id="N04-4028.16">extracted fields</entity> and entire <entity id="N04-4028.17">multi-field records</entity>, obtaining an <entity id="N04-4028.18">average precision</entity> of 98% for retrieving correct <entity id="N04-4028.19">fields</entity> and 87% for multi-field records.
</abstract>



</text>

<text id="M92-1025"><title>
GE NLTOOLSET: Description Of The System As Used For MUC-4
</title><abstract>
The <entity id="M92-1025.1">GE NLToolset</entity> is a set of <entity id="M92-1025.2">text interpretation tools</entity> designed to be easily adapted to new <entity id="M92-1025.3">domains</entity>. This report summarizes the system and its performance on the <entity id="M92-1025.4">MUC-4 task</entity>. 
</abstract>



</text>

<text id="P05-1028">
<title>
Exploring And Exploiting The Limited Utility Of Captions In Recognizing Intention In Information Graphics </title><abstract>
This paper presents a <entity id="P05-1028.1">corpus study</entity> that explores the extent to which captions contribute to recognizing the intended message of an <entity id="P05-1028.2">information graphic</entity>. It then presents an implemented <entity id="P05-1028.3">graphic interpretation system</entity> that takes into account a variety of <entity id="P05-1028.4">communicative signals</entity>, and an evaluation study showing that evidence obtained from <entity id="P05-1028.5">shallow processing</entity> of the graphic's caption has a significant impact on the system's success. This work is part of a larger project whose goal is to provide <entity id="P05-1028.6">sight-impaired users</entity> with effective access to <entity id="P05-1028.7">information graphics</entity>.
</abstract>



</text>

<text id="P05-1057">
<title>
 Log-Linear Models For Word Alignment </title><abstract>
We present a framework for <entity id="P05-1057.1">word alignment</entity> based on <entity id="P05-1057.2">log-linear models</entity>. All <entity id="P05-1057.3">knowledge sources</entity> are treated as <entity id="P05-1057.4">feature functions</entity>, which depend on the <entity id="P05-1057.5">source langauge sentence</entity>, the <entity id="P05-1057.6">target language sentence</entity> and possible additional variables. <entity id="P05-1057.7">Log-linear models</entity> allow <entity id="P05-1057.8">statistical alignment models</entity> to be easily extended by incorporating <entity id="P05-1057.9">syntactic information</entity>. In this paper, we use <entity id="P05-1057.10">IBM Model 3 alignment probabilities</entity>, <entity id="P05-1057.11">POS correspondence</entity>, and <entity id="P05-1057.12">bilingual dictionary coverage</entity> as <entity id="P05-1057.13">features</entity>. Our experiments show that <entity id="P05-1057.14">log-linear models</entity> significantly outperform <entity id="P05-1057.15">IBM translation models</entity>.
</abstract>



</text>

<text id="P05-2013">
<title> Automatic Induction Of A CCG Grammar For Turkish </title><abstract>
This paper presents the results of automatically inducing a <entity id="P05-2013.1">Combinatory Categorial Grammar (CCG) lexicon</entity> from a <entity id="P05-2013.2">Turkish dependency treebank</entity>. The fact that <entity id="P05-2013.3">Turkish</entity> is an <entity id="P05-2013.4">agglutinating free word order language</entity> presents a challenge for <entity id="P05-2013.5">language theories</entity>. We explored possible ways to obtain a <entity id="P05-2013.6">compact lexicon</entity>, consistent with <entity id="P05-2013.7">CCG principles</entity>, from a <entity id="P05-2013.8">treebank</entity> which is an order of magnitude smaller than <entity id="P05-2013.9">Penn WSJ</entity>.
</abstract>



</text>

<text id="I05-2044">
<title>
 Two-Phase Shift-Reduce Deterministic Dependency Parser of Chinese </title><abstract>
In the <entity id="I05-2044.1">Chinese language</entity>, a <entity id="I05-2044.2">verb</entity> may have its <entity id="I05-2044.3">dependents</entity> on its left, right or on both sides. The <entity id="I05-2044.4">ambiguity resolution</entity> of <entity id="I05-2044.5">right-side dependencies</entity> is essential for <entity id="I05-2044.6">dependency parsing</entity> of <entity id="I05-2044.7">sentences</entity> with two or more <entity id="I05-2044.8">verbs</entity>. Previous works on <entity id="I05-2044.9">shift-reduce dependency parsers</entity> may not guarantee the <entity id="I05-2044.10">connectivity</entity> of a <entity id="I05-2044.11">dependency tree</entity> due to their weakness at resolving the <entity id="I05-2044.12">right-side dependencies</entity>. This paper proposes a <entity id="I05-2044.13">two-phase shift-reduce dependency parser</entity> based on <entity id="I05-2044.14">SVM learning</entity>. The <entity id="I05-2044.15">left-side dependents</entity> and <entity id="I05-2044.16">right-side nominal dependents</entity> are detected in Phase I, and <entity id="I05-2044.17">right-side verbal dependents</entity> are decided in Phase II. In experimental evaluation, our proposed method outperforms previous <entity id="I05-2044.18">shift-reduce dependency parsers</entity> for the <entity id="I05-2044.19">Chine language</entity>, showing improvement of <entity id="I05-2044.20">dependency accuracy</entity> by 10.08%.
</abstract>



</text>

<text id="E99-1038">
<title>Focusing On Focus : A Formalization</title><abstract>
We present an operable definition of <entity id="E99-1038.1">focus</entity> which is argued to be of a cognito-pragmatic nature and explore how it is determined in <entity id="E99-1038.2">discourse</entity> in a formalized manner. For this purpose, a file card model of <entity id="E99-1038.3">discourse model</entity> and <entity id="E99-1038.4">knowledge store</entity> is introduced enabling the <entity id="E99-1038.5">decomposition</entity> and <entity id="E99-1038.6">formal representation</entity> of its <entity id="E99-1038.7">determination process</entity> as a programmable algorithm (<entity id="E99-1038.8">FDA</entity>). Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of <entity id="E99-1038.9">focus</entity> via <entity id="E99-1038.10">FDA</entity> as a <entity id="E99-1038.11">discourse-level construct</entity> into <entity id="E99-1038.12">speech synthesis systems</entity>, in particular, <entity id="E99-1038.13">concept-to-speech systems</entity>, is also briefly discussed.
</abstract>



</text>

<text id="E87-1037">
<title>
A Comparison Of Rule-Invocation Strategies In Context-Free Chart Parsing </title><abstract>
Currently several <entity id="E87-1037.1">grammatical formalisms</entity> converge towards being declarative and towards utilizing <entity id="E87-1037.2">context-free phrase-structure grammar</entity> as a backbone, e.g. <entity id="E87-1037.3">LFG</entity> and <entity id="E87-1037.4">PATR-II</entity>. Typically the processing of these formalisms is organized within a <entity id="E87-1037.5">chart-parsing framework</entity>. The declarative character of the <entity id="E87-1037.6">formalisms</entity> makes it important to decide upon an overall <entity id="E87-1037.7">optimal control strategy</entity> on the part of the processor. In particular, this brings the <entity id="E87-1037.8">rule-invocation strategy</entity> into critical focus: to gain maximal <entity id="E87-1037.9">processing efficiency</entity>, one has to determine the best way of putting the <entity id="E87-1037.10">rules</entity> to use. The aim of this paper is to provide a survey and a practical comparison of fundamental <entity id="E87-1037.11">rule-invocation strategies</entity> within <entity id="E87-1037.12">context-free chart parsing</entity>.
</abstract>



</text>

<text id="E91-1043">
<title>
A Bidirectional Model For Natural Language Processing </title><abstract>
In this paper I will argue for a <entity id="E91-1043.1">model of grammatical processing</entity> that is based on <entity id="E91-1043.2">uniform processing</entity> and <entity id="E91-1043.3">knowledge sources</entity>. The main <entity id="E91-1043.4">feature</entity> of this model is to view <entity id="E91-1043.5">parsing</entity> and <entity id="E91-1043.6">generation</entity> as two strongly interleaved tasks performed by a single <entity id="E91-1043.7">parametrized deduction</entity> process. It will be shown that this view supports flexible and efficient <entity id="E91-1043.8">natural language processing</entity>.
</abstract>



</text>

<text id="E93-1023">
<title>
A Probabilistic Context-Free Grammar For Disambiguation In Morphological Parsing </title><abstract>
One of the major problems one is faced with when decomposing <entity id="E93-1023.1">words</entity> into their <entity id="E93-1023.2">constituent parts</entity> is <entity id="E93-1023.3">ambiguity</entity>: the <entity id="E93-1023.4">generation</entity> of multiple <entity id="E93-1023.5">analyses</entity> for one <entity id="E93-1023.6">input word</entity>, many of which are implausible. In order to deal with <entity id="E93-1023.7">ambiguity</entity>, the <entity id="E93-1023.8">MORphological PArser MORPA</entity> is provided with a <entity id="E93-1023.9">probabilistic context-free grammar (PCFG)</entity>, i.e. it combines a <entity id="E93-1023.10">"conventional" context-free morphological grammar</entity> to filter out <entity id="E93-1023.11">ungrammatical segmentations</entity> with a <entity id="E93-1023.12">probability-based scoring function</entity> which determines the likelihood of each successful <entity id="E93-1023.13">parse</entity>. Consequently, remaining <entity id="E93-1023.14">analyses</entity> can be ordered along a scale of plausibility. Test performance data will show that a <entity id="E93-1023.15">PCFG</entity> yields good results in <entity id="E93-1023.16">morphological parsing</entity>. <entity id="E93-1023.17">MORPA</entity> is a fully implemented <entity id="E93-1023.18">parser</entity> developed for use in a <entity id="E93-1023.19">text-to-speech conversion system</entity>.
</abstract>



</text>

<text id="I05-3022">
<title> Chinese Word Segmentation in FTRD Beijing</title><abstract>
This paper presents a <entity id="I05-3022.1">word segmentation system</entity> in France Telecom R&amp;D Beijing, which uses a unified approach to <entity id="I05-3022.2">word breaking</entity> and <entity id="I05-3022.3">OOV identification</entity>. The <entity id="I05-3022.4">output</entity> can be customized to meet different <entity id="I05-3022.5">segmentation standards</entity> through the application of an ordered list of transformation. The <entity id="I05-3022.6">system</entity> participated in all the tracks of the <entity id="I05-3022.7">segmentation bakeoff</entity> -- <entity id="I05-3022.8">PK-open</entity>, <entity id="I05-3022.9">PK-closed</entity>, <entity id="I05-3022.10">AS-open</entity>, <entity id="I05-3022.11">AS-closed</entity>, <entity id="I05-3022.12">HK-open</entity>, <entity id="I05-3022.13">HK-closed</entity>, <entity id="I05-3022.14">MSR-open</entity> and <entity id="I05-3022.15">MSR- closed</entity> -- and achieved the <entity id="I05-3022.16">state-of-the-art performance</entity> in <entity id="I05-3022.17">MSR-open</entity>, <entity id="I05-3022.18">MSR-close</entity> and <entity id="I05-3022.19">PK-open</entity> tracks. Analysis of the results shows that each component of the system contributed to the <entity id="I05-3022.20">scores</entity>.
</abstract>



</text>

<text id="E93-1043">
<title>
Coping With Derivation In A Morphological Component </title><abstract>
In this paper a <entity id="E93-1043.1">morphological component</entity> with a limited capability to automatically interpret (and generate) <entity id="E93-1043.2">derived words</entity> is presented. The system combines an extended <entity id="E93-1043.3">two-level morphology</entity> [Trost, 1991a; Trost, 1991b] with a <entity id="E93-1043.4">feature-based word grammar</entity> building on a <entity id="E93-1043.5">hierarchical lexicon</entity>. <entity id="E93-1043.6">Polymorphemic stems</entity> not explicitly stored in the <entity id="E93-1043.7">lexicon</entity> are given a <entity id="E93-1043.8">compositional interpretation</entity>. That way the system allows to minimize redundancy in the <entity id="E93-1043.9">lexicon</entity> because <entity id="E93-1043.10">derived words</entity> that are transparent need not to be stored explicitly. Also, <entity id="E93-1043.11">words formed ad-hoc</entity> can be recognized correctly. The system is implemented in CommonLisp and has been tested on examples from <entity id="E93-1043.12">German derivation</entity>.
</abstract>



</text>

<text id="E99-1014">
<title>
Full Text Parsing Using Cascades Of Rules: An Information Extraction Perspective</title><abstract>
This paper proposes an approach to <entity id="E99-1014.1">full parsing</entity> suitable for <entity id="E99-1014.2">Information Extraction</entity> from <entity id="E99-1014.3">texts</entity>. Sequences of cascades of <entity id="E99-1014.4">rules</entity> deterministically analyze the <entity id="E99-1014.5">text</entity>, building <entity id="E99-1014.6">unambiguous structures</entity>. Initially basic <entity id="E99-1014.7">chunks</entity> are analyzed; then <entity id="E99-1014.8">argumental relations</entity> are recognized; finally <entity id="E99-1014.9">modifier attachment</entity> is performed and the <entity id="E99-1014.10">global parse tree</entity> is built. The approach was proven to work for three <entity id="E99-1014.11">languages</entity> and different <entity id="E99-1014.12">domains</entity>. It was implemented in the <entity id="E99-1014.13">IE module</entity> of <entity id="E99-1014.14">FACILE, a EU project for multilingual text classification and IE</entity>.
</abstract>



</text>

<text id="H91-1010">
<title>
New Results With The Lincoln Tied-Mixture HMM CSR System </title><abstract>
The following describes recent work on the <entity id="H91-1010.1">Lincoln CSR system</entity>. Some new variations in <entity id="H91-1010.2">semiphone modeling</entity> have been tested. A very simple improved <entity id="H91-1010.3">duration model</entity> has reduced the <entity id="H91-1010.4">error rate</entity> by about 10% in both <entity id="H91-1010.5">triphone and semiphone systems</entity>. A new <entity id="H91-1010.6">training strategy</entity> has been tested which, by itself, did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique. Finally, the <entity id="H91-1010.7">recognizer</entity> has been modified to use <entity id="H91-1010.8">bigram back-off language models</entity>. The system was then transferred from the <entity id="H91-1010.9">RM task</entity> to the <entity id="H91-1010.10">ATIS CSR task</entity> and a limited number of development tests performed. Evaluation test results are presented for both the <entity id="H91-1010.11">RM and ATIS CSR tasks</entity>.
</abstract>



</text>

<text id="A97-1020"> 
<title>Reading more into Foreign Languages</title>
<abstract>
<entity id="A97-1020.1">GLOSSER</entity> is designed to support reading and learning to read in a foreign <entity id="A97-1020.2">language</entity>. There are four <entity id="A97-1020.3">language pairs</entity> currently supported by <entity id="A97-1020.4">GLOSSER</entity>: <entity id="A97-1020.5">English-Bulgarian</entity>, <entity id="A97-1020.6">English-Estonian</entity>, <entity id="A97-1020.7">English-Hungarian</entity> and <entity id="A97-1020.8">French-Dutch</entity>. The program is operational on UNIX and Windows '95 platforms, and has undergone a pilot user-study. A demonstration (in UNIX) for <entity id="A97-1020.9">Applied Natural Language Processing</entity> emphasizes components put to novel technical uses in <entity id="A97-1020.10">intelligent computer-assisted morphological analysis (ICALL)</entity>, including <entity id="A97-1020.11">disambiguated morphological analysis</entity> and <entity id="A97-1020.12">lemmatized indexing</entity> for an <entity id="A97-1020.13">aligned bilingual corpus</entity> of <entity id="A97-1020.14">word examples</entity>. </abstract>



</text>

<text id="A97-1042">
<title> Identifying Topics By Position</title><abstract>
This paper addresses the problem of identifying likely <entity id="A97-1042.1">topics</entity> of <entity id="A97-1042.2">texts</entity> by their position in the <entity id="A97-1042.3">text</entity>. It describes the automated <entity id="A97-1042.4">training</entity> and evaluation of an <entity id="A97-1042.5">Optimal Position Policy</entity>, a method of locating the likely positions of <entity id="A97-1042.6">topic-bearing sentences</entity> based on <entity id="A97-1042.7">genre-specific regularities</entity> of <entity id="A97-1042.8">discourse structure</entity>. This method can be used in applications such as <entity id="A97-1042.9">information retrieval</entity>, <entity id="A97-1042.10">routing</entity>, and <entity id="A97-1042.11">text summarization</entity>.
</abstract>



</text>

<text id="H05-1064">
<title>
 Hidden-Variable Models For Discriminative Reranking </title><abstract>
We describe a new method for the representation of <entity id="H05-1064.1">NLP structures</entity> within <entity id="H05-1064.2">reranking approaches</entity>. We make use of a <entity id="H05-1064.3">conditional log-linear model</entity>, with <entity id="H05-1064.4">hidden variables</entity> representing the <entity id="H05-1064.5">assignment</entity> of <entity id="H05-1064.6">lexical items</entity> to <entity id="H05-1064.7">word clusters</entity> or <entity id="H05-1064.8">word senses</entity>. The model learns to automatically make these <entity id="H05-1064.9">assignments</entity> based on a <entity id="H05-1064.10">discriminative training criterion</entity>. <entity id="H05-1064.11">Training</entity> and <entity id="H05-1064.12">decoding</entity> with the model requires summing over an exponential number of <entity id="H05-1064.13">hidden-variable assignments</entity>: the required summations can be computed efficiently and exactly using <entity id="H05-1064.14">dynamic programming</entity>. As a case study, we apply the model to <entity id="H05-1064.15">parse reranking</entity>. The model gives an <entity id="H05-1064.16">F-measure improvement</entity> of ~1.25% beyond the <entity id="H05-1064.17">base parser</entity>, and an ~0.25% improvement beyond <entity id="H05-1064.18">Collins (2000) reranker</entity>. Although our experiments are focused on <entity id="H05-1064.19">parsing</entity>, the techniques described generalize naturally to <entity id="H05-1064.20">NLP structures</entity> other than <entity id="H05-1064.21">parse trees</entity>.
</abstract>



</text>

<text id="I05-4008">
<title>
 Taiwan Child Language Corpus : Data Collection and Annotation </title><abstract>
<entity id="I05-4008.1">Taiwan Child Language Corpus</entity> contains <entity id="I05-4008.2">scripts</entity> transcribed from about 330 hours of <entity id="I05-4008.3">recordings</entity> of fourteen young children from <entity id="I05-4008.4">Southern Min Chinese</entity> speaking families in Taiwan. The format of the <entity id="I05-4008.5">corpus</entity> adopts the <entity id="I05-4008.6">Child Language Data Exchange System (CHILDES)</entity>. The size of the <entity id="I05-4008.7">corpus</entity> is about 1.6 million <entity id="I05-4008.8">words</entity>. In this paper, we describe <entity id="I05-4008.9">data collection</entity>, <entity id="I05-4008.10">transcription</entity>, <entity id="I05-4008.11">word segmentation</entity>, and <entity id="I05-4008.12">part-of-speech annotation</entity> of this <entity id="I05-4008.13">corpus</entity>. Applications of the <entity id="I05-4008.14">corpus</entity> are also discussed.
</abstract>



</text>

<text id="P81-1032"> 

<title> Dynamic Strategy Selection in Flexible Parsing </title>
<abstract>
Robust <entity id="P81-1032.1">natural language interpretation</entity> requires strong <entity id="P81-1032.2">semantic domain models</entity>, <entity id="P81-1032.3">fail-soft recovery heuristics</entity>, and very flexible <entity id="P81-1032.4">control structures</entity>. Although <entity id="P81-1032.5">single-strategy parsers</entity> have met with a measure of success, a <entity id="P81-1032.6">multi-strategy approach</entity> is shown to provide a much higher degree of flexibility, redundancy, and ability to bring <entity id="P81-1032.7">task-specific domain knowledge</entity> (in addition to <entity id="P81-1032.8">general linguistic knowledge</entity>) to bear on both <entity id="P81-1032.9">grammatical and ungrammatical input</entity>. A <entity id="P81-1032.10">parsing algorithm</entity> is presented that integrates several different <entity id="P81-1032.11">parsing strategies</entity>, with <entity id="P81-1032.12">case-frame instantiation</entity> dominating. Each of these <entity id="P81-1032.13">parsing strategies</entity> exploits different <entity id="P81-1032.14">types of knowledge</entity>; and their combination provides a strong framework in which to process <entity id="P81-1032.15">conjunctions</entity>, <entity id="P81-1032.16">fragmentary input</entity>, and <entity id="P81-1032.17">ungrammatical structures</entity>, as well as less exotic, <entity id="P81-1032.18">grammatically correct input</entity>. Several <entity id="P81-1032.19">specific heuristics</entity> for handling <entity id="P81-1032.20">ungrammatical input</entity> are presented within this <entity id="P81-1032.21">multi-strategy framework</entity>. </abstract>



</text>

<text id="P85-1015"> 

<title> Parsing with Discontinuous Constituents </title>
<abstract>
By generalizing the notion of <entity id="P85-1015.1">location of a constituent</entity> to allow <entity id="P85-1015.2">discontinuous locations</entity>, one can describe the <entity id="P85-1015.3">discontinuous constituents</entity> of <entity id="P85-1015.4">non-configurational languages</entity>. These <entity id="P85-1015.5">discontinuous constituents</entity> can be described by a variant of <entity id="P85-1015.6">definite clause grammars</entity>, and these <entity id="P85-1015.7">grammars</entity> can be used in conjunction with a <entity id="P85-1015.8">proof procedure</entity> to create a <entity id="P85-1015.9">parser for non-configurational languages</entity>. </abstract>



</text>

<text id="P91-1016"> 

<title>The Acquisition and Application of Context Sensitive Grammar for English </title>
<abstract>
 A system is described for acquiring a <entity id="P91-1016.1">context-sensitive, phrase structure grammar</entity> which is applied by a <entity id="P91-1016.2">best-path, bottom-up, deterministic parser</entity>. The <entity id="P91-1016.3">grammar</entity> was based on <entity id="P91-1016.4">English news stories</entity> and a high degree of success in <entity id="P91-1016.5">parsing</entity> is reported. Overall, this research concludes that <entity id="P91-1016.6">CSG</entity> is a computationally and conceptually tractable approach to the construction of <entity id="P91-1016.7">phrase structure grammar</entity> for <entity id="P91-1016.8">news story text</entity>. </abstract>



</text>

<text id="P95-1027"> 

<title>A Quantitative Evaluation of Linguistic Tests for the Automatic Prediction of Semantic Markedness </title>
<abstract>
We present a <entity id="P95-1027.1">corpus-based study</entity> of methods that have been proposed in the <entity id="P95-1027.2">linguistics literature</entity> for selecting the <entity id="P95-1027.3">semantically unmarked term</entity> out of a pair of <entity id="P95-1027.4">antonymous adjectives</entity>. Solutions to this problem are applicable to the more general task of selecting the positive <entity id="P95-1027.5">term</entity> from the pair. Using <entity id="P95-1027.6">automatically collected data</entity>, the <entity id="P95-1027.7">accuracy</entity> and applicability of each method is quantified, and a <entity id="P95-1027.8">statistical analysis</entity> of the significance of the results is performed. We show that some simple methods are indeed good indicators for the answer to the problem while other proposed methods fail to perform better than would be attributable to chance. In addition, one of the simplest methods, <entity id="P95-1027.9">text frequency</entity>, dominates all others. We also apply two <entity id="P95-1027.10">generic statistical learning methods</entity> for combining the indications of the individual methods, and compare their performance to the simple methods. The most sophisticated <entity id="P95-1027.11">complex learning method</entity> offers a small, but statistically significant, improvement over the original tests. </abstract>



</text>

<text id="P97-1015"> 

<title>Probing the lexicon in evaluating commercial MT systems </title>
<abstract>
 In the past the evaluation of <entity id="P97-1015.1">machine translation systems</entity> has focused on single system evaluations because there were only few systems available. But now there are several commercial systems for the same <entity id="P97-1015.2">language pair</entity>. This requires new methods of comparative evaluation. In the paper we propose a <entity id="P97-1015.3">black-box method</entity> for comparing the <entity id="P97-1015.4">lexical coverage</entity> of <entity id="P97-1015.5">MT systems</entity>. The method is based on lists of <entity id="P97-1015.6">words</entity> from different <entity id="P97-1015.7">frequency classes</entity>. It is shown how these <entity id="P97-1015.8">word lists</entity> can be compiled and used for testing. We also present the results of using our method on 6 <entity id="P97-1015.9">MT systems</entity> that translate between <entity id="P97-1015.10">English</entity> and <entity id="P97-1015.11">German</entity>. </abstract>



</text>

<text id="P97-1052"> 

<title>On Interpreting F-Structures as UDRSs </title>
<abstract>
We describe a method for interpreting <entity id="P97-1052.1">abstract flat syntactic representations, LFG f-structures</entity>, as <entity id="P97-1052.2">underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs)</entity>. The method establishes a <entity id="P97-1052.3">one-to-one correspondence</entity> between subsets of the <entity id="P97-1052.4">LFG</entity> and <entity id="P97-1052.5">UDRS</entity> formalisms. It provides a <entity id="P97-1052.6">model theoretic interpretation</entity> and an <entity id="P97-1052.7">inferential component</entity> which operates directly on <entity id="P97-1052.8">underspecified representations</entity> for <entity id="P97-1052.9">f-structures</entity> through the <entity id="P97-1052.10">translation images</entity> of <entity id="P97-1052.11">f-structures</entity> as <entity id="P97-1052.12">UDRSs</entity>. </abstract>



</text>

<text id="P99-1025"> 

<title> Construct Algebra : Analytical Dialog Management </title>
<abstract>
In this paper we describe a systematic approach for creating a <entity id="P99-1025.1">dialog management system</entity> based on a <entity id="P99-1025.2">Construct Algebra</entity>, a <entity id="P99-1025.3">collection of relations and operations</entity> on a <entity id="P99-1025.4">task representation</entity>. These <entity id="P99-1025.5">relations and operations</entity> are <entity id="P99-1025.6">analytical components</entity> for building higher level abstractions called <entity id="P99-1025.7">dialog motivators</entity>. The <entity id="P99-1025.8">dialog manager</entity>, consisting of a <entity id="P99-1025.9">collection of dialog motivators</entity>, is entirely built using the <entity id="P99-1025.10">Construct Algebra</entity>. </abstract>



</text>

<text id="P99-1068"> 

<title>Mining the Web for Bilingual Text </title>
<abstract>
<entity id="P99-1068.1">STRAND</entity> (Resnik, 1998) is a <entity id="P99-1068.2">language-independent system</entity> for <entity id="P99-1068.3">automatic discovery of text</entity> in <entity id="P99-1068.4">parallel translation</entity> on the World Wide Web. This paper extends the preliminary <entity id="P99-1068.5">STRAND</entity> results by adding <entity id="P99-1068.6">automatic language identification</entity>, scaling up by orders of magnitude, and formally evaluating performance. The most recent end-product is an <entity id="P99-1068.7">automatically acquired parallel corpus</entity> comprising 2491 <entity id="P99-1068.8">English-French document pairs</entity>, approximately 1.5 million <entity id="P99-1068.9">words</entity> per <entity id="P99-1068.10">language</entity>. </abstract>



</text>

<text id="L08-1260">
<title> Verb-Noun Collocation SyntLex Dictionary : Corpus-Based Approach </title>
<abstract>
The project presented here is a part of a long term research program aiming at a full <entity id="L08-1260.1">lexicon grammar for Polish (SyntLex)</entity>. The main of this project is <entity id="L08-1260.2">computer-assisted acquisition and morpho-syntactic description of verb-noun collocations</entity> in <entity id="L08-1260.3">Polish</entity>. We present methodology and resources obtained in three main project phases which are: <entity id="L08-1260.4">dictionary-based acquisition</entity> of <entity id="L08-1260.5">collocation lexicon</entity>, feasibility study for <entity id="L08-1260.6">corpus-based lexicon enlargement</entity> phase, <entity id="L08-1260.7">corpus-based lexicon enlargement</entity> and <entity id="L08-1260.8">collocation description</entity>. In this paper we focus on the results of the third phase. The presented here <entity id="L08-1260.9">corpus-based approach</entity> permitted us to triple the size the <entity id="L08-1260.10">verb-noun collocation dictionary for Polish</entity>. In the paper we describe the <entity id="L08-1260.11">SyntLex Dictionary of Collocations</entity> and announce some future research intended to be a separate project continuation.
</abstract>



</text>

<text id="L08-1540">
<title> Czech MWE Database </title>
<abstract>
In this paper we deal with a recently developed <entity id="L08-1540.1">large Czech MWE database</entity> containing at the moment 160 000 <entity id="L08-1540.2">MWEs</entity> (treated as <entity id="L08-1540.3">lexical units</entity>). It was compiled from various resources such as <entity id="L08-1540.4">encyclopedias</entity> and <entity id="L08-1540.5">dictionaries</entity>, public <entity id="L08-1540.6">databases</entity> of <entity id="L08-1540.7">proper names</entity> and <entity id="L08-1540.8">toponyms</entity>, <entity id="L08-1540.9">collocations</entity> obtained from <entity id="L08-1540.10">Czech WordNet</entity>, lists of <entity id="L08-1540.11">botanical and zoological terms</entity> and others. We describe the structure of the <entity id="L08-1540.12">database</entity> and give basic types of <entity id="L08-1540.13">MWEs</entity> according to domains they belong to. We compare the built <entity id="L08-1540.14">MWEs database</entity> with the <entity id="L08-1540.15">corpus data</entity> from <entity id="L08-1540.16">Czech National Corpus</entity> (approx. 100 mil. tokens) and present results of this comparison in the paper. These <entity id="L08-1540.17">MWEs</entity> have not been obtained from the <entity id="L08-1540.18">corpus</entity> since their frequencies in it are rather low. To obtain a more complete list of <entity id="L08-1540.19">MWEs</entity> we propose and use a technique exploiting the <entity id="L08-1540.20">Word Sketch Engine</entity>, which allows us to work with <entity id="L08-1540.21">statistical parameters</entity> such as frequency of <entity id="L08-1540.22">MWEs</entity> and their components as well as with the <entity id="L08-1540.23">salience</entity> for the whole <entity id="L08-1540.24">MWEs</entity>. We also discuss exploitation of the <entity id="L08-1540.25">database</entity> for working out a more adequate <entity id="L08-1540.26">tagging</entity> and <entity id="L08-1540.27">lemmatization</entity>. The final goal is to be able to recognize <entity id="L08-1540.28">MWEs</entity> in <entity id="L08-1540.29">corpus text</entity> and lemmatize them as complete <entity id="L08-1540.30">lexical units</entity>, i. e. to make <entity id="L08-1540.31">tagging</entity> and <entity id="L08-1540.32">lemmatization</entity> more adequate.
</abstract>



</text>

<text id="L08-1110">
<title>Using Log-linear Models for Tuning Machine Translation Output </title>
<abstract>
We describe a set of experiments to explore <entity id="L08-1110.1">statistical techniques</entity> for ranking and selecting the best <entity id="L08-1110.2">translations</entity> in a <entity id="L08-1110.3">graph</entity> of <entity id="L08-1110.4">translation hypotheses</entity>. In a previous paper (Carl, 2007) we have described how the <entity id="L08-1110.5">hypotheses graph</entity> is generated through <entity id="L08-1110.6">shallow mapping</entity> and <entity id="L08-1110.7">permutation rules</entity>. We have given examples of its <entity id="L08-1110.8">nodes</entity> consisting of <entity id="L08-1110.9">vectors representing morpho-syntactic properties</entity> of <entity id="L08-1110.10">words</entity> and <entity id="L08-1110.11">phrases</entity>. This paper describes a number of methods for elaborating <entity id="L08-1110.12">statistical feature functions</entity> from some of the <entity id="L08-1110.13">vector components</entity>. The <entity id="L08-1110.14">feature functions</entity> are trained off-line on different types of <entity id="L08-1110.15">text</entity> and their <entity id="L08-1110.16">log-linear combination</entity> is then used to retrieve the best M <entity id="L08-1110.17">translation paths</entity> in the <entity id="L08-1110.18">graph</entity>. We compare two <entity id="L08-1110.19">language modelling toolkits</entity>, the <entity id="L08-1110.20">CMU</entity> and the <entity id="L08-1110.21">SRI toolkit</entity> and arrive at three results: 1) <entity id="L08-1110.22">word-lemma based feature function models</entity> produce better results than <entity id="L08-1110.23">token-based models</entity>, 2) adding a <entity id="L08-1110.24">PoS-tag feature function</entity> to the <entity id="L08-1110.25">word-lemma model</entity> improves the output and 3) <entity id="L08-1110.26">weights</entity> for <entity id="L08-1110.27">lexical translations</entity> are suitable if the <entity id="L08-1110.28">training material</entity> is similar to the <entity id="L08-1110.29">texts</entity> to be translated.</abstract>



</text>

<text id="L08-1154">
<title> Chinese Term Extraction Based on Delimiters </title>
<abstract>
Existing techniques extract <entity id="L08-1154.1">term candidates</entity> by looking for <entity id="L08-1154.2">internal and contextual information</entity> associated with <entity id="L08-1154.3">domain specific terms</entity>. The algorithms always face the dilemma that fewer <entity id="L08-1154.4">features</entity> are not enough to distinguish <entity id="L08-1154.5">terms</entity> from <entity id="L08-1154.6">non-terms</entity> whereas more <entity id="L08-1154.7">features</entity> lead to more conflicts among selected <entity id="L08-1154.8">features</entity>. This paper presents a novel approach for <entity id="L08-1154.9">term extraction</entity> based on <entity id="L08-1154.10">delimiters</entity> which are much more stable and domain independent. The proposed approach is not as sensitive to <entity id="L08-1154.11">term frequency</entity> as that of previous works. This approach has no strict limit or <entity id="L08-1154.12">hard rules</entity> and thus they can deal with all kinds of <entity id="L08-1154.13">terms</entity>. It also requires no prior <entity id="L08-1154.14">domain knowledge</entity> and no additional <entity id="L08-1154.15">training</entity> to adapt to new <entity id="L08-1154.16">domains</entity>. Consequently, the proposed approach can be applied to different <entity id="L08-1154.17">domains</entity> easily and it is especially useful for <entity id="L08-1154.18">resource-limited domains</entity>. Evaluations conducted on two different <entity id="L08-1154.19">domains</entity> for <entity id="L08-1154.20">Chinese term extraction</entity> show significant improvements over existing techniques which verifies its efficiency and domain independent nature. Experiments on <entity id="L08-1154.21">new term extraction</entity> indicate that the proposed approach can also serve as an effective tool for <entity id="L08-1154.22">domain lexicon expansion</entity>.
</abstract>



</text>

<text id="L08-1050">
<title>From Sentence to Discourse : Building an Annotation Scheme for Discourse Based on Prague Dependency Treebank </title>
<abstract>
The present paper reports on a preparatory research for building a <entity id="L08-1050.1">language corpus annotation scenario</entity> capturing the <entity id="L08-1050.2">discourse relations</entity> in <entity id="L08-1050.3">Czech</entity>. We primarily focus on the description of the <entity id="L08-1050.4">syntactically motivated relations</entity> in <entity id="L08-1050.5">discourse</entity>, basing our findings on the theoretical background of the <entity id="L08-1050.6">Prague Dependency Treebank 2.0</entity> and the <entity id="L08-1050.7">Penn Discourse Treebank 2</entity>. Our aim is to revisit the present-day <entity id="L08-1050.8">syntactico-semantic (tectogrammatical) annotation</entity> in the <entity id="L08-1050.9">Prague Dependency Treebank</entity>, extend it for the purposes of a <entity id="L08-1050.10">sentence-boundary-crossing representation</entity> and eventually to design a new, <entity id="L08-1050.11">discourse level</entity> of <entity id="L08-1050.12">annotation</entity>. In this paper, we propose a feasible process of such a transfer, comparing the possibilities the <entity id="L08-1050.13">Praguian dependency-based approach</entity> offers with the <entity id="L08-1050.14">Penn discourse annotation</entity> based primarily on the analysis and classification of <entity id="L08-1050.15">discourse connectives</entity>.
</abstract>



</text>

<text id="L08-1097">
<title> Unsupervised Acquisition of Verb Subcategorization Frames from Shallow-Parsed Corpora </title>
<abstract>
In this paper, we reported experiments of <entity id="L08-1097.1">unsupervised automatic acquisition</entity> of <entity id="L08-1097.2">Italian and English verb subcategorization frames (SCFs)</entity> from <entity id="L08-1097.3">general and domain corpora</entity>. The proposed technique operates on <entity id="L08-1097.4">syntactically shallow-parsed corpora</entity> on the basis of a limited number of <entity id="L08-1097.5">search heuristics</entity> not relying on any previous <entity id="L08-1097.6">lexico-syntactic knowledge</entity> about <entity id="L08-1097.7">SCFs</entity>. Although preliminary, reported results are in line with <entity id="L08-1097.8">state-of-the-art lexical acquisition systems</entity>. The issue of whether <entity id="L08-1097.9">verbs</entity> sharing similar <entity id="L08-1097.10">SCFs distributions</entity> happen to share <entity id="L08-1097.11">similar semantic properties</entity> as well was also explored by clustering <entity id="L08-1097.12">verbs</entity> that share <entity id="L08-1097.13">frames</entity> with the same <entity id="L08-1097.14">distribution</entity> using the <entity id="L08-1097.15">Minimum Description Length Principle (MDL)</entity>. First experiments in this direction were carried out on <entity id="L08-1097.16">Italian verbs</entity> with encouraging results.
</abstract>



</text>

<text id="N04-2005">
<title>A Multi-Path Architecture For Machine Translation Of English Text Into American Sign Language Animation </title>
<abstract>
The <entity id="N04-2005.1">translation</entity> of <entity id="N04-2005.2">English text</entity> into <entity id="N04-2005.3">American Sign Language (ASL) animation</entity> tests the limits of <entity id="N04-2005.4">traditional MT architectural designs</entity>. A new <entity id="N04-2005.5">semantic representation</entity> is proposed that uses <entity id="N04-2005.6">virtual reality 3D scene modeling software</entity> to produce <entity id="N04-2005.7">spatially complex ASL phenomena</entity> called "<entity id="N04-2005.8">classifier predicates</entity>." The model acts as an <entity id="N04-2005.9">interlingua</entity> within a new <entity id="N04-2005.10">multi-pathway MT architecture design</entity> that also incorporates <entity id="N04-2005.11">transfer</entity> and <entity id="N04-2005.12">direct approaches</entity> into a single system.
</abstract>



</text>

<text id="A92-1010">
<title>Integrating Natural Language Components Into Graphical Discourse </title>
<abstract>
In our current research into the design of <entity id="A92-1010.1">cognitively well-motivated interfaces</entity> relying primarily on the <entity id="A92-1010.2">display of graphical information</entity>, we have observed that <entity id="A92-1010.3">graphical information</entity> alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users' expectations. This can occur due to too much <entity id="A92-1010.4">information</entity> being requested, too little, <entity id="A92-1010.5">information</entity> of the wrong kind, etc. To solve this problem, we are working towards the integration of <entity id="A92-1010.6">natural language generation</entity> to augment the <entity id="A92-1010.7">interaction</entity></abstract>



</text>

<text id="H94-1064">
<title>The LIMSI Continuous Speech Dictation System </title>
<abstract>
A major axis of research at LIMSI is directed at <entity id="H94-1064.1">multilingual, speaker-independent, large vocabulary speech dictation</entity>. In this paper the <entity id="H94-1064.2">LIMSI recognizer</entity> which was evaluated in the <entity id="H94-1064.3">ARPA NOV93 CSR test</entity> is described, and experimental results on the <entity id="H94-1064.4">WSJ and BREF corpora</entity> under closely matched conditions are reported. For both <entity id="H94-1064.5">corpora</entity> <entity id="H94-1064.6">word recognition experiments</entity> were carried out with <entity id="H94-1064.7">vocabularies</entity> containing up to 20k <entity id="H94-1064.8">words</entity>. The recognizer makes use of <entity id="H94-1064.9">continuous density HMM</entity> with <entity id="H94-1064.10">Gaussian mixture</entity> for <entity id="H94-1064.11">acoustic modeling</entity> and <entity id="H94-1064.12">n-gram statistics</entity> estimated on the <entity id="H94-1064.13">newspaper texts</entity> for <entity id="H94-1064.14">language modeling</entity>. The recognizer uses a <entity id="H94-1064.15">time-synchronous graph-search strategy</entity> which is shown to still be viable with a 20k-word vocabulary when used with <entity id="H94-1064.16">bigram back-off language models</entity>. A second <entity id="H94-1064.17">forward pass</entity>, which makes use of a <entity id="H94-1064.18">word graph</entity> generated with the <entity id="H94-1064.19">bigram</entity>, incorporates a <entity id="H94-1064.20">trigram language model</entity>. <entity id="H94-1064.21">Acoustic modeling</entity> uses <entity id="H94-1064.22">cepstrum-based features</entity>, <entity id="H94-1064.23">context-dependent phone models (intra and interword)</entity>, <entity id="H94-1064.24">phone duration models</entity>, and <entity id="H94-1064.25">sex-dependent models</entity>.
</abstract>



</text>

<text id="A00-1024">
<title>
Categorizing Unknown Words : Using Decision Trees To Identify Names And Misspellings </title>
<abstract>
This paper introduces a <entity id="A00-1024.1">system for categorizing unknown words</entity>. The <entity id="A00-1024.2">system</entity> is based on a <entity id="A00-1024.3">multi-component architecture</entity> where each <entity id="A00-1024.4">component</entity> is responsible for identifying one class of <entity id="A00-1024.5">unknown words</entity>. The focus of this paper is the <entity id="A00-1024.6">components</entity> that identify <entity id="A00-1024.7">names</entity> and <entity id="A00-1024.8">spelling errors</entity>. Each <entity id="A00-1024.9">component</entity> uses a <entity id="A00-1024.10">decision tree architecture</entity> to combine multiple types of <entity id="A00-1024.11">evidence</entity> about the <entity id="A00-1024.12">unknown word</entity>. The <entity id="A00-1024.13">system</entity> is evaluated using data from <entity id="A00-1024.14">live closed captions</entity> - a genre replete with a wide variety of <entity id="A00-1024.15">unknown words</entity>.
</abstract>



</text>

<text id="X96-1059">
<title>NEC Corporation And University Of Sheffield: Description Of NEC/Sheffleld System Used For MET Japanese </title>
<abstract>
<entity id="X96-1059.1">Recognition of proper nouns</entity> in <entity id="X96-1059.2">Japanese text</entity> has been studied as a part of the more general problem of <entity id="X96-1059.3">morphological analysis</entity> in <entity id="X96-1059.4">Japanese text processing</entity> ([1] [2]). It has also been studied in the framework of <entity id="X96-1059.5">Japanese information extraction</entity> ([3]) in recent years. Our approach to the Multi-lingual Evaluation Task (MET) for <entity id="X96-1059.6">Japanese text</entity> is to consider the given task as a <entity id="X96-1059.7">morphological analysis problem</entity> in <entity id="X96-1059.8">Japanese</entity>. Our <entity id="X96-1059.9">morphological analyzer</entity> has done all the necessary work for the <entity id="X96-1059.10">recognition and classification of proper names, numerical and temporal expressions, i.e. Named Entity (NE) items</entity> in the <entity id="X96-1059.11">Japanese text</entity>. The <entity id="X96-1059.12">analyzer</entity> is called "Amorph". Amorph recognizes <entity id="X96-1059.13">NE items</entity> in two stages: <entity id="X96-1059.14">dictionary lookup</entity> and <entity id="X96-1059.15">rule application</entity>. First, it uses several kinds of <entity id="X96-1059.16">dictionaries</entity> to segment and tag <entity id="X96-1059.17">Japanese character strings</entity>. Second, based on the information resulting from the <entity id="X96-1059.18">dictionary lookup stage</entity>, a set of <entity id="X96-1059.19">rules</entity> is applied to the <entity id="X96-1059.20">segmented strings</entity> in order to identify <entity id="X96-1059.21">NE items</entity>. When a <entity id="X96-1059.22">segment</entity> is found to be an <entity id="X96-1059.23">NE item</entity>, this information is added to the <entity id="X96-1059.24">segment</entity> and it is used to generate the final output.
</abstract>



</text>

<text id="H05-1041">
<title>A Practically Unsupervised Learning Method To Identify Single-Snippet Answers To Definition Questions On The Web</title>
<abstract>
We present a <entity id="H05-1041.1">practically unsupervised learning method</entity> to produce <entity id="H05-1041.2">single-snippet answers</entity> to <entity id="H05-1041.3">definition questions</entity> in <entity id="H05-1041.4">question answering systems</entity> that supplement <entity id="H05-1041.5">Web search engines</entity>. The method exploits <entity id="H05-1041.6">on-line encyclopedias and dictionaries</entity> to generate automatically an arbitrarily large number of <entity id="H05-1041.7">positive and negative definition examples</entity>, which are then used to train an <entity id="H05-1041.8">svm</entity> to separate the two classes. We show experimentally that the proposed method is viable, that it outperforms the alternative of training the <entity id="H05-1041.9">system</entity> on <entity id="H05-1041.10">questions</entity> and <entity id="H05-1041.11">news articles from trec</entity>, and that it helps the <entity id="H05-1041.12">search engine</entity> handle <entity id="H05-1041.13">definition questions</entity> significantly better.
</abstract>



</text>

<text id="W02-1403">
<title> Lexically-Based Terminology Structuring : Some Inherent Limits</title>
<abstract>
<entity id="W02-1403.1">Terminology structuring</entity> has been the subject of much work in the context of <entity id="W02-1403.2">terms</entity> extracted from <entity id="W02-1403.3">corpora</entity>: given a set of <entity id="W02-1403.4">terms</entity>, obtained from an existing resource or extracted from a <entity id="W02-1403.5">corpus</entity>, identifying <entity id="W02-1403.6">hierarchical (or other types of) relations</entity> between these <entity id="W02-1403.7">terms</entity>. The present paper focusses on <entity id="W02-1403.8">terminology structuring</entity> by <entity id="W02-1403.9">lexical methods</entity>, which match <entity id="W02-1403.10">terms</entity> on the basis on their <entity id="W02-1403.11">content words</entity>, taking <entity id="W02-1403.12">morphological variants</entity> into account. Experiments are done on a 'flat' list of <entity id="W02-1403.13">terms</entity> obtained from an originally <entity id="W02-1403.14">hierarchically-structured terminology</entity>: the French version of the <entity id="W02-1403.15">US National Library of Medicine MeSH thesaurus</entity>. We compare the <entity id="W02-1403.16">lexically-induced relations</entity> with the original <entity id="W02-1403.17">MeSH relations</entity>: after a quantitative evaluation of their congruence through <entity id="W02-1403.18">recall and precision metrics</entity>, we perform a qualitative, human analysis ofthe 'new' <entity id="W02-1403.19">relations</entity> not present in the <entity id="W02-1403.20">MeSH</entity>. This analysis shows, on the one hand, the limits of the <entity id="W02-1403.21">lexical structuring method</entity>. On the other hand, it also reveals some specific structuring choices and <entity id="W02-1403.22">naming conventions</entity> made by the <entity id="W02-1403.23">MeSH</entity> designers, and emphasizes ontological commitments that cannot be left to <entity id="W02-1403.24">automatic structuring</entity>.
</abstract>



</text>

<text id="W02-1404">
<title> Alignment And Extraction Of Bilingual Legal Terminology From Context Profiles </title>
<abstract>
In this study, we propose a <entity id="W02-1404.1">knowledge-independent method</entity> for aligning <entity id="W02-1404.2">terms</entity> and thus extracting <entity id="W02-1404.3">translations</entity> from a <entity id="W02-1404.4">small, domain-specific corpus</entity> consisting of <entity id="W02-1404.5">parallel English and Chinese court judgments</entity> from Hong Kong. With a <entity id="W02-1404.6">sentence-aligned corpus</entity>, <entity id="W02-1404.7">translation equivalences</entity> are suggested by analysing the <entity id="W02-1404.8">frequency profiles</entity> of <entity id="W02-1404.9">parallel concordances</entity>. The method overcomes the limitations of <entity id="W02-1404.10">conventional statistical methods</entity> which require <entity id="W02-1404.11">large corpora</entity> to be effective, and <entity id="W02-1404.12">lexical approaches</entity> which depend on existing <entity id="W02-1404.13">bilingual dictionaries</entity>. Pilot testing on a <entity id="W02-1404.14">parallel corpus</entity> of about 113K <entity id="W02-1404.15">Chinese words</entity> and 120K <entity id="W02-1404.16">English words</entity> gives an encouraging 85% <entity id="W02-1404.17">precision</entity> and 45% <entity id="W02-1404.18">recall</entity>. Future work includes fine-tuning the <entity id="W02-1404.19">algorithm</entity> upon the analysis of the errors, and acquiring a <entity id="W02-1404.20">translation lexicon</entity> for <entity id="W02-1404.21">legal terminology</entity> by filtering out <entity id="W02-1404.22">general terms</entity>.
</abstract>



</text>

<text id="W02-1602">
<title> Coedition To Share Text Revision Across Languages And Improve MT A Posteriori</title>
<abstract>
<entity id="W02-1602.1">Coedition</entity> of a <entity id="W02-1602.2">natural language text</entity> and its representation in some <entity id="W02-1602.3">interlingual form</entity> seems the best and simplest way to share <entity id="W02-1602.4">text revision</entity> across <entity id="W02-1602.5">languages</entity>. For various reasons, <entity id="W02-1602.6">UNL graphs</entity> are the best candidates in this context. We are developing a <entity id="W02-1602.7">prototype</entity> where, in the simplest <entity id="W02-1602.8">sharing scenario</entity>, naive users interact directly with the <entity id="W02-1602.9">text</entity> in their <entity id="W02-1602.10">language (L0)</entity>, and indirectly with the associated <entity id="W02-1602.11">graph</entity>. The modified <entity id="W02-1602.12">graph</entity> is then sent to the <entity id="W02-1602.13">UNL-L0 deconverter</entity> and the result shown. If is is satisfactory, the errors were probably due to the <entity id="W02-1602.14">graph</entity>, not to the <entity id="W02-1602.15">deconverter</entity>, and the <entity id="W02-1602.16">graph</entity> is sent to <entity id="W02-1602.17">deconverters</entity> in other <entity id="W02-1602.18">languages</entity>. Versions in some other <entity id="W02-1602.19">languages</entity> known by the user may be displayed, so that improvement sharing is visible and encouraging. As new versions are added with appropriate <entity id="W02-1602.20">tags</entity> and <entity id="W02-1602.21">attributes</entity> in the <entity id="W02-1602.22">original multilingual document</entity>, nothing is ever lost, and cooperative working on a <entity id="W02-1602.23">document</entity> is rendered feasible. On the internal side, liaisons are established between elements of the <entity id="W02-1602.24">text</entity> and the <entity id="W02-1602.25">graph</entity> by using broadly available resources such as a <entity id="W02-1602.26">LO-English or better a L0-UNL dictionary</entity>, a <entity id="W02-1602.27">morphosyntactic parser of L0</entity>, and a <entity id="W02-1602.28">canonical graph2tree transformation</entity>. Establishing a "best" correspondence between the "<entity id="W02-1602.29">UNL-tree+L0</entity>" and the "<entity id="W02-1602.30">MS-L0 structure</entity>", a <entity id="W02-1602.31">lattice</entity>, may be done using the <entity id="W02-1602.32">dictionary</entity> and trying to align the <entity id="W02-1602.33">tree</entity> and the selected <entity id="W02-1602.34">trajectory</entity> with as few <entity id="W02-1602.35">crossing liaisons</entity> as possible. A central goal of this research is to merge approaches from <entity id="W02-1602.36">pivot MT</entity>, <entity id="W02-1602.37">interactive MT</entity>, and <entity id="W02-1602.38">multilingual text authoring</entity>.
</abstract>



</text>

<text id="W03-0406">

<title>
Unsupervised Learning Of Word Sense Disambiguation Rules By Estimating An Optimum Iteration Number In The EM Algorithm </title>
<abstract>
In this paper, we improve an <entity id="W03-0406.1">unsupervised learning method</entity> using the <entity id="W03-0406.2">Expectation-Maximization (EM) algorithm</entity> proposed by Nigam et al. for <entity id="W03-0406.3">text classification problems</entity> in order to apply it to <entity id="W03-0406.4">word sense disambiguation (WSD) problems</entity>. The improved method stops the <entity id="W03-0406.5">EM algorithm</entity> at the <entity id="W03-0406.6">optimum iteration number</entity>. To estimate that number, we propose two methods. In experiments, we solved 50 <entity id="W03-0406.7">noun WSD problems</entity> in the <entity id="W03-0406.8">Japanese Dictionary Task in SENSEVAL2</entity>. The score of our method is a match for the best public score of this task. Furthermore, our methods were confirmed to be effective also for <entity id="W03-0406.9">verb WSD problems</entity>.
</abstract>



</text>

<text id="W99-0408">

<title>Modeling User Language Proficiency In A Writing Tutor For Deaf Learners Of English </title>
<abstract>
In this paper we discuss a proposed <entity id="W99-0408.1">user knowledge modeling architecture</entity> for the <entity id="W99-0408.2">ICICLE system</entity>, a <entity id="W99-0408.3">language tutoring application</entity> for deaf learners of <entity id="W99-0408.4">written English</entity>. The model will represent the <entity id="W99-0408.5">language proficiency</entity> of the user and is designed to be referenced during both <entity id="W99-0408.6">writing analysis</entity> and <entity id="W99-0408.7">feedback production</entity>. We motivate our <entity id="W99-0408.8">model design</entity> by citing relevant research on <entity id="W99-0408.9">second language and cognitive skill acquisition</entity>, and briefly discuss preliminary empirical evidence supporting the <entity id="W99-0408.10">design</entity>. We conclude by showing how our <entity id="W99-0408.11">design</entity> can provide a rich and <entity id="W99-0408.12">robust information base</entity> to a language assessment / correction application by modeling <entity id="W99-0408.13">user proficiency</entity> at a high level of granularity and specificity.
</abstract>



</text>

<text id="P98-1083">

<title>
Using Decision Trees to Construct a Practical Parser </title>
<abstract>
This paper describes novel and practical <entity id="P98-1083.1">Japanese parsers</entity> that uses <entity id="P98-1083.2">decision trees</entity>. First, we construct a single <entity id="P98-1083.3">decision tree</entity> to estimate <entity id="P98-1083.4">modification probabilities</entity>; how one <entity id="P98-1083.5">phrase</entity> tends to modify another. Next, we introduce a <entity id="P98-1083.6">boosting algorithm</entity> in which several <entity id="P98-1083.7">decision trees</entity> are constructed and then combined for <entity id="P98-1083.8">probability estimation</entity>. The two constructed <entity id="P98-1083.9">parsers</entity> are evaluated by using the <entity id="P98-1083.10">EDR Japanese annotated corpus</entity>. The single-tree method outperforms the <entity id="P98-1083.11">conventional Japanese stochastic methods</entity> by 4%. Moreover, the boosting version is shown to have significant advantages; 1) better <entity id="P98-1083.12">parsing accuracy</entity> than its single-tree counterpart for any amount of <entity id="P98-1083.13">training data</entity> and 2) no <entity id="P98-1083.14">over-fitting to data</entity> for various <entity id="P98-1083.15">iterations</entity>.
</abstract>



</text>

<text id="I08-1027">

<title>
 Automatic Estimation of Word Significance oriented for Speech-based Information Retrieval </title>
<abstract>
<entity id="I08-1027.1">Automatic estimation</entity> of <entity id="I08-1027.2">word significance</entity> oriented for <entity id="I08-1027.3">speech-based Information Retrieval (IR)</entity> is addressed. Since the <entity id="I08-1027.4">significance</entity> of <entity id="I08-1027.5">words</entity> differs in <entity id="I08-1027.6">IR</entity>, <entity id="I08-1027.7">automatic speech recognition (ASR) performance</entity> has been evaluated based on <entity id="I08-1027.8">weighted word error rate (WWER)</entity>, which gives a <entity id="I08-1027.9">weight</entity> on errors from the viewpoint of <entity id="I08-1027.10">IR</entity>, instead of <entity id="I08-1027.11">word error rate (WER)</entity>, which treats all <entity id="I08-1027.12">words</entity> uniformly. A <entity id="I08-1027.13">decoding strategy</entity> that minimizes <entity id="I08-1027.14">WWER</entity> based on a <entity id="I08-1027.15">Minimum Bayes-Risk framework</entity> has been shown, and the reduction of errors on both <entity id="I08-1027.16">ASR</entity> and <entity id="I08-1027.17">IR</entity> has been reported. In this paper, we propose an <entity id="I08-1027.18">automatic estimation method</entity> for <entity id="I08-1027.19">word significance (weights)</entity> based on its influence on <entity id="I08-1027.20">IR</entity>. Specifically, <entity id="I08-1027.21">weights</entity> are estimated so that <entity id="I08-1027.22">evaluation measures</entity> of <entity id="I08-1027.23">ASR</entity> and <entity id="I08-1027.24">IR</entity> are equivalent. We apply the proposed method to a <entity id="I08-1027.25">speech-based information retrieval system</entity>, which is a typical <entity id="I08-1027.26">IR system</entity>, and show that the method works well.
</abstract>



</text>

<text id="I08-1043">

<title>
Paraphrasing Depending on Bilingual Context Toward Generalization of Translation Knowledge </title>
<abstract>
This study presents a <entity id="I08-1043.1">method to automatically acquire paraphrases</entity> using <entity id="I08-1043.2">bilingual corpora</entity>, which utilizes the <entity id="I08-1043.3">bilingual dependency relations</entity> obtained by projecting a <entity id="I08-1043.4">monolingual dependency parse</entity> onto the other language sentence based on <entity id="I08-1043.5">statistical alignment techniques</entity>. Since the <entity id="I08-1043.6">paraphrasing method</entity> is capable of clearly disambiguating the <entity id="I08-1043.7">sense</entity> of an original <entity id="I08-1043.8">phrase</entity> using the <entity id="I08-1043.9">bilingual context</entity> of <entity id="I08-1043.10">dependency relation</entity>, it would be possible to obtain interchangeable <entity id="I08-1043.11">paraphrases</entity> under a given <entity id="I08-1043.12">context</entity>. Also, we provide an advanced method to acquire <entity id="I08-1043.13">generalized translation knowledge</entity> using the extracted <entity id="I08-1043.14">paraphrases</entity>. We applied the method to acquire the <entity id="I08-1043.15">generalized translation knowledge</entity> for <entity id="I08-1043.16">Korean-English translation</entity>. Through experiments with <entity id="I08-1043.17">parallel corpora</entity> of a <entity id="I08-1043.18">Korean and English language pairs</entity>, we show that our <entity id="I08-1043.19">paraphrasing method</entity> effectively extracts <entity id="I08-1043.20">paraphrases</entity> with high <entity id="I08-1043.21">precision</entity>, 94.3% and 84.6% respectively for <entity id="I08-1043.22">Korean</entity> and <entity id="I08-1043.23">English</entity>, and the <entity id="I08-1043.24">translation knowledge</entity> extracted from the <entity id="I08-1043.25">bilingual corpora</entity> could be generalized successfully using the <entity id="I08-1043.26">paraphrases</entity> with the 12.5% <entity id="I08-1043.27">compression ratio</entity>.
</abstract>



</text>

<text id="W04-1307">
<title>
 Statistics Learning And Universal Grammar : Modeling Word Segmentation </title>
<abstract>
This paper describes a <entity id="W04-1307.1">computational model</entity> of <entity id="W04-1307.2">word segmentation</entity> and presents simulation results on <entity id="W04-1307.3">realistic acquisition</entity>. In particular, we explore the capacity and limitations of <entity id="W04-1307.4">statistical learning mechanisms</entity> that have recently gained prominence in <entity id="W04-1307.5">cognitive psychology</entity> and <entity id="W04-1307.6">linguistics</entity>.
</abstract>



</text>

<text id="W04-2204">
<title>
Automatic Construction Of A Transfer Dictionary Considering Directionality </title>
<abstract>
In this paper, we show how to construct a <entity id="W04-2204.1">transfer dictionary</entity> automatically. <entity id="W04-2204.2">Dictionary construction</entity>, one of the most difficult tasks in developing a <entity id="W04-2204.3">machine translation system</entity>, is expensive. To avoid this problem, we investigate how we build a <entity id="W04-2204.4">dictionary</entity> using existing <entity id="W04-2204.5">linguistic resources</entity>. Our <entity id="W04-2204.6">algorithm</entity> can be applied to any <entity id="W04-2204.7">language pairs</entity>, but for the present we focus on building a <entity id="W04-2204.8">Korean-to-Japanese dictionary</entity> using <entity id="W04-2204.9">English</entity> as a <entity id="W04-2204.10">pivot</entity>. We attempt three ways of <entity id="W04-2204.11">automatic construction</entity> to corroborate the effect of the <entity id="W04-2204.12">directionality</entity> of <entity id="W04-2204.13">dictionaries</entity>. First, we introduce <entity id="W04-2204.14">"one-time look up" method</entity> using a <entity id="W04-2204.15">Korean-to-English and a Japanese-to-English dictionary</entity>. Second, we show a method using <entity id="W04-2204.16">"overlapping constraint"</entity> with a <entity id="W04-2204.17">Korean-to-English dictionary</entity> and an <entity id="W04-2204.18">English-to-Japanese dictionary</entity>. Third, we consider another alternative method rarely used for building a <entity id="W04-2204.19">dictionary</entity>: an <entity id="W04-2204.20">English-to-Korean dictionary</entity> and <entity id="W04-2204.21">English-to-Japanese dictionary</entity>. We found that the first method is the most effective and the best result can be obtained from combining the three methods.
</abstract>



</text>

<text id="W04-2703">
<title>
Annotating Discourse Connectives And Their Arguments</title>
<abstract>
This paper describes a new, <entity id="W04-2703.1">large scale discourse-level annotation</entity> project - the <entity id="W04-2703.2">Penn Discourse TreeBank (PDTB)</entity>. We present an approach to annotating a level of <entity id="W04-2703.3">discourse structure</entity> that is based on identifying <entity id="W04-2703.4">discourse connectives</entity> and their <entity id="W04-2703.5">arguments</entity>. The <entity id="W04-2703.6">PDTB</entity> is being built directly on top of the <entity id="W04-2703.7">Penn TreeBank</entity> and <entity id="W04-2703.8">Propbank</entity>, thus supporting the extraction of useful <entity id="W04-2703.9">syntactic and semantic features</entity> and providing a richer substrate for the development and evaluation of <entity id="W04-2703.10">practical algorithms</entity>. We provide a detailed preliminary analysis of <entity id="W04-2703.11">inter-annotator agreement</entity> - both the <entity id="W04-2703.12">level of agreement</entity> and the types of <entity id="W04-2703.13">inter-annotator variation</entity>.
</abstract>



</text>

<text id="W05-1308">
<title>
INTEX : A Syntactic Role Driven Protein-Protein Interaction Extractor For Bio-Medical Text </title>
<abstract>
In this paper, we present a <entity id="W05-1308.1">fully automated extraction system</entity>, named <entity id="W05-1308.2">IntEx</entity>, to identify <entity id="W05-1308.3">gene and protein interactions</entity> in <entity id="W05-1308.4">biomedical text</entity>. Our approach is based on first splitting <entity id="W05-1308.5">complex sentences</entity> into <entity id="W05-1308.6">simple clausal structures</entity> made up of <entity id="W05-1308.7">syntactic roles</entity>. Then, tagging <entity id="W05-1308.8">biological entities</entity> with the help of <entity id="W05-1308.9">biomedical and linguistic ontologies</entity>. Finally, extracting <entity id="W05-1308.10">complete interactions</entity> by analyzing the matching contents of <entity id="W05-1308.11">syntactic roles</entity> and their linguistically significant combinations. Our <entity id="W05-1308.12">extraction system</entity> handles <entity id="W05-1308.13">complex sentences</entity> and extracts <entity id="W05-1308.14">multiple and nested interactions</entity> specified in a <entity id="W05-1308.15">sentence</entity>. Experimental evaluations with two other state of the art <entity id="W05-1308.16">extraction systems</entity> indicate that the <entity id="W05-1308.17">IntEx system</entity> achieves better <entity id="W05-1308.18">performance</entity> without the labor intensive <entity id="W05-1308.19">pattern engineering requirement</entity>.
</abstract>



</text>

<text id="W06-1605">

<title>
 Distributional Measures Of Concept- Distance : A Task-Oriented Evaluation</title>
<abstract>
We propose a framework to derive the <entity id="W06-1605.1">distance</entity> between <entity id="W06-1605.2">concepts</entity> from <entity id="W06-1605.3">distributional measures of word co-occurrences</entity>. We use the <entity id="W06-1605.4">categories</entity> in a published <entity id="W06-1605.5">thesaurus</entity> as <entity id="W06-1605.6">coarse-grained concepts</entity>, allowing all possible <entity id="W06-1605.7">distance values</entity> to be stored in a <entity id="W06-1605.8">concept-concept matrix</entity> roughly.01% the size of that created by existing measures. We show that the newly proposed <entity id="W06-1605.9">concept-distance measures</entity> outperform <entity id="W06-1605.10">traditional distributional word-distance measures</entity> in the tasks of (1) ranking <entity id="W06-1605.11">word pairs</entity> in order of <entity id="W06-1605.12">semantic distance</entity>, and (2) correcting <entity id="W06-1605.13">real-word spelling errors</entity>. In the latter task, of all the <entity id="W06-1605.14">WordNet-based measures</entity>, only that proposed by Jiang and Conrath outperforms the best <entity id="W06-1605.15">distributional concept-distance measures</entity>.
</abstract>



</text>

<text id="W07-0208">
<title>
Learning to Transform Linguistic Graphs </title>
<abstract>
We argue in favor of the the use of <entity id="W07-0208.1">labeled directed graph</entity> to represent various types of <entity id="W07-0208.2">linguistic structures</entity>, and illustrate how this allows one to view <entity id="W07-0208.3">NLP tasks</entity> as <entity id="W07-0208.4">graph transformations</entity>. We present a general method for learning such <entity id="W07-0208.5">transformations</entity> from an <entity id="W07-0208.6">annotated corpus</entity> and describe experiments with two applications of the method: <entity id="W07-0208.7">identification of non-local depenencies</entity> (using <entity id="W07-0208.8">Penn Treebank data</entity>) and <entity id="W07-0208.9">semantic role labeling</entity> (using <entity id="W07-0208.10">Proposition Bank data</entity>).
</abstract>



</text>

<text id="P08-1105">
<title>
Credibility Improves Topical Blog Post Retrieval </title>
<abstract>
<entity id="P08-1105.1">Topical blog post retrieval</entity> is the task of ranking <entity id="P08-1105.2">blog posts</entity> with respect to their <entity id="P08-1105.3">relevance</entity> for a given <entity id="P08-1105.4">topic</entity>. To improve <entity id="P08-1105.5">topical blog post retrieval</entity> we incorporate <entity id="P08-1105.6">textual credibility indicators</entity> in the <entity id="P08-1105.7">retrieval process</entity>. We consider two groups of <entity id="P08-1105.8">indicators</entity>: post level (determined using information about individual <entity id="P08-1105.9">blog posts</entity> only) and blog level (determined using information from the underlying <entity id="P08-1105.10">blogs</entity>). We describe how to estimate these <entity id="P08-1105.11">indicators</entity> and how to integrate them into a <entity id="P08-1105.12">retrieval approach</entity> based on <entity id="P08-1105.13">language models</entity>. Experiments on the <entity id="P08-1105.14">TREC Blog track test set</entity> show that both groups of <entity id="P08-1105.15">credibility indicators</entity> significantly improve <entity id="P08-1105.16">retrieval effectiveness</entity>; the best performance is achieved when combining them.
</abstract>



</text>

<text id="P08-2034">
<title>
Lyric-based Song Sentiment Classification with Sentiment Vector Space Model </title><abstract>
<entity id="P08-2034.1">Lyric-based song sentiment classification</entity> seeks to assign songs appropriate <entity id="P08-2034.2">sentiment labels</entity> such as light-hearted heavy-hearted. Four problems render <entity id="P08-2034.3">vector space model (VSM)-based text classification approach</entity> ineffective: 1) Many <entity id="P08-2034.4">words</entity> within <entity id="P08-2034.5">song lyrics</entity> actually contribute little to <entity id="P08-2034.6">sentiment</entity>; 2) <entity id="P08-2034.7">Nouns</entity> and <entity id="P08-2034.8">verbs</entity> used to express <entity id="P08-2034.9">sentiment</entity> are ambiguous; 3) <entity id="P08-2034.10">Negations</entity> and <entity id="P08-2034.11">modifiers</entity> around the <entity id="P08-2034.12">sentiment keywords</entity> make particular contributions to <entity id="P08-2034.13">sentiment</entity>; 4) <entity id="P08-2034.14">Song lyric</entity> is usually very short. To address these problems, the <entity id="P08-2034.15">sentiment vector space model (s-VSM)</entity> is proposed to represent <entity id="P08-2034.16">song lyric document</entity>. The preliminary experiments prove that the <entity id="P08-2034.17">s-VSM model</entity> outperforms the <entity id="P08-2034.18">VSM model</entity> in the <entity id="P08-2034.19">lyric-based song sentiment classification task</entity>. 
</abstract>



</text>

<text id="C08-1118">

<title>
Source Language Markers in EUROPARL Translations </title>
<abstract>
This paper shows that it is very often possible to identify the <entity id="C08-1118.1">source language</entity> of medium-length speeches in the <entity id="C08-1118.2">EUROPARL corpus</entity> on the basis of <entity id="C08-1118.3">frequency counts</entity> of <entity id="C08-1118.4">word n-grams</entity> (87.2%-96.7% <entity id="C08-1118.5">accuracy</entity> depending on <entity id="C08-1118.6">classification method</entity>). The paper also examines in detail which <entity id="C08-1118.7">positive markers</entity> are most powerful and identifies a number of linguistic aspects as well as culture- and domain-related ones.
</abstract>



</text>

<text id="C08-1128">
<title>
Bayesian Semi-Supervised Chinese Word Segmentation for Statistical Machine Translation </title>
<abstract>
<entity id="C08-1128.1">Words</entity> in <entity id="C08-1128.2">Chinese text</entity> are not naturally separated by <entity id="C08-1128.3">delimiters</entity>, which poses a challenge to <entity id="C08-1128.4">standard machine translation (MT) systems</entity>. In <entity id="C08-1128.5">MT</entity>, the widely used approach is to apply a <entity id="C08-1128.6">Chinese word segmenter</entity> trained from <entity id="C08-1128.7">manually annotated data</entity>, using a fixed <entity id="C08-1128.8">lexicon</entity>. Such <entity id="C08-1128.9">word segmentation</entity> is not necessarily optimal for <entity id="C08-1128.10">translation</entity>. We propose a <entity id="C08-1128.11">Bayesian semi-supervised Chinese word segmentation model</entity> which uses both <entity id="C08-1128.12">monolingual and bilingual information</entity> to derive a <entity id="C08-1128.13">segmentation</entity> suitable for <entity id="C08-1128.14">MT</entity>. Experiments show that our method improves a <entity id="C08-1128.15">state-of-the-art MT system</entity> in a small and a <entity id="C08-1128.16">large data environment</entity>.
</abstract>



</text>

<text id="C08-2010">
<title>
The Impact of Reference Quality on Automatic MT Evaluation </title>
<abstract>
<entity id="C08-2010.1">Language resource quality</entity> is crucial in <entity id="C08-2010.2">NLP</entity>. Many of the resources used are derived from data created by human beings out of an <entity id="C08-2010.3">NLP</entity> context, especially regarding <entity id="C08-2010.4">MT</entity> and <entity id="C08-2010.5">reference translations</entity>. Indeed, <entity id="C08-2010.6">automatic evaluations</entity> need <entity id="C08-2010.7">high-quality data</entity> that allow the comparison of both <entity id="C08-2010.8">automatic and human translations</entity>. The validation of these resources is widely recommended before being used. This paper describes the impact of using <entity id="C08-2010.9">different-quality references</entity> on <entity id="C08-2010.10">evaluation</entity>. Surprisingly enough, similar scores are obtained in many cases regardless of the quality. Thus, the limitations of the <entity id="C08-2010.11">automatic metrics</entity> used within <entity id="C08-2010.12">MT</entity> are also discussed in this regard.
</abstract>



</text>

<text id="C08-3010">

<title>
A Linguistic Knowledge Discovery Tool : Very Large Ngram Database Search with Arbitrary Wildcards </title>
<abstract>
In this paper, we will describe a <entity id="C08-3010.1">search tool</entity> for a huge set of <entity id="C08-3010.2">ngrams</entity>. The tool supports <entity id="C08-3010.3">queries</entity> with an arbitrary number of <entity id="C08-3010.4">wildcards</entity>. It takes a fraction of a second for a search, and can provide the <entity id="C08-3010.5">fillers</entity> of the <entity id="C08-3010.6">wildcards</entity>. The system runs on a single Linux PC with reasonable size <entity id="C08-3010.7">memory</entity> (less than 4GB) and <entity id="C08-3010.8">disk space</entity> (less than 400GB). This system can be a very useful tool for <entity id="C08-3010.9">linguistic knowledge discovery</entity> and other <entity id="C08-3010.10">NLP tasks</entity>.
</abstract>



</text>

<text id="W03-2907">
<title>
Unsupervised Learning of Bulgarian POS Tags </title>
<abstract>
This paper presents an approach to the <entity id="W03-2907.1">unsupervised learning</entity> of <entity id="W03-2907.2">parts of speech</entity> which uses both <entity id="W03-2907.3">morphological and syntactic information</entity>. While the <entity id="W03-2907.4">model</entity> is more complex than those which have been employed for <entity id="W03-2907.5">unsupervised learning</entity> of <entity id="W03-2907.6">POS tags in English</entity>, which use only <entity id="W03-2907.7">syntactic information</entity>, the variety of <entity id="W03-2907.8">languages</entity> in the world requires that we consider <entity id="W03-2907.9">morphology</entity> as well. In many <entity id="W03-2907.10">languages</entity>, <entity id="W03-2907.11">morphology</entity> provides better clues to a word's category than <entity id="W03-2907.12">word order</entity>. We present the <entity id="W03-2907.13">computational model</entity> for <entity id="W03-2907.14">POS learning</entity>, and present results for applying it to <entity id="W03-2907.15">Bulgarian</entity>, a <entity id="W03-2907.16">Slavic language</entity> with relatively <entity id="W03-2907.17">free word order</entity> and <entity id="W03-2907.18">rich morphology</entity>.
</abstract>



</text>

<text id="W08-2122">

<title>
A Latent Variable Model of Synchronous Parsing for Syntactic and Semantic Dependencies </title>
<abstract>
We propose a solution to the challenge of the <entity id="W08-2122.1">CoNLL 2008 shared task</entity> that uses a <entity id="W08-2122.2">generative history-based latent variable model</entity> to predict the most likely <entity id="W08-2122.3">derivation</entity> of a <entity id="W08-2122.4">synchronous dependency parser</entity> for both <entity id="W08-2122.5">syntactic and semantic dependencies</entity>. The submitted <entity id="W08-2122.6">model</entity> yields 79.1% <entity id="W08-2122.7">macro-average F1 performance</entity>, for the joint task, 86.9% <entity id="W08-2122.8">syntactic dependencies LAS</entity> and 71.0% <entity id="W08-2122.9">semantic dependencies F1</entity>. A larger <entity id="W08-2122.10">model</entity> trained after the deadline achieves 80.5% <entity id="W08-2122.11">macro-average F1</entity>, 87.6% <entity id="W08-2122.12">syntactic dependencies LAS</entity>, and 73.1% <entity id="W08-2122.13">semantic dependencies F1</entity>.
</abstract>



</text>

<text id="P03-1034">

<title>
Integrating Discourse Markers Into A Pipelined Natural Language Generation Architecture </title>
<abstract>
<entity id="P03-1034.1">Pipelined Natural Language Generation (NLG) systems</entity> have grown increasingly complex as <entity id="P03-1034.2">architectural modules</entity> were added to support <entity id="P03-1034.3">language functionalities</entity> such as <entity id="P03-1034.4">referring expressions</entity>, <entity id="P03-1034.5">lexical choice</entity>, and <entity id="P03-1034.6">revision</entity>. This has given rise to discussions about the relative placement of these new <entity id="P03-1034.7">modules</entity> in the overall <entity id="P03-1034.8">architecture</entity>. Recent work on another aspect of <entity id="P03-1034.9">multi-paragraph text</entity>, <entity id="P03-1034.10">discourse markers</entity>, indicates it is time to consider where a <entity id="P03-1034.11">discourse marker insertion algorithm</entity> fits in. We present examples which suggest that in a <entity id="P03-1034.12">pipelined NLG architecture</entity>, the best approach is to strongly tie it to a <entity id="P03-1034.13">revision component</entity>. Finally, we evaluate the approach in a working <entity id="P03-1034.14">multi-page system</entity>.
</abstract>



</text>

<text id="P06-1088">

<title>
 Multi-Tagging For Lexicalized-Grammar Parsing </title>
<abstract>
With performance above 97% <entity id="P06-1088.1">accuracy</entity> for <entity id="P06-1088.2">newspaper text</entity>, <entity id="P06-1088.3">part of speech (pos) tagging</entity> might be considered a solved problem. Previous studies have shown that allowing the <entity id="P06-1088.4">parser</entity> to resolve <entity id="P06-1088.5">pos tag ambiguity</entity> does not improve performance. However, for <entity id="P06-1088.6">grammar formalisms</entity> which use more <entity id="P06-1088.7">fine-grained grammatical categories</entity>, for example <entity id="P06-1088.8">tag</entity> and <entity id="P06-1088.9">ccg</entity>, <entity id="P06-1088.10">tagging accuracy</entity> is much lower. In fact, for these <entity id="P06-1088.11">formalisms</entity>, premature <entity id="P06-1088.12">ambiguity resolution</entity> makes <entity id="P06-1088.13">parsing</entity> infeasible. We describe a <entity id="P06-1088.14">multi-tagging approach</entity> which maintains a suitable level of <entity id="P06-1088.15">lexical category ambiguity</entity> for accurate and efficient <entity id="P06-1088.16">ccg parsing</entity>. We extend this <entity id="P06-1088.17">multi-tagging approach</entity> to the <entity id="P06-1088.18">pos level</entity> to overcome errors introduced by automatically assigned <entity id="P06-1088.19">pos tags</entity>. Although <entity id="P06-1088.20">pos tagging accuracy</entity> seems high, maintaining some <entity id="P06-1088.21">pos tag ambiguity</entity> in the <entity id="P06-1088.22">language processing pipeline</entity> results in more accurate <entity id="P06-1088.23">ccg supertagging</entity>.
</abstract>



</text>

<text id="P06-3008">

<title>
 Discursive Usage Of Six Chinese Punctuation Marks </title>
<abstract>
Both <entity id="P06-3008.1">rhetorical structure</entity> and <entity id="P06-3008.2">punctuation</entity> have been helpful in <entity id="P06-3008.3">discourse processing</entity>. Based on a <entity id="P06-3008.4">corpus annotation project</entity>, this paper reports the <entity id="P06-3008.5">discursive usage</entity> of 6 <entity id="P06-3008.6">Chinese punctuation marks</entity> in <entity id="P06-3008.7">news commentary texts</entity>: <entity id="P06-3008.8">Colon</entity>, <entity id="P06-3008.9">Dash</entity>, <entity id="P06-3008.10">Ellipsis</entity>, <entity id="P06-3008.11">Exclamation Mark</entity>, <entity id="P06-3008.12">Question Mark</entity>, and <entity id="P06-3008.13">Semicolon</entity>. The <entity id="P06-3008.14">rhetorical patterns</entity> of these marks are compared against <entity id="P06-3008.15">patterns</entity> around <entity id="P06-3008.16">cue phrases</entity> in general. Results show that these <entity id="P06-3008.17">Chinese punctuation marks</entity>, though fewer in number than <entity id="P06-3008.18">cue phrases</entity>, are easy to identify, have strong correlation with certain relations, and can be used as distinctive indicators of nuclearity in <entity id="P06-3008.19">Chinese texts</entity>.
</abstract>



</text>

<text id="C90-3007">

<title> Partial Descriptions And Systemic Grammar</title>
<abstract>
This paper examines the properties of <entity id="C90-3007.1">feature-based partial descriptions</entity> built on top of <entity id="C90-3007.2">Halliday's systemic networks</entity>. We show that the crucial operation of <entity id="C90-3007.3">consistency checking</entity> for such descriptions is NP-complete, and therefore probably intractable, but proceed to develop <entity id="C90-3007.4">algorithms</entity> which can sometimes alleviate the unpleasant consequences of this <entity id="C90-3007.5">intractability</entity>.
</abstract>



</text>

<text id="C94-1091">

<title> Classifier Assignment By Corpus-Based Approach</title>
<abstract>
This paper presents an algorithm for selecting an appropriate <entity id="C94-1091.1">classifier word</entity> for a <entity id="C94-1091.2">noun</entity>. In <entity id="C94-1091.3">Thai language</entity>, it frequently happens that there is fluctuation in the choice of <entity id="C94-1091.4">classifier</entity> for a given <entity id="C94-1091.5">concrete noun</entity>, both from the point of view of the whole <entity id="C94-1091.6">speech community</entity> and <entity id="C94-1091.7">individual speakers</entity>. Basically, there is no exact rule for <entity id="C94-1091.8">classifier selection</entity>. As far as we can do in the <entity id="C94-1091.9">rule-based approach</entity> is to give a <entity id="C94-1091.10">default rule</entity> to pick up a corresponding <entity id="C94-1091.11">classifier</entity> of each <entity id="C94-1091.12">noun</entity>. Registration of <entity id="C94-1091.13">classifier</entity> for each <entity id="C94-1091.14">noun</entity> is limited to the <entity id="C94-1091.15">type of unit classifier</entity> because other types are open due to the meaning of representation. We propose a <entity id="C94-1091.16">corpus-based method</entity> (Biber,1993; Nagao,1993; Smadja,1993) which generates <entity id="C94-1091.17">Noun Classifier Associations (NCA)</entity> to overcome the problems in <entity id="C94-1091.18">classifier assignment</entity> and <entity id="C94-1091.19">semantic construction of noun phrase</entity>. The <entity id="C94-1091.20">NCA</entity> is created statistically from a large <entity id="C94-1091.21">corpus</entity> and recomposed under <entity id="C94-1091.22">concept hierarchy constraints</entity> and <entity id="C94-1091.23">frequency of occurrences</entity>. 
</abstract>



</text>

<text id="C02-1120">

<title>
An Unsupervised Learning Method For Associative Relationships Between Verb Phrases </title>
<abstract>
This paper describes an <entity id="C02-1120.1">unsupervised learning method</entity> for <entity id="C02-1120.2">associative relationships between verb phrases</entity>, which is important in developing reliable <entity id="C02-1120.3">Q&amp;A systems</entity>. Consider the situation that a user gives a <entity id="C02-1120.4">query</entity> "How much petrol was imported to Japan from Saudi Arabia?" to a <entity id="C02-1120.5">Q&amp;A system</entity>, but the <entity id="C02-1120.6">text</entity> given to the system includes only the <entity id="C02-1120.7">description</entity> "X tonnes of petrol was conveyed to Japan from Saudi Arabia". We think that the <entity id="C02-1120.8">description</entity> is a good clue to find the answer for our <entity id="C02-1120.9">query</entity>, "X tonnes". But there is no <entity id="C02-1120.10">large-scale database</entity> that provides the <entity id="C02-1120.11">associative relationship</entity> between "imported" and "conveyed". Our aim is to develop an <entity id="C02-1120.12">unsupervised learning method</entity> that can obtain such an <entity id="C02-1120.13">associative relationship</entity>, which we call <entity id="C02-1120.14">scenario consistency</entity>. The method we are currently working on uses an <entity id="C02-1120.15">expectation-maximization (EM) based word-clustering algorithm</entity>, and we have evaluated the effectiveness of this method using <entity id="C02-1120.16">Japanese verb phrases</entity>. 
</abstract>



</text>

<text id="C04-1022">

<title> Automatic Learning Of Language Model Structure </title>
<abstract>
<entity id="C04-1022.1">Statistical language modeling</entity> remains a challenging task, in particular for <entity id="C04-1022.2">morphologically rich languages</entity>. Recently, new approaches based on <entity id="C04-1022.3">factored language models</entity> have been developed to address this problem. These <entity id="C04-1022.4">models</entity> provide principled ways of including additional <entity id="C04-1022.5">conditioning variables</entity> other than the <entity id="C04-1022.6">preceding words</entity>, such as <entity id="C04-1022.7">morphological or syntactic features</entity>. However, the number of possible choices for <entity id="C04-1022.8">model parameters</entity> creates a <entity id="C04-1022.9">large space of models</entity> that cannot be searched exhaustively. This paper presents an <entity id="C04-1022.10">entirely data-driven model selection procedure</entity> based on <entity id="C04-1022.11">genetic search</entity>, which is shown to outperform both <entity id="C04-1022.12">knowledge-based and random selection procedures</entity> on two different <entity id="C04-1022.13">language modeling tasks</entity> (<entity id="C04-1022.14">Arabic</entity> and <entity id="C04-1022.15">Turkish</entity>).
</abstract>



</text>

<text id="E85-1004">
<title>
 Montagovian Definite Clause Grammar </title>
<abstract>
This paper reports a completed stage of ongoing research at the University of York. Landsbergen's advocacy of <entity id="E85-1004.1">analytical inverses</entity> for <entity id="E85-1004.2">compositional syntax rules</entity> encourages the application of <entity id="E85-1004.3">Definite Clause Grammar techniques</entity> to the construction of a <entity id="E85-1004.4">parser</entity> returning <entity id="E85-1004.5"> Montague analysis trees</entity>. A <entity id="E85-1004.6">parser MDCC</entity> is presented which implements an <entity id="E85-1004.7">augmented Friedman - Warren algorithm</entity> permitting <entity id="E85-1004.8">post referencing</entity>* and interfaces with a language of <entity id="E85-1004.9">intenslonal logic translator LILT</entity> so as to display the <entity id="E85-1004.10">derivational history</entity> of corresponding <entity id="E85-1004.11">reduced IL formulae</entity>. Some familiarity with <entity id="E85-1004.12">Montague's PTQ</entity> and the <entity id="E85-1004.13">basic DCG mechanism</entity> is assumed.
</abstract>



</text>

<text id="E89-1040">

<title>
An Approach To Sentence-Level Anaphora In Machine Translation </title>
<abstract>
Theoretical research in the area of <entity id="E89-1040.1">machine translation</entity> usually involves the search for and creation of an appropriate <entity id="E89-1040.2">formalism</entity>. An important issue in this respect is the way in which the <entity id="E89-1040.3">compositionality</entity> of <entity id="E89-1040.4">translation</entity> is to be defined. In this paper, we will introduce the <entity id="E89-1040.5">anaphoric component</entity> of the <entity id="E89-1040.6">Mimo formalism</entity>. It makes the definition and <entity id="E89-1040.7">translation</entity> of <entity id="E89-1040.8">anaphoric relations</entity> possible, <entity id="E89-1040.9">relations</entity> which are usually problematic for systems that adhere to <entity id="E89-1040.10">strict compositionality</entity>. In <entity id="E89-1040.11">Mimo</entity>, the <entity id="E89-1040.12">translation</entity> of <entity id="E89-1040.13">anaphoric relations</entity> is compositional. The <entity id="E89-1040.14">anaphoric component</entity> is used to define <entity id="E89-1040.15">linguistic phenomena</entity> such as <entity id="E89-1040.16">wh-movement</entity>, the <entity id="E89-1040.17">passive</entity> and the <entity id="E89-1040.18">binding of reflexives and pronouns</entity> mono-lingually. The actual working of the component will be shown in this paper by means of a detailed discussion of <entity id="E89-1040.19">wh-movement</entity>.
</abstract>



</text>

<text id="C96-1062">

<title> Interpretation Of Nominal Compounds : Combining Domain-Independent And Domain-Specific Information </title>
<abstract>
A <entity id="C96-1062.1">domain independent model</entity> is proposed for the <entity id="C96-1062.2">automated interpretation</entity> of <entity id="C96-1062.3">nominal compounds</entity> in <entity id="C96-1062.4">English</entity>. This <entity id="C96-1062.5">model</entity> is meant to account for <entity id="C96-1062.6">productive rules of interpretation</entity> which are inferred from the <entity id="C96-1062.7">morpho-syntactic and semantic characteristics</entity> of the <entity id="C96-1062.8">nominal constituents</entity>. In particular, we make extensive use of Pustejovsky's principles concerning the <entity id="C96-1062.9">predicative information</entity> associated with <entity id="C96-1062.10">nominals</entity>. We argue that it is necessary to draw a line between <entity id="C96-1062.11">generalizable semantic principles</entity> and <entity id="C96-1062.12">domain-specific semantic information</entity>. We explain this distinction and we show how this model may be applied to the <entity id="C96-1062.13">interpretation</entity> of <entity id="C96-1062.14">compounds</entity> in <entity id="C96-1062.15">real texts</entity>, provided that complementary <entity id="C96-1062.16">semantic information</entity> are retrieved.
</abstract>



</text>

<text id="P05-1018"><title>Modeling Local Coherence: An Entity-Based Approach</title>
<abstract>
This paper considers the problem of automatic assessment of <entity id="P05-1018.1">local coherence</entity>. We present a novel <entity id="P05-1018.2">entity-based representation</entity> of <entity id="P05-1018.3">discourse</entity> which is inspired by <entity id="P05-1018.4">Centering Theory</entity> and can be computed automatically from <entity id="P05-1018.5">raw text</entity>. We view <entity id="P05-1018.6">coherence assessment</entity> as a <entity id="P05-1018.7">ranking learning problem</entity> and show that the proposed <entity id="P05-1018.8">discourse representation</entity> supports the effective learning of a <entity id="P05-1018.9">ranking function</entity>. Our experiments demonstrate that the <entity id="P05-1018.10">induced model</entity> achieves significantly higher <entity id="P05-1018.11">accuracy</entity> than a <entity id="P05-1018.12">state-of-the-art coherence model</entity>.
</abstract>


</text>

<text id="P05-1056"><title>
Using Conditional Random Fields For Sentence Boundary Detection In Speech</title>
<abstract>
<entity id="P05-1056.1">Sentence boundary detection</entity> in <entity id="P05-1056.2">speech</entity> is important for enriching <entity id="P05-1056.3">speech recognition</entity> output, making it easier for humans to read and downstream modules to process. In previous work, we have developed <entity id="P05-1056.4">hidden Markov model (HMM) and maximum entropy (Maxent) classifiers</entity> that integrate textual and prosodic <entity id="P05-1056.5">knowledge sources</entity> for detecting <entity id="P05-1056.6">sentence boundaries</entity>. In this paper, we evaluate the use of a <entity id="P05-1056.7">conditional random field (CRF)</entity> for this task and relate results with this model to our prior work. We evaluate across two corpora (conversational <entity id="P05-1056.8">telephone speech</entity> and <entity id="P05-1056.9">broadcast news speech</entity>) on both <entity id="P05-1056.10">human transcriptions</entity> and <entity id="P05-1056.11">speech recognition</entity> output. In general, our <entity id="P05-1056.12">CRF</entity> model yields a lower error rate than the <entity id="P05-1056.13">HMM and Max-ent models</entity> on the <entity id="P05-1056.14">NIST sentence boundary detection task</entity> in <entity id="P05-1056.15">speech</entity>, although it is interesting to note that the best results are achieved by <entity id="P05-1056.16">three-way voting</entity> among the <entity id="P05-1056.17">classifiers</entity>. This probably occurs because each <entity id="P05-1056.18">model</entity> has different strengths and weaknesses for modeling the <entity id="P05-1056.19">knowledge sources</entity>.
</abstract>


</text>

<text id="P05-2008"><title>
Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification</title>
<abstract>
<entity id="P05-2008.1">Sentiment Classification</entity> seeks to identify a piece of <entity id="P05-2008.2">text</entity> according to its author's general feeling toward their <entity id="P05-2008.3">subject</entity>, be it positive or negative. Traditional <entity id="P05-2008.4">machine learning techniques</entity> have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the <entity id="P05-2008.5">training and test data</entity> with respect to <entity id="P05-2008.6">topic</entity>. This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with <entity id="P05-2008.7">training data</entity> labeled with <entity id="P05-2008.8">emoticons</entity>, which has the potential of being independent of <entity id="P05-2008.9">domain</entity>, <entity id="P05-2008.10">topic</entity> and time.
</abstract>


</text>

<text id="I05-2043"><title>Trend Survey on <entity id="I05-2043.1">Japanese Natural Language Processing</entity> Studies over the Last Decade
</title><abstract>
Using <entity id="I05-2043.2">natural language processing</entity>, we carried out a trend survey on <entity id="I05-2043.3">Japanese natural language processing studies</entity> that have been done over the last ten years. We determined the changes in the number of papers published for each research organization and on each research area as well as the relationship between research organizations and research areas. This paper is useful for both recognizing trends in <entity id="I05-2043.4">Japanese NLP</entity> and constructing a method of supporting trend surveys using <entity id="I05-2043.5">NLP</entity>.
</abstract>


</text>

<text id="E99-1034"><title>
Finding Content-Bearing Terms Using Term Similarities</title>
<abstract>
This paper explores the issue of using different <entity id="E99-1034.1">co-occurrence similarities</entity> between <entity id="E99-1034.2">terms</entity> for separating <entity id="E99-1034.3">query terms</entity> that are useful for <entity id="E99-1034.4">retrieval</entity> from those that are harmful. The hypothesis under examination is that <entity id="E99-1034.5">useful terms</entity> tend to be more similar to each other than to other <entity id="E99-1034.6">query terms</entity>. Preliminary experiments with similarities computed using <entity id="E99-1034.7">first-order and second-order co-occurrence</entity> seem to confirm the hypothesis. <entity id="E99-1034.8">Term similarities</entity> could then be used for determining which <entity id="E99-1034.9">query terms</entity> are useful and best reflect the user's information need. A possible application would be to use this source of evidence for tuning the <entity id="E99-1034.10">weights</entity> of the <entity id="E99-1034.11">query terms</entity>.
</abstract>


</text>

<text id="E85-1041"><title>
The Structure Of Communicative Context Of Dialogue Interaction</title>
<abstract>
We propose a draft scheme of the <entity id="E85-1041.1">model</entity> formalizing the <entity id="E85-1041.2">structure of communicative context</entity> in <entity id="E85-1041.3">dialogue interaction</entity>. The relationships between the interacting partners are considered as system of three automata representing the partners of the <entity id="E85-1041.4">dialogue</entity> and environment.
</abstract>


</text>

<text id="E91-1012"><title>
Non-Deterministic Recursive Ascent Parsing</title>
<abstract>
A purely functional implementation of <entity id="E91-1012.1">LR-parsers</entity> is given, together with a simple <entity id="E91-1012.2">correctness proof</entity>. It is presented as a generalization of the <entity id="E91-1012.3">recursive descent parser</entity>. For <entity id="E91-1012.4">non-LR grammars</entity> the time-complexity of our <entity id="E91-1012.5">parser</entity> is cubic if the functions that constitute the <entity id="E91-1012.6">parser</entity> are implemented as <entity id="E91-1012.7">memo-functions</entity>, i.e. functions that memorize the results of previous invocations. <entity id="E91-1012.8">Memo-functions</entity> also facilitate a simple way to construct a very compact representation of the <entity id="E91-1012.9">parse forest</entity>. For <entity id="E91-1012.10">LR(0) grammars</entity>, our algorithm is closely related to the <entity id="E91-1012.11">recursive ascent parsers</entity> recently discovered by Kruse-man Aretz [1] and Roberts [2]. <entity id="E91-1012.12">Extended CF grammars</entity> (<entity id="E91-1012.13">grammars</entity> with <entity id="E91-1012.14">regular expressions</entity> at the right hand side) can be parsed with a simple modification of the <entity id="E91-1012.15">LR-parser</entity> for normal <entity id="E91-1012.16">CF grammars</entity>.
</abstract>



</text>

<text id="P05-1010"><title>
Probabilistic CFG With Latent Annotations
</title><abstract>
This paper defines a <entity id="P05-1010.1">generative probabilistic model</entity> of <entity id="P05-1010.2">parse trees</entity>, which we call <entity id="P05-1010.3">PCFG-LA</entity>. This <entity id="P05-1010.4">model</entity> is an extension of <entity id="P05-1010.5">PCFG</entity> in which <entity id="P05-1010.6">non-terminal symbols</entity> are augmented with <entity id="P05-1010.7">latent variables</entity>. Finegrained <entity id="P05-1010.8">CFG rules</entity> are automatically induced from a <entity id="P05-1010.9">parsed corpus</entity> by <entity id="P05-1010.10">training</entity> a <entity id="P05-1010.11">PCFG-LA model</entity> using an <entity id="P05-1010.12">EM-algorithm</entity>. Because exact <entity id="P05-1010.13">parsing</entity> with a <entity id="P05-1010.14">PCFG-LA</entity> is <entity id="P05-1010.15">NP-hard</entity>, several <entity id="P05-1010.16">approximations</entity> are described and empirically compared. In experiments using the <entity id="P05-1010.17">Penn WSJ corpus</entity>, our automatically trained <entity id="P05-1010.18">model</entity> gave a <entity id="P05-1010.19">performance</entity> of 86.6% (F1, <entity id="P05-1010.20">sentences</entity> &lt; 40 <entity id="P05-1010.21">words</entity>), which is comparable to that of an <entity id="P05-1010.22">unlexicalized PCFG parser</entity> created using extensive <entity id="P05-1010.23">manual feature selection</entity>.
</abstract>



</text>

<text id="P05-1053"><title>
Exploring Various Knowledge In Relation Extraction</title>
<abstract>
<entity id="P05-1053.1">Extracting semantic relationships between entities</entity> is challenging. This paper investigates the incorporation of diverse <entity id="P05-1053.2">lexical, syntactic and semantic knowledge</entity> in <entity id="P05-1053.3">feature-based relation extraction</entity> using <entity id="P05-1053.4">SVM</entity>. Our study illustrates that the base <entity id="P05-1053.5">phrase chunking</entity> information is very effective for <entity id="P05-1053.6">relation extraction</entity> and contributes to most of the <entity id="P05-1053.7">performance improvement</entity> from <entity id="P05-1053.8">syntactic</entity> aspect while additional information from <entity id="P05-1053.9">full parsing</entity> gives limited further enhancement. This suggests that most of useful information in <entity id="P05-1053.10">full parse trees</entity> for <entity id="P05-1053.11">relation extraction</entity> is shallow and can be captured by <entity id="P05-1053.12">chunking</entity>. We also demonstrate how <entity id="P05-1053.13">semantic information</entity> such as <entity id="P05-1053.14">WordNet</entity> and <entity id="P05-1053.15">Name List</entity>, can be used in <entity id="P05-1053.16">feature-based relation extraction</entity> to further improve the <entity id="P05-1053.17">performance</entity>. <entity id="P05-1053.18">Evaluation</entity> on the <entity id="P05-1053.19">ACE corpus</entity> shows that effective incorporation of diverse <entity id="P05-1053.20">features</entity> enables our <entity id="P05-1053.21">system</entity> outperform previously best-reported <entity id="P05-1053.22">systems</entity> on the <entity id="P05-1053.23">24 ACE relation subtypes</entity> and significantly outperforms <entity id="P05-1053.24">tree kernel-based systems</entity> by over 20 in <entity id="P05-1053.25">F-measure</entity> on the <entity id="P05-1053.26">5 ACE relation types</entity>.
</abstract>



</text>

<text id="P05-1076"><title>Automatic Acquisition Of Adjectival Subcategorization From Corpora</title>
<abstract>
This paper describes a novel <entity id="P05-1076.1">system</entity> for <entity id="P05-1076.2">acquiring adjectival subcategorization frames</entity> (<entity id="P05-1076.3">scfs</entity>) and associated frequency information from <entity id="P05-1076.4">English</entity> <entity id="P05-1076.5">corpus data</entity>. The <entity id="P05-1076.6">system</entity> incorporates a <entity id="P05-1076.7">decision-tree classifier</entity> for 30 <entity id="P05-1076.8">scf types</entity> which tests for the presence of <entity id="P05-1076.9">grammatical relations</entity> (<entity id="P05-1076.10">grs</entity>) in the <entity id="P05-1076.11">output</entity> of a robust <entity id="P05-1076.12">statistical parser</entity>. It uses a powerful <entity id="P05-1076.13">pattern-matching language</entity> to classify <entity id="P05-1076.14">grs</entity> into <entity id="P05-1076.15">frames</entity> hierarchically in a way that mirrors <entity id="P05-1076.16">inheritance-based lexica</entity>. The <entity id="P05-1076.17">experiments</entity> show that the <entity id="P05-1076.18">system</entity> is able to detect <entity id="P05-1076.19">scf types</entity> with <entity id="P05-1076.20">70% precision</entity> and <entity id="P05-1076.21">66% recall rate</entity>. A new <entity id="P05-1076.22">tool</entity> for <entity id="P05-1076.23">linguistic annotation</entity> of <entity id="P05-1076.24">scfs</entity> in <entity id="P05-1076.25">corpus data</entity> is also introduced which can considerably alleviate the process of obtaining <entity id="P05-1076.26">training and test data</entity> for <entity id="P05-1076.27">subcategorization acquisition</entity>.
</abstract>



</text>

<text id="I05-2013"> <title>Automatic recognition of French expletive pronoun occurrences</title>
<abstract>
We present a <entity id="I05-2013.1">tool</entity>, called <entity id="I05-2013.2">ILIMP</entity>, which takes as input a <entity id="I05-2013.3">raw text</entity> in <entity id="I05-2013.4">French</entity> and produces as output the same <entity id="I05-2013.5">text</entity> in which every occurrence of the <entity id="I05-2013.6">pronoun il</entity> is tagged either with tag <entity id="I05-2013.7">[ANA]</entity> for <entity id="I05-2013.8">anaphoric</entity> or <entity id="I05-2013.9">[IMP]</entity> for <entity id="I05-2013.10">impersonal</entity> or <entity id="I05-2013.11">expletive</entity>. This <entity id="I05-2013.12">tool</entity> is therefore designed to distinguish between the <entity id="I05-2013.13">anaphoric occurrences of il</entity>, for which an <entity id="I05-2013.14">anaphora resolution system</entity> has to look for an antecedent, and the <entity id="I05-2013.15">expletive occurrences</entity> of this <entity id="I05-2013.16">pronoun</entity>, for which it does not make sense to look for an antecedent. The <entity id="I05-2013.17">precision rate</entity> for <entity id="I05-2013.18">ILIMP</entity> is 97,5%. The few <entity id="I05-2013.19">errors</entity> are analyzed in detail. Other <entity id="I05-2013.20">tasks</entity> using the <entity id="I05-2013.21">method</entity> developed for <entity id="I05-2013.22">ILIMP</entity> are described briefly, as well as the use of <entity id="I05-2013.23">ILIMP</entity> in a modular <entity id="I05-2013.24">syntactic analysis system</entity>. </abstract>



</text>

<text id="E85-1037"> <title>A PROBLEM SOLVING APPROACH TO GENERATING TEXT FROM SYSTEMIC GRAMMARS</title>
<abstract>
<entity id="E85-1037.1">Systemic grammar</entity> has been used for <entity id="E85-1037.2">AI text generation</entity> work in the past, but the <entity id="E85-1037.3">implementations</entity> have tended be ad hoc or inefficient. This paper presents an approach to systemic <entity id="E85-1037.4">text generation</entity> where <entity id="E85-1037.5">AI problem solving techniques</entity> are applied directly to an unadulterated <entity id="E85-1037.6">systemic grammar</entity>. This <entity id="E85-1037.7">approach</entity> is made possible by a special relationship between <entity id="E85-1037.8">systemic grammar</entity> and <entity id="E85-1037.9">problem solving</entity>: both are organized primarily as choosing from alternatives. The result is simple, efficient <entity id="E85-1037.10">text generation</entity> firmly based in a <entity id="E85-1037.11">linguistic theory</entity>. </abstract>



</text>

<text id="E89-1016"><title>User Studies And The Design Of Natural Language Systems</title>
<abstract>
This paper presents a <entity id="E89-1016.1">critical discussion</entity> of the various <entity id="E89-1016.2">approaches</entity> that have been used in the <entity id="E89-1016.3">evaluation of Natural Language systems</entity>. We conclude that previous <entity id="E89-1016.4">approaches</entity> have neglected to evaluate <entity id="E89-1016.5">systems</entity> in the context of their use, e.g. solving a <entity id="E89-1016.6">task</entity> requiring <entity id="E89-1016.7">data retrieval</entity>. This raises questions about the validity of such <entity id="E89-1016.8">approaches</entity>. In the second half of the paper, we report a <entity id="E89-1016.9">laboratory study</entity> using the <entity id="E89-1016.10">Wizard of Oz technique</entity> to identify <entity id="E89-1016.11">NL requirements</entity> for carrying out this <entity id="E89-1016.12">task</entity>. We evaluate the demands that <entity id="E89-1016.13">task dialogues</entity> collected using this <entity id="E89-1016.14">technique</entity>, place upon a <entity id="E89-1016.15">prototype Natural Language system</entity>. We identify three important requirements which arose from the <entity id="E89-1016.16">task</entity> that we gave our subjects: operators specific to the task of <entity id="E89-1016.17">database access</entity>, complex <entity id="E89-1016.18">contextual reference</entity> and reference to the <entity id="E89-1016.19">structure</entity> of the <entity id="E89-1016.20">information source</entity>. We discuss how these might be satisfied by future <entity id="E89-1016.21">Natural Language systems</entity>.
</abstract>



</text>

<text id="E93-1013"><title>
LFG Semantics Via Constraints
</title>
<abstract>
<entity id="E93-1013.1">Semantic theories</entity> of <entity id="E93-1013.2">natural language</entity> associate <entity id="E93-1013.3">meanings</entity> with <entity id="E93-1013.4">utterances</entity> by providing <entity id="E93-1013.5">meanings</entity> for <entity id="E93-1013.6">lexical items</entity> and <entity id="E93-1013.7">rules</entity> for determining the <entity id="E93-1013.8">meaning</entity> of larger <entity id="E93-1013.9">units</entity> given the <entity id="E93-1013.10">meanings</entity> of their parts. Traditionally, <entity id="E93-1013.11">meanings</entity> are combined via <entity id="E93-1013.12">function composition</entity>, which works well when <entity id="E93-1013.13">constituent structure trees</entity> are used to guide <entity id="E93-1013.14">semantic composition</entity>. More recently, the <entity id="E93-1013.15">functional structure</entity> of <entity id="E93-1013.16">LFG</entity> has been used to provide the <entity id="E93-1013.17">syntactic information</entity> necessary for constraining <entity id="E93-1013.18">derivations</entity> of <entity id="E93-1013.19">meaning</entity> in a <entity id="E93-1013.20">cross-linguistically uniform format</entity>. It has been difficult, however, to reconcile this <entity id="E93-1013.21">approach</entity> with the combination of <entity id="E93-1013.22">meanings</entity> by <entity id="E93-1013.23">function composition</entity>. 
In contrast to <entity id="E93-1013.24">compositional approaches</entity>, we present a <entity id="E93-1013.25">deductive approach</entity> to assembling <entity id="E93-1013.26">meanings</entity>, based on <entity id="E93-1013.27">reasoning with constraints</entity>, which meshes well with the unordered nature of <entity id="E93-1013.28">information</entity> in the <entity id="E93-1013.29">functional structure</entity>. Our use of <entity id="E93-1013.30">linear logic</entity> as a 'glue' for assembling <entity id="E93-1013.31">meanings</entity> also allows for a coherent treatment of <entity id="E93-1013.32">modification</entity> as well as of the <entity id="E93-1013.33">LFG</entity> requirements of <entity id="E93-1013.34">completeness</entity> and <entity id="E93-1013.35">coherence</entity>.
</abstract>


</text>

<text id="E95-1036"><title>Splitting The Reference Time: Temporal Anaphora And Quantification In DRT
</title>
<abstract>
This paper presents an analysis of <entity id="E95-1036.1">temporal anaphora</entity> in <entity id="E95-1036.2">sentences</entity> which contain <entity id="E95-1036.3">quantification over events</entity>, within the framework of <entity id="E95-1036.4">Discourse Representation Theory</entity>. The analysis in (Partee, 1984) of <entity id="E95-1036.5">quantified sentences</entity>, introduced by a <entity id="E95-1036.6">temporal connective</entity>, gives the wrong <entity id="E95-1036.7">truth-conditions</entity> when the <entity id="E95-1036.8">temporal connective</entity> in the <entity id="E95-1036.9">subordinate clause</entity> is before or after. This <entity id="E95-1036.10">problem</entity> has been previously analyzed in (de Swart, 1991) as an instance of the <entity id="E95-1036.11">proportion problem</entity> and given a solution from a <entity id="E95-1036.12">Generalized Quantifier approach</entity>. By using a careful distinction between the different notions of <entity id="E95-1036.13">reference time</entity> based on (Kamp and Reyle, 1993), we propose a solution to this <entity id="E95-1036.14">problem</entity>, within the framework of <entity id="E95-1036.15">DRT</entity>. We show some applications of this <entity id="E95-1036.16">solution</entity> to additional <entity id="E95-1036.17">temporal anaphora phenomena</entity> in <entity id="E95-1036.18">quantified sentences</entity>.
</abstract>


</text>

<text id="H89-2019"><title>
A Proposal For SLS Evaluation</title>
<abstract>
This paper proposes an automatic, essentially <entity id="H89-2019.1">domain-independent means of evaluating Spoken Language Systems (SLS)</entity> which combines <entity id="H89-2019.2">software</entity> we have developed for that purpose (the "<entity id="H89-2019.3">Comparator</entity>") and a set of <entity id="H89-2019.4">specifications</entity> for <entity id="H89-2019.5">answer expressions</entity> (the "<entity id="H89-2019.6">Common Answer Specification</entity>", or <entity id="H89-2019.7">CAS</entity>). The <entity id="H89-2019.8">Comparator</entity> checks whether the answer provided by a <entity id="H89-2019.9">SLS</entity> accords with a <entity id="H89-2019.10">canonical answer</entity>, returning either true or false. The <entity id="H89-2019.11">Common Answer Specification</entity> determines the <entity id="H89-2019.12">syntax</entity> of <entity id="H89-2019.13">answer expressions</entity>, the minimal <entity id="H89-2019.14">content</entity> that must be included in them, the <entity id="H89-2019.15">data</entity> to be included in and excluded from <entity id="H89-2019.16">test corpora</entity>, and the <entity id="H89-2019.17">procedures</entity> used by the <entity id="H89-2019.18">Comparator</entity>. Though some details of the <entity id="H89-2019.19">CAS</entity> are particular to individual <entity id="H89-2019.20">domains</entity>, the <entity id="H89-2019.21">Comparator software</entity> is <entity id="H89-2019.22">domain-independent</entity>, as is the <entity id="H89-2019.23">CAS approach</entity>.
</abstract>


</text>

<text id="H93-1076"> <title><entity id="H93-1076.1">Speech and Text-Image Processing</entity> in <entity id="H93-1076.2">Documents</entity></title>
<abstract>
Two themes have evolved in <entity id="H93-1076.3">speech and text image processing</entity> work at <entity id="H93-1076.4">Xerox PARC</entity> that expand and redefine the role of <entity id="H93-1076.5">recognition technology</entity> in <entity id="H93-1076.6">document-oriented applications</entity>. One is the development of systems that provide functionality similar to that of <entity id="H93-1076.7">text processors</entity> but operate directly on <entity id="H93-1076.8">audio and scanned image data</entity>. A second, related theme is the use of <entity id="H93-1076.9">speech and text-image recognition</entity> to retrieve arbitrary, user-specified information from <entity id="H93-1076.10">documents with signal content</entity>. This paper discusses three research initiatives at <entity id="H93-1076.11">PARC</entity> that exemplify these themes: a <entity id="H93-1076.12">text-image editor</entity>[1], a <entity id="H93-1076.13">wordspotter</entity> for <entity id="H93-1076.14">voice editing and indexing</entity>[12], and a <entity id="H93-1076.15">decoding framework</entity> for <entity id="H93-1076.16">scanned-document content retrieval</entity>[4]. The discussion focuses on key concepts embodied in the research that enable novel <entity id="H93-1076.17">signal-based document processing functionality</entity>.
</abstract>


</text>

<text id="A97-1028"><title>
A Statistical Profile Of The Named Entity Task</title>
<abstract>
In this paper we present a <entity id="A97-1028.1">statistical profile</entity> of the <entity id="A97-1028.2">Named Entity task</entity>, a specific <entity id="A97-1028.3">information extraction task</entity> for which <entity id="A97-1028.4">corpora</entity> in several <entity id="A97-1028.5">languages</entity> are available. Using the <entity id="A97-1028.6">results</entity> of the <entity id="A97-1028.7">statistical analysis</entity>, we propose an <entity id="A97-1028.8">algorithm</entity> for <entity id="A97-1028.9">lower bound estimation</entity> for <entity id="A97-1028.10">Named Entity corpora</entity> and discuss the significance of the <entity id="A97-1028.11">cross-lingual comparisons</entity> provided by the <entity id="A97-1028.12">analysis</entity>.
</abstract>


</text>

<text id="H05-1115"><title>
Using Random Walks For Question-Focused Sentence Retrieval</title>
<abstract>
We consider the problem of <entity id="H05-1115.1">question-focused sentence retrieval</entity> from complex <entity id="H05-1115.2">news articles</entity> describing <entity id="H05-1115.3">multi-event stories published over time</entity>. <entity id="H05-1115.4">Annotators</entity> generated a list of <entity id="H05-1115.5">questions</entity> central to understanding each <entity id="H05-1115.6">story</entity> in our <entity id="H05-1115.7">corpus</entity>. Because of the dynamic nature of the <entity id="H05-1115.8">stories</entity>, many <entity id="H05-1115.9">questions</entity> are time-sensitive (e.g. "How many victims have been found?"). <entity id="H05-1115.10">Judges</entity> found <entity id="H05-1115.11">sentences</entity> providing an <entity id="H05-1115.12">answer</entity> to each <entity id="H05-1115.13">question</entity>. To address the <entity id="H05-1115.14">sentence retrieval problem</entity>, we apply a <entity id="H05-1115.15">stochastic, graph-based method</entity> for comparing the relative importance of the <entity id="H05-1115.16">textual units</entity>, which was previously used successfully for <entity id="H05-1115.17">generic summarization</entity>. Currently, we present a topic-sensitive version of our <entity id="H05-1115.18">method</entity> and hypothesize that it can outperform a competitive <entity id="H05-1115.19">baseline</entity>, which compares the <entity id="H05-1115.20">similarity</entity> of each <entity id="H05-1115.21">sentence</entity> to the input <entity id="H05-1115.22">question</entity> via <entity id="H05-1115.23">IDF-weighted word overlap</entity>. In our experiments, the <entity id="H05-1115.24">method</entity> achieves a <entity id="H05-1115.25">TRDR score</entity> that is significantly higher than that of the <entity id="H05-1115.26">baseline</entity>.
</abstract>


</text>

<text id="J89-4003"><title>
A Formal Model For Context-Free Languages Augmented With Reduplication</title>
<abstract>
A <entity id="J89-4003.1">model</entity> is presented to characterize the <entity id="J89-4003.2">class of languages</entity> obtained by adding <entity id="J89-4003.3">reduplication</entity> to <entity id="J89-4003.4">context-free languages</entity>. The <entity id="J89-4003.5">model</entity> is a <entity id="J89-4003.6">pushdown automaton</entity> augmented with the ability to check <entity id="J89-4003.7">reduplication</entity> by using the <entity id="J89-4003.8">stack</entity> in a new way. The <entity id="J89-4003.9">class of languages</entity> generated is shown to lie strictly between the <entity id="J89-4003.10">context-free languages</entity> and the <entity id="J89-4003.11">indexed languages</entity>. The <entity id="J89-4003.12">model</entity> appears capable of accommodating the sort of <entity id="J89-4003.13">reduplications</entity> that have been observed to occur in <entity id="J89-4003.14">natural languages</entity>, but it excludes many of the unnatural <entity id="J89-4003.15">constructions</entity> that other <entity id="J89-4003.16">formal models</entity> have permitted.
</abstract>


</text>

<text id="I05-6010"><title>
Some remarks on the Annotation of Quantifying Noun Groups in Treebanks
</title>
<abstract>
This article is devoted to the problem of <entity id="I05-6010.1">quantifying noun groups</entity> in <entity id="I05-6010.2">German</entity>. After a thorough description of the phenomena, the results of <entity id="I05-6010.3">corpus-based investigations</entity> are described. Moreover, some examples are given that underline the necessity of integrating some kind of information other than <entity id="I05-6010.4">grammar sensu stricto</entity> into the <entity id="I05-6010.5">treebank</entity>. We argue that a more sophisticated and fine-grained <entity id="I05-6010.6">annotation</entity> in the <entity id="I05-6010.7">tree-bank</entity> would have very positve effects on <entity id="I05-6010.8">stochastic parsers</entity> trained on the <entity id="I05-6010.9">tree-bank</entity> and on <entity id="I05-6010.10">grammars</entity> induced from the <entity id="I05-6010.11">treebank</entity>, and it would make the <entity id="I05-6010.12">treebank</entity> more valuable as a <entity id="I05-6010.13">source of data</entity> for <entity id="I05-6010.14">theoretical linguistic investigations</entity>. The information gained from <entity id="I05-6010.15">corpus research</entity> and the analyses that are proposed are realized in the framework of <entity id="I05-6010.16">SILVA</entity>, a <entity id="I05-6010.17">parsing</entity> and <entity id="I05-6010.18">extraction tool</entity> for <entity id="I05-6010.19">German text corpora</entity>.
</abstract>


</text>

<text id="P83-1004"> 
<title>Formal Constraints on Metarules</title> 
<abstract>
<entity id="P83-1004.1">Metagrammatical formalisms</entity> that combine <entity id="P83-1004.2">context-free phrase structure rules</entity> and <entity id="P83-1004.3">metarules (MPS grammars)</entity> allow concise statement of generalizations about the <entity id="P83-1004.4">syntax</entity> of <entity id="P83-1004.5">natural languages</entity>. <entity id="P83-1004.6">Unconstrained MPS grammars</entity>, unfortunately, are not computationally safe. We evaluate several proposals for constraining them, basing our assessment on <entity id="P83-1004.7">computational tractability and explanatory adequacy</entity>. We show that none of them satisfies both criteria, and suggest new directions for research on alternative <entity id="P83-1004.8">metagrammatical formalisms</entity>. </abstract>


</text>

<text id="P87-1022"> 
<title>A CENTERING APPROACH TO PRONOUNS</title> 
<abstract>
In this paper we present a <entity id="P87-1022.1">formalization</entity> of the <entity id="P87-1022.2">centering approach</entity> to modeling <entity id="P87-1022.3">attentional structure in discourse</entity> and use it as the basis for an <entity id="P87-1022.4">algorithm</entity> to track <entity id="P87-1022.5">discourse context</entity> and bind <entity id="P87-1022.6">pronouns</entity>. As described in [GJW86], the process of <entity id="P87-1022.7">centering attention on entities in the discourse</entity> gives rise to the <entity id="P87-1022.8">intersentential transitional states of continuing, retaining and shifting</entity>. We propose an extension to these <entity id="P87-1022.9">states</entity> which handles some additional cases of multiple <entity id="P87-1022.10">ambiguous pronouns</entity>. The <entity id="P87-1022.11">algorithm</entity> has been implemented in an <entity id="P87-1022.12">HPSG natural language system</entity> which serves as the interface to a <entity id="P87-1022.13">database query application</entity>. </abstract>


</text>

<text id="P95-1013"> 
<title>Compilation of HPSG to TAG</title> 
<abstract>
We present an implemented <entity id="P95-1013.1">compilation algorithm</entity> that translates <entity id="P95-1013.2">HPSG</entity> into <entity id="P95-1013.3">lexicalized feature-based TAG</entity>, relating concepts of the two <entity id="P95-1013.4">theories</entity>. While <entity id="P95-1013.5">HPSG</entity> has a more elaborated <entity id="P95-1013.6">principle-based theory</entity> of possible <entity id="P95-1013.7">phrase structures</entity>, <entity id="P95-1013.8">TAG</entity> provides the means to represent <entity id="P95-1013.9">lexicalized structures</entity> more explicitly. Our objectives are met by giving clear definitions that determine the <entity id="P95-1013.10">projection of structures</entity> from the <entity id="P95-1013.11">lexicon</entity>, and identify <entity id="P95-1013.12">maximal projections</entity>, <entity id="P95-1013.13">auxiliary trees</entity> and <entity id="P95-1013.14">foot nodes</entity>. </abstract>


</text>

<text id="P97-1002"> 
<title>Fast Context-Free Parsing Requires Fast Boolean Matrix Multiplication</title> 
<abstract>
Valiant showed that <entity id="P97-1002.1">Boolean matrix multiplication (BMM)</entity> can be used for <entity id="P97-1002.2">CFG parsing</entity>. We prove a dual result: <entity id="P97-1002.3">CFG parsers</entity> running in <entity id="P97-1002.4">time O(|G||w|3-e)</entity> on a <entity id="P97-1002.5">grammar G</entity> and a <entity id="P97-1002.6">string w</entity> can be used to multiply <entity id="P97-1002.7">m x m Boolean matrices</entity> in <entity id="P97-1002.8">time O(m3-e/3)</entity>. In the process we also provide a <entity id="P97-1002.9">formal definition</entity> of <entity id="P97-1002.10">parsing</entity> motivated by an informal notion due to Lang. Our result establishes one of the first limitations on general <entity id="P97-1002.11">CFG parsing</entity>: a fast, practical <entity id="P97-1002.12">CFG parser</entity> would yield a fast, practical <entity id="P97-1002.13">BMM algorithm</entity>, which is not believed to exist. </abstract>


</text>

<text id="P97-1040"> 
<title>Efficient Generation in Primitive Optimality Theory</title> 
<abstract>
This paper introduces <entity id="P97-1040.1">primitive Optimality Theory (OTP)</entity>, a linguistically motivated formalization of <entity id="P97-1040.2">OT</entity>. <entity id="P97-1040.3">OTP</entity> specifies the <entity id="P97-1040.4">class of autosegmental representations</entity>, the <entity id="P97-1040.5">universal generator Gen</entity>, and the two simple families of <entity id="P97-1040.6">permissible constraints</entity>. In contrast to less restricted <entity id="P97-1040.7">theories</entity> using <entity id="P97-1040.8">Generalized Alignment</entity>, <entity id="P97-1040.9">OTP</entity>'s optimal <entity id="P97-1040.10">surface forms</entity> can be generated with <entity id="P97-1040.11">finite-state methods</entity> adapted from (Ellison, 1994). Unfortunately these <entity id="P97-1040.12">methods</entity> take <entity id="P97-1040.13">time exponential on the size of the grammar</entity>. Indeed the <entity id="P97-1040.14">generation problem</entity> is shown <entity id="P97-1040.15">NP-complete</entity> in this sense. However, techniques are discussed for making <entity id="P97-1040.16">Ellison's approach</entity> fast in the typical case, including a simple trick that alone provides a 100-fold speedup on a <entity id="P97-1040.17">grammar</entity> fragment of moderate size. One avenue for future improvements is a new <entity id="P97-1040.18">finite-state notion</entity>, <entity id="P97-1040.19">factored automata</entity>, where <entity id="P97-1040.20">regular languages</entity> are represented compactly via <entity id="P97-1040.21">formal intersections of FSAs</entity>. </abstract>


</text>

<text id="P97-1072"> 
<title>Towards resolution of bridging descriptions</title> 
<abstract>
We present preliminary results concerning robust techniques for <entity id="P97-1072.1">resolving bridging definite descriptions</entity>. We report our analysis of a collection of 20 <entity id="P97-1072.2">Wall Street Journal articles</entity> from the <entity id="P97-1072.3">Penn Treebank Corpus</entity> and our experiments with <entity id="P97-1072.4">WordNet</entity> to identify relations between <entity id="P97-1072.5">bridging descriptions</entity> and their <entity id="P97-1072.6">antecedents</entity>. </abstract>


</text>

<text id="P99-1058"> 
<title>A semantically-derived subset of English for hardware verification</title> 
<abstract>
To verify <entity id="P99-1058.1">hardware designs</entity> by <entity id="P99-1058.2">model checking</entity>, <entity id="P99-1058.3">circuit specifications</entity> are commonly expressed in the <entity id="P99-1058.4">temporal logic CTL</entity>. <entity id="P99-1058.5">Automatic conversion of English to CTL</entity> requires the definition of an appropriately <entity id="P99-1058.6">restricted subset</entity> of <entity id="P99-1058.7">English</entity>. We show how the limited <entity id="P99-1058.8">semantic expressibility</entity> of <entity id="P99-1058.9">CTL</entity> can be exploited to derive a hierarchy of <entity id="P99-1058.10">subsets</entity>. Our strategy avoids potential difficulties with approaches that take existing <entity id="P99-1058.11">computational semantic analyses</entity> of <entity id="P99-1058.12">English</entity> as their starting point--such as the need to ensure that all <entity id="P99-1058.13">sentences</entity> in the <entity id="P99-1058.14">subset</entity> possess a <entity id="P99-1058.15">CTL translation</entity>. </abstract>


</text>

<text id="P05-1039">
<title>
What To Do When Lexicalization Fails: Parsing German With Suffix Analysis And Smoothing </title><abstract>
In this paper, we present an <entity id="P05-1039.1">unlexicalized parser</entity> for <entity id="P05-1039.2">German</entity> which employs <entity id="P05-1039.3">smoothing</entity> and <entity id="P05-1039.4">suffix analysis</entity> to achieve a <entity id="P05-1039.5">labelled bracket F-score</entity> of 76.2, higher than previously reported results on the <entity id="P05-1039.6">NEGRA corpus</entity>. In addition to the high <entity id="P05-1039.7">accuracy</entity> of the model, the use of <entity id="P05-1039.8">smoothing</entity> in an <entity id="P05-1039.9">unlexicalized parser</entity> allows us to better examine the interplay between <entity id="P05-1039.10">smoothing</entity> and <entity id="P05-1039.11">parsing</entity> results.
</abstract>



</text>

<text id="P05-1058">
<title> Alignment Model Adaptation For Domain-Specific Word Alignment </title><abstract>
This paper proposes an <entity id="P05-1058.1">alignment adaptation approach</entity> to improve <entity id="P05-1058.2">domain-specific (in-domain) word alignment</entity>. The basic idea of <entity id="P05-1058.3">alignment adaptation</entity> is to use <entity id="P05-1058.4">out-of-domain corpus</entity> to improve <entity id="P05-1058.5">in-domain word alignment</entity> results. In this paper, we first train two <entity id="P05-1058.6">statistical word alignment models</entity> with the large-scale <entity id="P05-1058.7">out-of-domain corpus</entity> and the small-scale <entity id="P05-1058.8">in-domain corpus</entity> respectively, and then interpolate these two models to improve the <entity id="P05-1058.9">domain-specific word alignment</entity>. Experimental results show that our approach improves <entity id="P05-1058.10">domain-specific word alignment</entity> in terms of both <entity id="P05-1058.11">precision</entity> and <entity id="P05-1058.12">recall</entity>, achieving a <entity id="P05-1058.13">relative error rate reduction</entity> of 6.56% as compared with the state-of-the-art technologies.
</abstract>



</text>

<text id="P05-3001">
<title>
An Information-State Approach To Collaborative Reference </title><abstract>
We describe a <entity id="P05-3001.1">dialogue system</entity> that works with its interlocutor to identify objects. Our contributions include a concise, <entity id="P05-3001.2">modular architecture</entity> with reversible processes of <entity id="P05-3001.3">understanding</entity> and <entity id="P05-3001.4">generation</entity>, an <entity id="P05-3001.5">information-state model of reference</entity>, and flexible links between <entity id="P05-3001.6">semantics</entity> and <entity id="P05-3001.7">collaborative problem solving</entity>.
</abstract>



</text>

<text id="E83-1021"> 
<title>AN APPROACH TO NATURAL LANGUAGE IN THE SI-NETS PARADIGM </title>
<abstract>
This article deals with the <entity id="E83-1021.1">interpretation</entity> of <entity id="E83-1021.2">conceptual operations</entity> underlying the communicative use of <entity id="E83-1021.3">natural language (NL)</entity> within the <entity id="E83-1021.4">Structured Inheritance Network (SI-Nets) paradigm</entity>. The operations are reduced to <entity id="E83-1021.5">functions</entity> of a <entity id="E83-1021.6">formal language</entity>, thus changing the level of abstraction of the operations to be performed on <entity id="E83-1021.7">SI-Nets</entity>. In this sense, operations on <entity id="E83-1021.8">SI-Nets</entity> are not merely isomorphic to single epistemological objects, but can be viewed as a simulation of processes on a different level, that pertaining to the <entity id="E83-1021.9">conceptual system</entity> of <entity id="E83-1021.10">NL</entity>. For this purpose, we have designed a version of <entity id="E83-1021.11">KL-ONE</entity> which represents the <entity id="E83-1021.12">epistemological level</entity>, while the new experimental language, <entity id="E83-1021.13">KL-Conc</entity>, represents the <entity id="E83-1021.14">conceptual level</entity>. KL-Conc would seem to be a more natural and intuitive way of interacting with <entity id="E83-1021.15">SI-Nets</entity>. </abstract>



</text>

<text id="E87-1043">
<title> Iteration, Habituality And Verb Form Semantics </title><abstract>
The <entity id="E87-1043.1">verb forms</entity> are often claimed to convey two kinds of <entity id="E87-1043.2">information</entity> : 1. whether the <entity id="E87-1043.3">event</entity> described in a <entity id="E87-1043.4">sentence</entity> is <entity id="E87-1043.5">present</entity>, <entity id="E87-1043.6">past</entity> or <entity id="E87-1043.7">future</entity> (= <entity id="E87-1043.8">deictic information</entity>) 2. whether the <entity id="E87-1043.9">event</entity> described in a <entity id="E87-1043.10">sentence</entity> is presented as completed, going on, just starting or being finished (= <entity id="E87-1043.11">aspectual information</entity>). It will be demonstrated in this paper that one has to add a third component to the analysis of <entity id="E87-1043.12">verb form meanings</entity>, namely whether or not they express <entity id="E87-1043.13">habituality</entity>. The framework of the analysis is <entity id="E87-1043.14">model-theoretic semantics</entity>. </abstract>



</text>

<text id="E91-1050">
<title>
A Language For The Statement Of Binary Relations Over Feature Structures </title><abstract>
<entity id="E91-1050.1">Unification</entity> is often the appropriate method for expressing <entity id="E91-1050.2">relations</entity> between <entity id="E91-1050.3">representations</entity> in the form of <entity id="E91-1050.4">feature structures</entity>; however, there are circumstances in which a different approach is desirable. A <entity id="E91-1050.5">declarative formalism</entity> is presented which permits direct <entity id="E91-1050.6">mappings</entity> of one <entity id="E91-1050.7">feature structure</entity> into another, and illustrative examples are given of its application to areas of current interest.
</abstract>



</text>

<text id="E93-1025"> 
<title>A Discourse Copying Algorithm for Ellipsis and Anaphora Resolution </title>
<abstract>
 We give an analysis of <entity id="E93-1025.1">ellipsis resolution</entity> in terms of a straightforward <entity id="E93-1025.2">discourse copying algorithm</entity> that correctly predicts a wide range of phenomena. The treatment does not suffer from problems inherent in <entity id="E93-1025.3">identity-of-relations analyses</entity>. Furthermore, in contrast to the approach of Dalrymple et al. [1991], the treatment directly encodes the intuitive distinction between <entity id="E93-1025.4">full NPs</entity> and the <entity id="E93-1025.5">referential elements</entity> that corefer with them through what we term <entity id="E93-1025.6">role linking</entity>. The correct <entity id="E93-1025.7">predictions</entity> for several problematic examples of <entity id="E93-1025.8">ellipsis</entity> naturally result. Finally, the analysis extends directly to other <entity id="E93-1025.9">discourse copying phenomena</entity>. </abstract>



</text>

<text id="E99-1023">
<title>Representing Text Chunks </title><abstract>
Dividing <entity id="E99-1023.1">sentences</entity> in <entity id="E99-1023.2">chunks of words</entity> is a useful preprocessing step for <entity id="E99-1023.3">parsing</entity>, <entity id="E99-1023.4">information extraction</entity> and <entity id="E99-1023.5">information retrieval</entity>. (Ramshaw and Marcus, 1995) have introduced a "convenient" <entity id="E99-1023.6">data representation</entity> for <entity id="E99-1023.7">chunking</entity> by converting it to a <entity id="E99-1023.8">tagging task</entity>. In this paper we will examine seven different <entity id="E99-1023.9">data representations</entity> for the problem of recognizing <entity id="E99-1023.10">noun phrase chunks</entity>. We will show that the <entity id="E99-1023.11">data representation choice</entity> has a minor influence on <entity id="E99-1023.12">chunking performance</entity>. However, equipped with the most suitable<entity id="E99-1023.13">data representation</entity>, our <entity id="E99-1023.14">memory-based learning chunker</entity> was able to improve the best published <entity id="E99-1023.15">chunking results</entity> for a <entity id="E99-1023.16">standard data set</entity>.
</abstract>



</text>

<text id="E95-1021">
<title>Tagging French - Comparing A Statistical And A Constraint-Based Method </title><abstract>
In this paper we compare two competing approaches to <entity id="E95-1021.1">part-of-speech tagging</entity>, <entity id="E95-1021.2">statistical and constraint-based disambiguation</entity>, using <entity id="E95-1021.3">French</entity> as our <entity id="E95-1021.4">test language</entity>. We imposed a time limit on our experiment: the amount of time spent on the design of our <entity id="E95-1021.5">constraint system</entity> was about the same as the time we used to train and test the easy-to-implement <entity id="E95-1021.6">statistical model</entity>. We describe the two systems and compare the results. The <entity id="E95-1021.7">accuracy</entity> of the <entity id="E95-1021.8">statistical method</entity> is reasonably good, comparable to <entity id="E95-1021.9">taggers</entity> for <entity id="E95-1021.10">English</entity>. But the <entity id="E95-1021.11">constraint-based tagger</entity> seems to be superior even with the limited time we allowed ourselves for <entity id="E95-1021.12">rule development</entity>.
</abstract>



</text>

<text id="E99-1015">
<title>
An Annotation Scheme For Discourse-Level Argumentation In Research Articles</title><abstract>
In order to build robust <entity id="E99-1015.1">automatic abstracting systems</entity>, there is a need for better <entity id="E99-1015.2">training resources</entity> than are currently available. In this paper, we introduce an <entity id="E99-1015.3">annotation scheme</entity> for scientific articles which can be used to build such a <entity id="E99-1015.4">resource</entity> in a consistent way. The seven categories of the <entity id="E99-1015.5">scheme</entity> are based on <entity id="E99-1015.6">rhetorical moves</entity> of <entity id="E99-1015.7">argumentation</entity>. Our experimental results show that the <entity id="E99-1015.8">scheme</entity> is stable, reproducible and intuitive to use.
</abstract>



</text>

<text id="H91-1067">
<title> Automatic Acquisition Of Subcategorization Frames From Tagged Text </title><abstract>
This paper describes an implemented program that takes a <entity id="H91-1067.1">tagged text corpus</entity> and generates a partial list of the <entity id="H91-1067.2">subcategorization frames</entity> in which each <entity id="H91-1067.3">verb</entity> occurs. The completeness of the output list increases monotonically with the total <entity id="H91-1067.4">occurrences</entity> of each <entity id="H91-1067.5">verb</entity> in the <entity id="H91-1067.6">training corpus</entity>. <entity id="H91-1067.7">False positive rates</entity> are one to three percent. Five <entity id="H91-1067.8">subcategorization frames</entity> are currently detected and we foresee no impediment to detecting many more. Ultimately, we expect to provide a large <entity id="H91-1067.9">subcategorization dictionary</entity> to the <entity id="H91-1067.10">NLP community</entity> and to train <entity id="H91-1067.11">dictionaries</entity> for specific <entity id="H91-1067.12">corpora</entity>.
</abstract>



</text>

<text id="A97-1021"> 
<title>Large-Scale Acquisition of LCS-Based Lexicons for Foreign Language Tutoring </title>
<abstract>
We focus on the problem of building large <entity id="A97-1021.1">repositories</entity> of <entity id="A97-1021.2">lexical conceptual structure (LCS) representations</entity> for <entity id="A97-1021.3">verbs</entity> in multiple <entity id="A97-1021.4">languages</entity>. One of the main results of this work is the definition of a relation between <entity id="A97-1021.5">broad semantic classes</entity> and <entity id="A97-1021.6">LCS meaning components</entity>. Our <entity id="A97-1021.7">acquisition program - LEXICALL -</entity> takes, as input, the result of previous work on <entity id="A97-1021.8">verb classification</entity> and <entity id="A97-1021.9">thematic grid tagging</entity>, and outputs <entity id="A97-1021.10">LCS representations</entity> for different <entity id="A97-1021.11">languages</entity>. These <entity id="A97-1021.12">representations</entity> have been ported into <entity id="A97-1021.13">English, Arabic and Spanish lexicons</entity>, each containing approximately 9000 <entity id="A97-1021.14">verbs</entity>. We are currently using these <entity id="A97-1021.15">lexicons</entity> in an <entity id="A97-1021.16">operational foreign language tutoring</entity> and <entity id="A97-1021.17">machine translation</entity>. 
</abstract>



</text>

<text id="A97-1050">
<title>
Semi-Automatic Acquisition Of Domain-Specific Translation Lexicons </title><abstract>
We investigate the utility of an <entity id="A97-1050.1">algorithm for translation lexicon acquisition (SABLE)</entity>, used previously on a very large <entity id="A97-1050.2">corpus</entity> to acquire general <entity id="A97-1050.3">translation lexicons</entity>, when that <entity id="A97-1050.4">algorithm</entity> is applied to a much smaller <entity id="A97-1050.5">corpus</entity> to produce candidates for <entity id="A97-1050.6">domain-specific translation lexicons</entity>.
</abstract>



</text>

<text id="J87-1003"> 
<title> SIMULTANEOUS-DISTRIBUTIVE COORDINATION AND CONTEXT-FREENESS </title>
<abstract>
<entity id="J87-1003.1">English</entity> is shown to be trans-context-free on the basis of <entity id="J87-1003.2">coordinations</entity> of the respectively type that involve <entity id="J87-1003.3">strictly syntactic cross-serial agreement</entity>. The <entity id="J87-1003.4">agreement</entity> in question involves <entity id="J87-1003.5">number</entity> in <entity id="J87-1003.6">nouns</entity> and <entity id="J87-1003.7">reflexive pronouns</entity> and is syntactic rather than semantic in nature because <entity id="J87-1003.8">grammatical number</entity> in <entity id="J87-1003.9">English</entity>, like <entity id="J87-1003.10">grammatical gender</entity> in <entity id="J87-1003.11">languages</entity> such as <entity id="J87-1003.12">French</entity>, is partly arbitrary. The formal proof, which makes crucial use of the <entity id="J87-1003.13">Interchange Lemma</entity> of Ogden et al., is so constructed as to be valid even if <entity id="J87-1003.14">English</entity> is presumed to contain <entity id="J87-1003.15">grammatical sentences</entity> in which respectively operates across a pair of <entity id="J87-1003.16">coordinate phrases</entity> one of whose members has fewer <entity id="J87-1003.17">conjuncts</entity> than the other; it thus goes through whatever the facts may be regarding <entity id="J87-1003.18">constructions</entity> with unequal numbers of <entity id="J87-1003.19">conjuncts</entity> in the <entity id="J87-1003.20">scope</entity> of respectively, whereas other <entity id="J87-1003.21">arguments</entity> have foundered on this problem. </abstract>



</text>

<text id="I05-5004">
<title>A Class-oriented Approach to Building a Paraphrase Corpus </title><abstract>
Towards deep analysis of <entity id="I05-5004.1">compositional classes of paraphrases</entity>, we have examined a <entity id="I05-5004.2">class-oriented framework</entity> for collecting <entity id="I05-5004.3">paraphrase examples</entity>, in which <entity id="I05-5004.4">sentential paraphrases</entity> are collected for each <entity id="I05-5004.5">paraphrase class</entity> separately by means of <entity id="I05-5004.6">automatic candidate generation</entity> and <entity id="I05-5004.7">manual judgement</entity>. Our preliminary experiments on building a <entity id="I05-5004.8">paraphrase corpus</entity> have so far been producing promising results, which we have evaluated according to <entity id="I05-5004.9">cost-efficiency</entity>, <entity id="I05-5004.10">exhaustiveness</entity>, and <entity id="I05-5004.11">reliability</entity>.
</abstract>



</text>

<text id="P81-1033"> 
<title>A Construction-Specific Approach to Focused Interaction in Flexible Parsing </title> 
<abstract>
A <entity id="P81-1033.1">flexible parser</entity> can deal with input that deviates from its <entity id="P81-1033.2">grammar</entity>, in addition to input that conforms to it. Ideally, such a <entity id="P81-1033.3">parser</entity> will correct the deviant input: sometimes, it will be unable to correct it at all; at other times, <entity id="P81-1033.4">correction</entity> will be possible, but only to within a range of ambiguous possibilities. This paper is concerned with such ambiguous situations, and with making it as easy as possible for the <entity id="P81-1033.5">ambiguity</entity> to be resolved through consultation with the user of the <entity id="P81-1033.6">parser</entity> - we presume interactive use. We show the importance of asking the user for clarification in as focused a way as possible. <entity id="P81-1033.7">Focused interaction</entity> of this kind is facilitated by a <entity id="P81-1033.8">construction-specific approach</entity> to <entity id="P81-1033.9">flexible parsing</entity>, with <entity id="P81-1033.10">specialized parsing techniques</entity> for each type of <entity id="P81-1033.11">construction</entity>, and specialized <entity id="P81-1033.12">ambiguity representations</entity> for each type of <entity id="P81-1033.13">ambiguity</entity> that a particular <entity id="P81-1033.14">construction</entity> can give rise to. A <entity id="P81-1033.15">construction-specific approach</entity> also aids in <entity id="P81-1033.16">task-specific language development</entity> by allowing a <entity id="P81-1033.17">language definition</entity> that is natural in terms of the <entity id="P81-1033.18">task domain</entity> to be interpreted directly without compilation into a <entity id="P81-1033.19">uniform grammar formalism</entity>, thus greatly speeding the <entity id="P81-1033.20">testing</entity> of changes to the <entity id="P81-1033.21">language definition</entity>. </abstract>



</text>

<text id="P85-1019"> 
<title> Semantic Caseframe Parsing and Syntactic Generality </title> 
<abstract>
We have implemented a <entity id="P85-1019.1">restricted domain parser</entity> called <entity id="P85-1019.2">Plume</entity>. Building on previous work at Carnegie-Mellon University e.g. [4, 5, 8], <entity id="P85-1019.3">Plume's approach to parsing</entity> is based on <entity id="P85-1019.4">semantic caseframe instantiation</entity>. This has the advantages of <entity id="P85-1019.5">efficiency</entity> on <entity id="P85-1019.6">grammatical input</entity>, and <entity id="P85-1019.7">robustness</entity> in the face of <entity id="P85-1019.8">ungrammatical input</entity>. While <entity id="P85-1019.9">Plume</entity> is well adapted to simple <entity id="P85-1019.10">declarative and imperative utterances</entity>, it handles <entity id="P85-1019.11">passives</entity>, <entity id="P85-1019.12">relative clauses</entity> and <entity id="P85-1019.13">interrogatives</entity> in an ad hoc manner leading to patchy <entity id="P85-1019.14">syntactic coverage</entity>. This paper outlines <entity id="P85-1019.15">Plume</entity> as it currently exists and describes our detailed design for extending <entity id="P85-1019.16">Plume</entity> to handle <entity id="P85-1019.17">passives</entity>, <entity id="P85-1019.18">relative clauses</entity>, and <entity id="P85-1019.19">interrogatives</entity> in a general manner. </abstract>



</text>

<text id="P91-1025"> 

<title>Resolving Translation Mismatches With Information Flow </title> 
<abstract>
<entity id="P91-1025.1">Languages</entity> differ in the <entity id="P91-1025.2">concepts</entity> and <entity id="P91-1025.3">real-world entities</entity> for which they have <entity id="P91-1025.4">words</entity> and <entity id="P91-1025.5">grammatical constructs</entity>. Therefore <entity id="P91-1025.6">translation</entity> must sometimes be a matter of approximating the <entity id="P91-1025.7">meaning</entity> of a <entity id="P91-1025.8">source language text</entity> rather than finding an exact counterpart in the <entity id="P91-1025.9">target language</entity>. We propose a <entity id="P91-1025.10">translation framework</entity> based on <entity id="P91-1025.11">Situation Theory</entity>. The basic ingredients are an <entity id="P91-1025.12">information lattice</entity>, a <entity id="P91-1025.13">representation scheme</entity> for <entity id="P91-1025.14">utterances</entity> embedded in <entity id="P91-1025.15">contexts</entity>, and a <entity id="P91-1025.16">mismatch resolution scheme</entity> defined in terms of <entity id="P91-1025.17">information flow</entity>. We motivate our approach with examples of <entity id="P91-1025.18">translation</entity> between <entity id="P91-1025.19">English</entity> and <entity id="P91-1025.20">Japanese</entity>. </abstract>



</text>

<text id="P95-1034"> 
<title> Two-Level, Many-Paths Generation </title> 
<abstract>
<entity id="P95-1034.1">Large-scale natural language generation</entity> requires the integration of vast amounts of <entity id="P95-1034.2">knowledge</entity>: lexical, grammatical, and conceptual. A <entity id="P95-1034.3">robust generator</entity> must be able to operate well even when pieces of <entity id="P95-1034.4">knowledge</entity> are missing. It must also be robust against <entity id="P95-1034.5">incomplete or inaccurate inputs</entity>. To attack these problems, we have built a <entity id="P95-1034.6">hybrid generator</entity>, in which gaps in <entity id="P95-1034.7">symbolic knowledge</entity> are filled by <entity id="P95-1034.8">statistical methods</entity>. We describe algorithms and show experimental results. We also discuss how the <entity id="P95-1034.9">hybrid generation model</entity> can be used to simplify current <entity id="P95-1034.10">generators</entity> and enhance their <entity id="P95-1034.11">portability</entity>, even when perfect <entity id="P95-1034.12">knowledge</entity> is in principle obtainable.</abstract>



</text>

<text id="P97-1017"> 
<title> Machine Transliteration </title> 
<abstract>
It is challenging to translate <entity id="P97-1017.1">names</entity> and <entity id="P97-1017.2">technical terms</entity> across <entity id="P97-1017.3">languages</entity> with different <entity id="P97-1017.4">alphabets</entity> and <entity id="P97-1017.5">sound inventories</entity>. These items are commonly transliterated, i.e., replaced with approximate <entity id="P97-1017.6">phonetic equivalents</entity>. For example, computer in <entity id="P97-1017.7">English</entity> comes out as ~ i/l:::'=--~-- (konpyuutaa) in <entity id="P97-1017.8">Japanese</entity>. Translating such items from <entity id="P97-1017.9">Japanese</entity> back to <entity id="P97-1017.10">English</entity> is even more challenging, and of practical interest, as transliterated items make up the bulk of <entity id="P97-1017.11">text phrases</entity> not found in <entity id="P97-1017.12">bilingual dictionaries</entity>. We describe and evaluate a method for performing <entity id="P97-1017.13">backwards transliterations</entity> by <entity id="P97-1017.14">machine</entity>. This method uses a <entity id="P97-1017.15">generative model</entity>, incorporating several distinct stages in the <entity id="P97-1017.16">transliteration process</entity>. </abstract>



</text>

<text id="P97-1058"> 
<title>Approximating Context-Free Grammars with a Finite-State Calculus </title> 
<abstract>
Although adequate models of <entity id="P97-1058.1">human language</entity> for <entity id="P97-1058.2">syntactic analysis</entity> and <entity id="P97-1058.3">semantic interpretation</entity> are of at least <entity id="P97-1058.4">context-free complexity</entity>, for applications such as <entity id="P97-1058.5">speech processing</entity> in which speed is important <entity id="P97-1058.6">finite-state models</entity> are often preferred. These requirements may be reconciled by using the more complex <entity id="P97-1058.7">grammar</entity> to automatically derive a <entity id="P97-1058.8">finite-state approximation</entity> which can then be used as a filter to guide <entity id="P97-1058.9">speech recognition</entity> or to reject many hypotheses at an early stage of processing. A method is presented here for calculating such <entity id="P97-1058.10">finite-state approximations</entity> from <entity id="P97-1058.11">context-free grammars</entity>. It is essentially different from the algorithm introduced by Pereira and Wright (1991; 1996), is faster in some cases, and has the advantage of being open-ended and adaptable.
</abstract>



</text>

<text id="P99-1036"> 

<title>A Part of Speech Estimation Method for Japanese Unknown Words using a Statistical Model of Morphology and Context </title> 
<abstract>
We present a <entity id="P99-1036.1">statistical model</entity> of <entity id="P99-1036.2">Japanese unknown words</entity> consisting of a set of <entity id="P99-1036.3">length and spelling models</entity> classified by the <entity id="P99-1036.4">character types</entity> that constitute a <entity id="P99-1036.5">word</entity>. The point is quite simple: different <entity id="P99-1036.6">character sets</entity> should be treated differently and the changes between <entity id="P99-1036.7">character types</entity> are very important because <entity id="P99-1036.8">Japanese script</entity> has both <entity id="P99-1036.9">ideograms</entity> like <entity id="P99-1036.10">Chinese</entity> (<entity id="P99-1036.11">kanji</entity>) and <entity id="P99-1036.12">phonograms</entity> like <entity id="P99-1036.13">English</entity> (<entity id="P99-1036.14">katakana</entity>). Both <entity id="P99-1036.15">word segmentation accuracy</entity> and <entity id="P99-1036.16">part of speech tagging accuracy</entity> are improved by the proposed model. The model can achieve 96.6% <entity id="P99-1036.17">tagging accuracy</entity> if <entity id="P99-1036.18">unknown words</entity> are correctly segmented. </abstract>



</text>

<text id="P99-1080"> 

<title>A Pylonic Decision-Tree Language Model with Optimal Question Selection </title> 
<abstract>
This paper discusses a <entity id="P99-1080.1">decision-tree approach</entity> to the problem of assigning <entity id="P99-1080.2">probabilities</entity> to <entity id="P99-1080.3">words</entity> following a given <entity id="P99-1080.4">text</entity>. In contrast with previous <entity id="P99-1080.5">decision-tree language model attempts</entity>, an algorithm for selecting <entity id="P99-1080.6">nearly optimal questions</entity> is considered. The model is to be tested on a standard task, <entity id="P99-1080.7">The Wall Street Journal</entity>, allowing a fair comparison with the well-known <entity id="P99-1080.8">tri-gram model</entity>. </abstract>



</text>

<text id="E93-1066">
<title> Two-Level Description Of Turkish Morphology </title>
<abstract>
This poster paper describes a <entity id="E93-1066.1">full scale two-level morphological description</entity> (Karttunen, 1983; Koskenniemi, 1983) of <entity id="E93-1066.2">Turkish word structures</entity>. The description has been implemented using the <entity id="E93-1066.3">PC-KIMMO environment</entity> (Antworth, 1990) and is based on a <entity id="E93-1066.4">root word lexicon</entity> of about 23,000 <entity id="E93-1066.5">roots words</entity>. Almost all the special cases of and exceptions to <entity id="E93-1066.6">phonological and morphological rules</entity> have been implemented. <entity id="E93-1066.7">Turkish</entity> is an <entity id="E93-1066.8">agglutinative language</entity> with <entity id="E93-1066.9">word structures</entity> formed by <entity id="E93-1066.10">productive affixations of derivational and inflectional suffixes</entity> to <entity id="E93-1066.11">root words</entity>. <entity id="E93-1066.12">Turkish</entity> has <entity id="E93-1066.13">finite-state</entity> but nevertheless rather complex morphotactics. <entity id="E93-1066.14">Morphemes</entity> added to a <entity id="E93-1066.15">root word</entity> or a <entity id="E93-1066.16">stem</entity> can convert the <entity id="E93-1066.17">word</entity> from a <entity id="E93-1066.18">nominal</entity> to a <entity id="E93-1066.19">verbal structure</entity> or vice-versa, or can create <entity id="E93-1066.20">adverbial constructs</entity>. The <entity id="E93-1066.21">surface realizations</entity> of <entity id="E93-1066.22">morphological constructions</entity> are constrained and modified by a number of <entity id="E93-1066.23">phonetic rules</entity> such as <entity id="E93-1066.24">vowel harmony</entity>.
</abstract>



</text>

<text id="X96-1041">
<title> TUIT : A Toolkit For Constructing Multilingual TIPSTER User Interfaces </title>
<abstract>
The <entity id="X96-1041.1">TIPSTER Architecture</entity> has been designed to enable a variety of different <entity id="X96-1041.2">text applications</entity> to use a set of <entity id="X96-1041.3">common text processing modules</entity>. Since <entity id="X96-1041.4">user interfaces</entity> work best when customized for <entity id="X96-1041.5">particular applications</entity> , it is appropriator that no particular <entity id="X96-1041.6">user interface styles or conventions</entity> are described in the <entity id="X96-1041.7">TIPSTER Architecture specification</entity>. However, the <entity id="X96-1041.8">Computing Research Laboratory (CRL)</entity> has constructed several <entity id="X96-1041.9">TIPSTER applications</entity> that use a common set of configurable <entity id="X96-1041.10">Graphical User Interface (GUI) functions</entity>. These <entity id="X96-1041.11">GUIs</entity> were constructed using <entity id="X96-1041.12">CRL's TIPSTER User Interface Toolkit (TUIT)</entity>. <entity id="X96-1041.13">TUIT</entity> is a <entity id="X96-1041.14">software library</entity> that can be used to construct <entity id="X96-1041.15">multilingual TIPSTER user interfaces</entity> for a set of common user tasks. <entity id="X96-1041.16">CRL</entity> developed <entity id="X96-1041.17">TUIT</entity> to support their work to integrate <entity id="X96-1041.18">TIPSTER modules</entity> for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects. This paper briefly describes <entity id="X96-1041.19">TUIT</entity> and its capabilities. 
</abstract>



</text>

<text id="P02-1008">
<title> Phonological Comprehension And The Compilation Of Optimality Theory </title>
<abstract>
This paper ties up some loose ends in <entity id="P02-1008.1">finite-state Optimality Theory</entity>. First, it discusses how to perform <entity id="P02-1008.2">comprehension</entity> under <entity id="P02-1008.3">Optimality Theory grammars</entity> consisting of <entity id="P02-1008.4">finite-state constraints</entity>. <entity id="P02-1008.5">Comprehension</entity> has not been much studied in <entity id="P02-1008.6">OT</entity>; we show that unlike <entity id="P02-1008.7">production</entity>, it does not always yield a regular set, making <entity id="P02-1008.8">finite-state methods</entity> inapplicable. However, after giving a suitably flexible presentation of <entity id="P02-1008.9">OT</entity>, we show carefully how to treat <entity id="P02-1008.10">comprehension</entity> under recent <entity id="P02-1008.11">variants of OT</entity>in which <entity id="P02-1008.12">grammars</entity> can be compiled into <entity id="P02-1008.13">finite-state transducers</entity>. We then unify these variants, showing that <entity id="P02-1008.14">compilation</entity> is possible if all components of the <entity id="P02-1008.15">grammar</entity> are <entity id="P02-1008.16">regular relations</entity>, including the <entity id="P02-1008.17">harmony ordering</entity> on <entity id="P02-1008.18">scored candidates</entity>. 
</abstract>



</text>

<text id="P98-2176">
<title>Learning Correlations between Linguistic Indicators and Semantic Constraints : Reuse of Context-Dependent Descriptions of Entities </title>
<abstract>
This paper presents the results of a study on the <entity id="P98-2176.1">semantic constraints</entity> imposed on <entity id="P98-2176.2">lexical choice</entity> by certain <entity id="P98-2176.3">contextual indicators</entity>. We show how such <entity id="P98-2176.4">indicators</entity> are computed and how <entity id="P98-2176.5">correlations</entity> between them and the <entity id="P98-2176.6">choice of a noun phrase description</entity> of a <entity id="P98-2176.7">named entity</entity> can be automatically established using <entity id="P98-2176.8">supervised learning</entity>. Based on this <entity id="P98-2176.9">correlation</entity>, we have developed a technique for <entity id="P98-2176.10">automatic lexical choice</entity> of <entity id="P98-2176.11">descriptions</entity> of <entity id="P98-2176.12">entities</entity> in <entity id="P98-2176.13">text generation</entity>. We discuss the underlying relationship between the <entity id="P98-2176.14">pragmatics</entity> of choosing an appropriate <entity id="P98-2176.15">description</entity> that serves a specific purpose in the <entity id="P98-2176.16">automatically generated text</entity> and the <entity id="P98-2176.17">semantics</entity> of the <entity id="P98-2176.18">description</entity> itself. We present our work in the framework of the more general concept of reuse of <entity id="P98-2176.19">linguistic structures</entity> that are automatically extracted from <entity id="P98-2176.20">large corpora</entity>. We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method.
</abstract>



</text>

<text id="H94-1024">
<title> Evaluation In The ARPA Machine Translation Program : 1993 Methodology</title>
<abstract>
In the second year of <entity id="H94-1024.1">evaluations</entity> of the <entity id="H94-1024.2">ARPA HLT Machine Translation (MT) Initiative</entity>, methodologies developed and tested in 1992 were applied to the <entity id="H94-1024.3">1993 MT test runs</entity>. The current methodology optimizes the inherently <entity id="H94-1024.4">subjective judgments</entity> on <entity id="H94-1024.5">translation accuracy and quality</entity> by channeling the <entity id="H94-1024.6">judgments</entity> of <entity id="H94-1024.7">non-translators</entity> into many <entity id="H94-1024.8">data points</entity> which reflect both the comparison of the <entity id="H94-1024.9">performance</entity> of the <entity id="H94-1024.10">research MT systems</entity> with <entity id="H94-1024.11">production MT systems</entity> and against the <entity id="H94-1024.12">performance</entity> of <entity id="H94-1024.13">novice translators</entity>. This paper discusses the three <entity id="H94-1024.14">evaluation methods</entity> used in the <entity id="H94-1024.15">1993 evaluation</entity>, the results of the evaluations , and preliminary characterizations of the <entity id="H94-1024.16"> Winter 1994 evaluation</entity>, now underway. The efforts under discussion focus on measuring the progress of <entity id="H94-1024.17">core MT technology</entity> and increasing the sensitivity and <entity id="H94-1024.18">portability</entity> of <entity id="H94-1024.19">MT evaluation methodology</entity>.
</abstract>



</text>

<text id="C02-1071">
<title>Integrating Shallow Linguistic Processing Into A Unification-Based Spanish Grammar </title>
<abstract>
This paper describes to what extent <entity id="C02-1071.1">deep processing</entity> may benefit from <entity id="C02-1071.2">shallow techniques</entity> and it presents a <entity id="C02-1071.3">NLP system</entity> which integrates a <entity id="C02-1071.4">linguistic PoS tagger and chunker</entity> as a preprocessing module of a <entity id="C02-1071.5">broad coverage unification based grammar of Spanish</entity>. Experiments show that the <entity id="C02-1071.6">efficiency</entity> of the overall analysis improves significantly and that our system also provides <entity id="C02-1071.7">robustness</entity> to the <entity id="C02-1071.8">linguistic processing</entity> while maintaining both the <entity id="C02-1071.9">accuracy</entity> and the <entity id="C02-1071.10">precision</entity> of the <entity id="C02-1071.11">grammar</entity>.
</abstract>



</text>

<text id="P06-2067">
<title> Parsing And Subcategorization Data </title>
<abstract>
In this paper, we compare the performance of a state-of-the-art <entity id="P06-2067.1">statistical parser</entity> (Bikel, 2004) in parsing <entity id="P06-2067.2">written and spoken language</entity> and in generating <entity id="P06-2067.3">sub-categorization cues</entity> from <entity id="P06-2067.4">written and spoken language</entity>. Although <entity id="P06-2067.5">Bikel's parser</entity> achieves a higher <entity id="P06-2067.6">accuracy</entity> for parsing <entity id="P06-2067.7">written language</entity>, it achieves a higher <entity id="P06-2067.8">accuracy</entity> when extracting <entity id="P06-2067.9">subcategorization cues</entity> from <entity id="P06-2067.10">spoken language</entity>. Our experiments also show that current technology for <entity id="P06-2067.11">extracting subcategorization frames</entity> initially designed for <entity id="P06-2067.12">written texts</entity> works equally well for <entity id="P06-2067.13">spoken language</entity>. Additionally, we explore the utility of <entity id="P06-2067.14">punctuation</entity> in helping <entity id="P06-2067.15">parsing</entity> and <entity id="P06-2067.16">extraction</entity> of <entity id="P06-2067.17">subcategorization cues</entity>. Our experiments show that <entity id="P06-2067.18">punctuation</entity> is of little help in parsing <entity id="P06-2067.19">spoken language</entity> and extracting <entity id="P06-2067.20">subcategorization cues</entity> from <entity id="P06-2067.21">spoken language</entity>. This indicates that there is no need to add <entity id="P06-2067.22">punctuation</entity> in transcribing <entity id="P06-2067.23">spoken corpora</entity> simply in order to help <entity id="P06-2067.24">parsers</entity>.
</abstract>



</text>

<text id="C94-1088">
<title> Character-Based Collocation For Mandarin Chinese </title>
<abstract>
This paper describes a <entity id="C94-1088.1">characters-based Chinese collocation system</entity> and discusses the advantages of it over a traditional <entity id="C94-1088.2">word-based system</entity>. Since <entity id="C94-1088.3">wordbreaks</entity> are not conventionally marked in <entity id="C94-1088.4">Chinese text corpora</entity>, a <entity id="C94-1088.5">character-based collocation system</entity> has the dual advantages of avoiding <entity id="C94-1088.6">pre-processing distortion</entity> and directly accessing <entity id="C94-1088.7">sub-lexical information</entity>. Furthermore, <entity id="C94-1088.8">word-based collocational properties</entity> can be obtained through an auxiliary module of <entity id="C94-1088.9">automatic segmentation</entity>.
</abstract>



</text>

<text id="C04-1024">
<title>Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors </title>
<abstract>
An efficient <entity id="C04-1024.1">bit-vector-based CKY-style parser</entity> for <entity id="C04-1024.2">context-free parsing</entity> is presented. The <entity id="C04-1024.3">parser</entity> computes a compact <entity id="C04-1024.4">parse forest representation</entity> of the complete set of possible <entity id="C04-1024.5">analyses for large treebank grammars</entity> and long <entity id="C04-1024.6">input sentences</entity>. The <entity id="C04-1024.7">parser</entity> uses <entity id="C04-1024.8">bit-vector operations</entity> to parallelise the <entity id="C04-1024.9">basic parsing operations</entity>. The <entity id="C04-1024.10">parser</entity> is particularly useful when all analyses are needed rather than just the most probable one.
</abstract>



</text>

<text id="N04-1008">
<title> Automatic Question Answering : Beyond The Factoid</title>
<abstract>
In this paper we describe and evaluate a <entity id="N04-1008.1">Question Answering system</entity> that goes beyond answering factoid questions. We focus on <entity id="N04-1008.2">FAQ-like questions and answers</entity> , and build our system around a <entity id="N04-1008.3">noisy-channel architecture</entity> which exploits both a <entity id="N04-1008.4">language model</entity> for <entity id="N04-1008.5">answers</entity> and a <entity id="N04-1008.6">transformation model</entity> for <entity id="N04-1008.7">answer/question terms</entity>, trained on a <entity id="N04-1008.8">corpus</entity> of 1 million <entity id="N04-1008.9">question/answer pairs</entity> collected from the Web.
</abstract>



</text>

<text id="P05-1046">
<title>
Unsupervised Learning Of Field Segmentation Models For Information Extraction </title><abstract>
The applicability of many current <entity id="P05-1046.1">information extraction techniques</entity> is severely limited by the need for <entity id="P05-1046.2">supervised training data</entity>. 
We demonstrate that for certain <entity id="P05-1046.3">field structured extraction tasks</entity>, such as classified advertisements and bibliographic citations, small amounts of <entity id="P05-1046.4">prior knowledge</entity> can be used to learn effective models in a primarily unsupervised fashion. Although <entity id="P05-1046.5">hidden Markov models (HMMs)</entity> provide a suitable <entity id="P05-1046.6">generative model</entity> for <entity id="P05-1046.7">field structured text</entity>, general <entity id="P05-1046.8">unsupervised HMM learning</entity> fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple <entity id="P05-1046.9">prior knowledge</entity> of the desired solutions. In both domains, we found that <entity id="P05-1046.10">unsupervised methods</entity> can attain <entity id="P05-1046.11">accuracies</entity> with 400 <entity id="P05-1046.12">unlabeled examples</entity> comparable to those attained by <entity id="P05-1046.13">supervised methods</entity> on 50 <entity id="P05-1046.14">labeled examples</entity>, and that <entity id="P05-1046.15">semi-supervised methods</entity> can make good use of small amounts of <entity id="P05-1046.16">labeled data</entity>.
</abstract>



</text>

<text id="P05-1073">
<title>
Joint Learning Improves Semantic Role Labeling </title><abstract>
Despite much recent progress on accurate <entity id="P05-1073.1">semantic role labeling</entity>, previous work has largely used <entity id="P05-1073.2">independent classifiers</entity>, possibly combined with separate <entity id="P05-1073.3">label sequence models</entity> via <entity id="P05-1073.4">Viterbi decoding</entity>. This stands in stark contrast to the linguistic observation that a <entity id="P05-1073.5">core argument frame</entity> is a joint structure, with strong <entity id="P05-1073.6">dependencies</entity> between <entity id="P05-1073.7">arguments</entity>. We show how to build a <entity id="P05-1073.8">joint model</entity> of <entity id="P05-1073.9">argument frames</entity>, incorporating novel <entity id="P05-1073.10">features</entity> that model these interactions into <entity id="P05-1073.11">discriminative log-linear models</entity>. This system achieves an <entity id="P05-1073.12">error reduction</entity> of 22% on all <entity id="P05-1073.13">arguments</entity> and 32% on <entity id="P05-1073.14">core arguments</entity> over a state-of-the art independent <entity id="P05-1073.15">classifier</entity> for <entity id="P05-1073.16">gold-standard parse trees</entity> on <entity id="P05-1073.17">PropBank</entity>.
</abstract>



</text>

<text id="P05-3030">
<title>
Organizing English Reading Materials For Vocabulary Learning </title><abstract>
We propose a method of organizing reading materials for <entity id="P05-3030.1">vocabulary learning</entity>. It enables us to select a concise set of reading <entity id="P05-3030.2">texts</entity> (from a <entity id="P05-3030.3">target corpus</entity>) that contains all the <entity id="P05-3030.4">target vocabulary</entity> to be learned. We used a specialized <entity id="P05-3030.5">vocabulary</entity> for an English certification test as the <entity id="P05-3030.6">target vocabulary</entity> and used <entity id="P05-3030.7">English Wikipedia</entity>, a free-content encyclopedia, as the <entity id="P05-3030.8">target corpus</entity>. The organized reading materials would enable learners not only to study the <entity id="P05-3030.9">target vocabulary</entity> efficiently but also to gain a variety of knowledge through reading. The reading materials are available on our web site.
</abstract>



</text>

<text id="E83-1029"> 
<title> NATURAL LANGUAGE INPUT FOR SCENE GENERATION </title>
<abstract>
In this paper a system which understands and conceptualizes <entity id="E83-1029.1">scenes descriptions in natural language</entity> is presented. Specifically, the following components of the system are described: the <entity id="E83-1029.2">syntactic analyzer</entity>, based on a <entity id="E83-1029.3">Procedural Systemic Grammar</entity>, the <entity id="E83-1029.4">semantic analyzer</entity> relying on the <entity id="E83-1029.5">Conceptual Dependency Theory</entity>, and the <entity id="E83-1029.6">dictionary</entity>. </abstract>



</text>

<text id="E89-1006"> 
<title> TENSES AS ANAPHORA </title>
<abstract>
A proposal to deal with <entity id="E89-1006.1">French tenses</entity> in the framework of <entity id="E89-1006.2">Discourse Representation Theory</entity> is presented, as it has been implemented for a fragment at the <entity id="E89-1006.3">IMS</entity>. It is based on the <entity id="E89-1006.4">theory of tenses</entity> of H. Kamp and Ch. Rohrer. Instead of using <entity id="E89-1006.5">operators</entity> to express the <entity id="E89-1006.6">meaning</entity> of the <entity id="E89-1006.7">tenses</entity> the Reichenbachian point of view is adopted and refined such that the impact of the <entity id="E89-1006.8">tenses</entity> with respect to the <entity id="E89-1006.9">meaning</entity> of the <entity id="E89-1006.10">text</entity> is understood as contribution to the integration of the <entity id="E89-1006.11">events</entity> of a <entity id="E89-1006.12">sentence</entity> in the <entity id="E89-1006.13">event structure</entity> of the preceeding <entity id="E89-1006.14">text</entity>. Thereby a <entity id="E89-1006.15">system of relevant times</entity> provided by the preceeding <entity id="E89-1006.16">text</entity> and by the <entity id="E89-1006.17">temporal adverbials</entity> of the <entity id="E89-1006.18">sentence</entity> being processed is used. This system consists of one or more <entity id="E89-1006.19">reference times</entity> and <entity id="E89-1006.20">temporal perspective times</entity>, the <entity id="E89-1006.21">speech time</entity> and the <entity id="E89-1006.22">location time</entity>. The special interest of our proposal is to establish a plausible choice of anchors for the <entity id="E89-1006.23">new event</entity> out of the <entity id="E89-1006.24">system of relevant times</entity> and to update this <entity id="E89-1006.25">system of temporal coordinates</entity> correctly. The problem of choice is largely neglected in the literature. In opposition to the approach of Kamp and Rohrer the exact <entity id="E89-1006.26">meaning</entity> of the <entity id="E89-1006.27">tenses</entity> is fixed by the <entity id="E89-1006.28">resolution component</entity> and not in the process of <entity id="E89-1006.29">syntactic analysis</entity>. </abstract>



</text>

<text id="E93-1004">
<title>
Talking About Trees </title><abstract>
In this paper we introduce a <entity id="E93-1004.1">modal language LT</entity>for imposing <entity id="E93-1004.2">constraints</entity> on <entity id="E93-1004.3">trees</entity>, and an extension <entity id="E93-1004.4">LT (LF)</entity> for imposing <entity id="E93-1004.5">constraints</entity> on <entity id="E93-1004.6">trees decorated with feature structures</entity>. The motivation for introducing these <entity id="E93-1004.7">languages</entity> is to provide tools for formalising <entity id="E93-1004.8">grammatical frameworks</entity> perspicuously, and the paper illustrates this by showing how the leading ideas of <entity id="E93-1004.9">GPSG</entity> can be captured in <entity id="E93-1004.10">LT (LF)</entity>. In addition, the role of <entity id="E93-1004.11">modal languages</entity> (and in particular, what we have called as <entity id="E93-1004.12">constraint formalisms</entity> for linguistic theorising is discussed in some detail.
</abstract>



</text>

<text id="E99-1029"> 
<title>Parsing with an Extended Domain of Locality </title>
<abstract>
One of the claimed benefits of <entity id="E99-1029.1">Tree Adjoining Grammars</entity> is that they have an <entity id="E99-1029.2">extended domain of locality (EDOL)</entity>. We consider how this can be exploited to limit the need for <entity id="E99-1029.3">feature structure unification</entity> during <entity id="E99-1029.4">parsing</entity>. We compare two wide-coverage <entity id="E99-1029.5">lexicalized grammars of English</entity>, <entity id="E99-1029.6">LEXSYS</entity> and <entity id="E99-1029.7">XTAG</entity>, finding that the two <entity id="E99-1029.8">grammars</entity> exploit <entity id="E99-1029.9">EDOL</entity> in different ways. </abstract>



</text>

<text id="E95-1033">
<title>
ParseTalk About Sentence- And Text-Level Anaphora</title><abstract>
We provide a unified account of <entity id="E95-1033.1">sentence-level and text-level anaphora</entity> within the framework of a <entity id="E95-1033.2">dependency-based grammar model</entity>. Criteria for <entity id="E95-1033.3">anaphora resolution</entity> within <entity id="E95-1033.4">sentence boundaries</entity> rephrase major concepts from <entity id="E95-1033.5">GB's binding theory</entity>, while those for <entity id="E95-1033.6">text-level anaphora</entity> incorporate an adapted version of a <entity id="E95-1033.7">Grosz-Sidner-style focus model</entity>.
</abstract>



</text>

<text id="H89-1027">
<title>
The MIT Summit Speech Recognition System : A Progress Report</title><abstract>
Recently, we initiated a project to develop a <entity id="H89-1027.1">phonetically-based spoken language understanding system</entity> called <entity id="H89-1027.2">SUMMIT</entity>. In contrast to many of the past efforts that make use of <entity id="H89-1027.3">heuristic rules</entity> whose development requires intense <entity id="H89-1027.4">knowledge engineering</entity>, our approach attempts to express the <entity id="H89-1027.5">speech knowledge</entity> within a formal framework using well-defined mathematical tools. In our system, <entity id="H89-1027.6">features</entity> and <entity id="H89-1027.7">decision strategies</entity> are discovered and trained automatically, using a large body of <entity id="H89-1027.8">speech data</entity>. This paper describes the system, and documents its current performance.
</abstract>



</text>

<text id="H91-1077">
<title>
A Proposal For Lexical Disambiguation </title><abstract>
A method of <entity id="H91-1077.1">sense resolution</entity> is proposed that is based on <entity id="H91-1077.2">WordNet</entity>, an on-line <entity id="H91-1077.3">lexical database</entity> that incorporates <entity id="H91-1077.4">semantic relations</entity> (<entity id="H91-1077.5">synonymy</entity>, <entity id="H91-1077.6">antonymy</entity>, <entity id="H91-1077.7">hyponymy</entity>, <entity id="H91-1077.8">meronymy</entity>, <entity id="H91-1077.9">causal and troponymic entailment</entity>) as <entity id="H91-1077.10">labeled pointers</entity> between <entity id="H91-1077.11">word senses</entity>. With <entity id="H91-1077.12">WordNet</entity>, it is easy to retrieve sets of <entity id="H91-1077.13">semantically related words</entity>, a facility that will be used for <entity id="H91-1077.14">sense resolution</entity> during <entity id="H91-1077.15">text processing</entity>, as follows. When a <entity id="H91-1077.16">word</entity> with multiple <entity id="H91-1077.17">senses</entity> is encountered, one of two procedures will be followed. Either, (1) <entity id="H91-1077.18">words</entity> related in <entity id="H91-1077.19">meaning</entity> to the <entity id="H91-1077.20">alternative senses</entity> of the <entity id="H91-1077.21">polysemous word</entity> will be retrieved; new <entity id="H91-1077.22">strings</entity> will be derived by substituting these related <entity id="H91-1077.23">words</entity> into the <entity id="H91-1077.24">context</entity> of the <entity id="H91-1077.25">polysemous word</entity>; a large <entity id="H91-1077.26">textual corpus</entity> will then be searched for these <entity id="H91-1077.27">derived strings</entity>; and that <entity id="H91-1077.28">sense</entity> will be chosen that corresponds to the <entity id="H91-1077.29">derived string</entity> that is found most often in the <entity id="H91-1077.30">corpus</entity>. Or, (2) the <entity id="H91-1077.31">context</entity> of the <entity id="H91-1077.32">polysemous word</entity> will be used as a key to search a large <entity id="H91-1077.33">corpus</entity>; all <entity id="H91-1077.34">words</entity> found to occur in that <entity id="H91-1077.35">context</entity> will be noted; <entity id="H91-1077.36">WordNet</entity> will then be used to estimate the <entity id="H91-1077.37">semantic distance</entity> from those <entity id="H91-1077.38">words</entity> to the <entity id="H91-1077.39">alternative senses</entity> of the <entity id="H91-1077.40">polysemous word</entity>; and that <entity id="H91-1077.41">sense</entity> will be chosen that is closest in <entity id="H91-1077.42">meaning</entity> to other <entity id="H91-1077.43">words</entity> occurring in the same <entity id="H91-1077.44">context</entity> If successful, this procedure could have practical applications to problems of <entity id="H91-1077.45">information retrieval</entity>, <entity id="H91-1077.46">mechanical translation</entity>, <entity id="H91-1077.47">intelligent tutoring systems</entity>, and elsewhere.
</abstract>



</text>

<text id="A97-1027">
<title>
Dutch Sublanguage Semantic Tagging Combined With Mark-Up Technology </title><abstract>
In this paper, we want to show how the <entity id="A97-1027.1">morphological component</entity> of an existing <entity id="A97-1027.2">NLP-system for Dutch (Dutch Medical Language Processor - DMLP)</entity> has been extended in order to produce output that is compatible with the <entity id="A97-1027.3">language independent modules</entity> of the <entity id="A97-1027.4">LSP-MLP system (Linguistic String Project - Medical Language Processor)</entity> of the New York University. The former can take advantage of the <entity id="A97-1027.5">language independent developments</entity> of the latter, while focusing on <entity id="A97-1027.6">idiosyncrasies</entity> for <entity id="A97-1027.7">Dutch</entity>. This general strategy will be illustrated by a practical application, namely the highlighting of relevant information in a <entity id="A97-1027.8">patient discharge summary (PDS)</entity> by means of modern <entity id="A97-1027.9">HyperText Mark-Up Language (HTML) technology</entity>. Such an application can be of use for medical administrative purposes in a hospital environment.
</abstract>



</text>

<text id="A97-1052">
<title> Automatic Extraction Of Subcategorization From Corpora </title>
<abstract>
We describe a novel technique and implemented system for constructing a <entity id="A97-1052.1">subcategorization dictionary</entity> from <entity id="A97-1052.2">textual corpora</entity>. Each <entity id="A97-1052.3">dictionary entry</entity> encodes the <entity id="A97-1052.4">relative frequency of occurrence</entity> of a comprehensive set of <entity id="A97-1052.5">subcategorization classes</entity> for <entity id="A97-1052.6">English</entity>. An initial experiment, on a sample of 14 <entity id="A97-1052.7">verbs</entity> which exhibit <entity id="A97-1052.8">multiple complementation patterns</entity>, demonstrates that the technique achieves <entity id="A97-1052.9">accuracy</entity> comparable to previous approaches, which are all limited to a highly restricted set of <entity id="A97-1052.10">subcategorization classes</entity>. We also demonstrate that a <entity id="A97-1052.11">subcategorization dictionary</entity> built with the system improves the <entity id="A97-1052.12">accuracy</entity> of a <entity id="A97-1052.13">parser</entity> by an appreciable amount</abstract>



</text>

<text id="J87-3001"> 
<title>PROCESSING DICTIONARY DEFINITIONS WITH PHRASAL PATTERN HIERARCHIES </title>
<abstract>
This paper shows how <entity id="J87-3001.1">dictionary word sense definitions</entity> can be analysed by applying a hierarchy of <entity id="J87-3001.2">phrasal patterns</entity>. An experimental system embodying this mechanism has been implemented for processing <entity id="J87-3001.3">definitions</entity> from the <entity id="J87-3001.4">Longman Dictionary of Contemporary English</entity>. A property of this <entity id="J87-3001.5">dictionary</entity>, exploited by the system, is that it uses a <entity id="J87-3001.6">restricted vocabulary</entity> in its <entity id="J87-3001.7">word sense definitions</entity>. The structures generated by the experimental system are intended to be used for the <entity id="J87-3001.8">classification</entity> of new <entity id="J87-3001.9">word senses</entity> in terms of the <entity id="J87-3001.10">senses</entity> of <entity id="J87-3001.11">words</entity> in the <entity id="J87-3001.12">restricted vocabulary</entity>. Examples illustrating the output generated are presented, and some qualitative performance results and problems that were encountered are discussed. The analysis process applies successively more specific <entity id="J87-3001.13">phrasal analysis rules</entity> as determined by a hierarchy of <entity id="J87-3001.14">patterns</entity> in which less specific <entity id="J87-3001.15">patterns</entity> dominate more specific ones. This ensures that reasonable incomplete analyses of the <entity id="J87-3001.16">definitions</entity> are produced when more complete analyses are not possible, resulting in a relatively robust <entity id="J87-3001.17">analysis mechanism</entity>. Thus the work reported addresses two <entity id="J87-3001.18">robustness problems</entity> faced by current experimental <entity id="J87-3001.19">natural language processing systems</entity>: coping with an incomplete <entity id="J87-3001.20">lexicon</entity> and with incomplete <entity id="J87-3001.21">knowledge</entity> of <entity id="J87-3001.22">phrasal constructions</entity>. 
</abstract>



</text>

<text id="I05-5009">
<title>Evaluating Contextual Dependency of Paraphrases using a Latent Variable Model </title><abstract>
This paper presents an <entity id="I05-5009.1">evaluation method</entity> employing a <entity id="I05-5009.2">latent variable model</entity> for <entity id="I05-5009.3">paraphrases</entity> with their <entity id="I05-5009.4">contexts</entity>. We assume that the <entity id="I05-5009.5">context</entity> of a <entity id="I05-5009.6">sentence</entity> is indicated by a <entity id="I05-5009.7">latent variable</entity> of the <entity id="I05-5009.8">model</entity> as a <entity id="I05-5009.9">topic</entity> and that the <entity id="I05-5009.10">likelihood</entity> of each <entity id="I05-5009.11">variable</entity> can be inferred. A <entity id="I05-5009.12">paraphrase</entity> is evaluated for whether its <entity id="I05-5009.13">sentences</entity> are used in the same <entity id="I05-5009.14">context</entity>. Experimental results showed that the proposed method achieves almost 60% <entity id="I05-5009.15">accuracy</entity> and that there is not a large performance difference between the two <entity id="I05-5009.16">models</entity>. The results also revealed an upper bound of <entity id="I05-5009.17">accuracy</entity> of 77% with the <entity id="I05-5009.18">method</entity> when using only <entity id="I05-5009.19">topic information</entity>.
</abstract>



</text>

<text id="P83-1003"> 
<title> Crossed Serial Dependencies : A low-power parseable extension to GPSG </title> 
<abstract>
An extension to the <entity id="P83-1003.1">GPSG grammatical formalism</entity> is proposed, allowing <entity id="P83-1003.2">non-terminals</entity> to consist of finite sequences of <entity id="P83-1003.3">category labels</entity>, and allowing <entity id="P83-1003.4">schematic variables</entity> to range over such sequences. The extension is shown to be sufficient to provide a strongly adequate <entity id="P83-1003.5">grammar</entity> for <entity id="P83-1003.6">crossed serial dependencies</entity>, as found in e.g. <entity id="P83-1003.7">Dutch subordinate clauses</entity>. The structures induced for such <entity id="P83-1003.8">constructions</entity> are argued to be more appropriate to data involving <entity id="P83-1003.9">conjunction</entity> than some previous proposals have been. The extension is shown to be parseable by a simple extension to an existing <entity id="P83-1003.10">parsing method</entity> for <entity id="P83-1003.11">GPSG</entity>. </abstract>



</text>
</doc>
